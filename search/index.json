[{"content":"引言 一个令人困惑的现象 我最近遇到了一个奇怪的现象：在一个 Java Spring Boot 项目开发中，应用启动时显示了一些日志，然后就完全停止输出，程序看起来像是卡住了。控制台光标在闪烁，但是什么都不会发生。\n然而，通过排除一个jar包，程序就能正常启动。这让我非常困惑。 为什么排除一个日志相关的依赖就能解决问题？程序真的卡住了吗？\n我将分析我最近遇到的一个真实案例。\n一、当程序\u0026quot;卡住\u0026quot;的那一刻 初步现象：程序似乎停止了 我执行通过IDEA执行mvn spring-boot:run 启动应用时，控制台显示了这些信息：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 SLF4J(W): Class path contains multiple SLF4J providers. SLF4J(W): Found provider [org.slf4j.reload4j.Reload4jServiceProvider@64cd705f] SLF4J(W): Found provider [org.apache.logging.slf4j.SLF4JServiceProvider@9225652] SLF4J(I): Actual provider is of type [org.slf4j.reload4j.Reload4jServiceProvider@64cd705f] log4j:WARN No appenders could be found for logger (com.sankuai.inf.octo.mns.util.ProcessInfoUtil). log4j:WARN Please initialize the log4j system properly. 2025-09-19T02:13:23.178052Z main INFO create XMDFileAppender [name=xmdCraneAppender fullFileName=/data/applogs/inf/com.example.com/crane.log appkey=com.example.com fullFilePattern=/data/applogs/inf/com.example.com/crane.log-%d{yyyy-MM-dd}-%i.log] 2025-09-19T02:13:23.178277Z main INFO create XMDFileAppender [name=xmdMafkaAppender fullFileName=/data/applogs/inf/com.example.com/mafka.log appkey=com.example.com fullFilePattern=/data/applogs/inf/com.example.com/mafka.log-%d{yyyy-MM-dd}-%i.log] 2025-09-19T02:13:23.178424Z main INFO create XMDFileAppender [name=xmdRhinoAppender fullFileName=/data/applogs/inf/com.example.com/rhino.log appkey=com.example.com fullFilePattern=/data/applogs/inf/com.example.com/rhino.log-%d{yyyy-MM-dd}-%i.log] 2025-09-19T02:13:23.178563Z main INFO create XMDFileAppender [name=xmdKmsAppender fullFileName=/data/applogs/inf/com.example.com/kms.log appkey=com.example.com fullFilePattern=/data/applogs/inf/com.example.com/kms.log-%d{yyyy-MM-dd}-%i.log] 2025-09-19T02:13:23.183208Z main INFO Begin Stop AsyncScibeAppender: ScribeAsyncAppender Queue still has 0 events 2025-09-19T02:13:23.183676Z main INFO End Stop AsyncScibeAppender: ScribeAsyncAppender Queue still has 0 events //////////////////////////////////////////////////////////////////// // _ooOoo_ // // o8888888o // // 88\u0026#34; . \u0026#34;88 // // (| ^_^ |) // // O\\ = /O // // ____/`---\u0026#39;\\____ // // .\u0026#39; \\\\| |// `. // // / \\\\||| : |||// \\ // // / _||||| -:- |||||- \\ // // | | \\\\\\ - /// | | // // | \\_| \u0026#39;\u0026#39;\\---/\u0026#39;\u0026#39; | | // // \\ .-\\__ `-` ___/-. / // // ___`. .\u0026#39; /--.--\\ `. . ___ // // .\u0026#34;\u0026#34; \u0026#39;\u0026lt; `.___\\_\u0026lt;|\u0026gt;_/___.\u0026#39; \u0026gt;\u0026#39;\u0026#34;\u0026#34;. // // | | : `- \\`.;`\\ _ /`;.`/ - ` : | | // // \\ \\ `-. \\_ __\\ /__ _/ .-` / / // // ========`-.____`-.___\\_____/___.-`____.-\u0026#39;======== // // `=---=\u0026#39; // // ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ // // 佛祖保佑 永不宕机 永无BUG // // Thanks to Buddha\u0026#39;s blessing, may our system // // remain stable and bug-free forever. // //////////////////////////////////////////////////////////////////// 9月 19, 2025 10:13:26 上午 org.bouncycastle.jsse.provider.PropertyUtils getStringSecurityProperty 信息: Found string security property [jdk.tls.disabledAlgorithms]: SSLv3, TLSv1, TLSv1.1, RC4, DES, MD5withRSA, DH keySize \u0026lt; 1024, EC keySize \u0026lt; 224, 3DES_EDE_CBC, anon, NULL 9月 19, 2025 10:13:26 上午 org.bouncycastle.jsse.provider.PropertyUtils getStringSecurityProperty 信息: Found string security property [jdk.certpath.disabledAlgorithms]: MD2, MD5, SHA1 jdkCA \u0026amp; usage TLSServer, RSA keySize \u0026lt; 1024, DSA keySize \u0026lt; 1024, EC keySize \u0026lt; 224, SHA1 usage SignedJAR \u0026amp; denyAfter 2019-01-01 9月 19, 2025 10:13:26 上午 org.bouncycastle.jsse.provider.DisabledAlgorithmConstraints create 警告: Ignoring unsupported entry in \u0026#39;jdk.certpath.disabledAlgorithms\u0026#39;: SHA1 jdkCA \u0026amp; usage TLSServer 9月 19, 2025 10:13:26 上午 org.bouncycastle.jsse.provider.DisabledAlgorithmConstraints create 警告: Ignoring unsupported entry in \u0026#39;jdk.certpath.disabledAlgorithms\u0026#39;: SHA1 usage SignedJAR \u0026amp; denyAfter 2019-01-01 2025-09-19T02:13:26.632017Z xmdlog-registry-service INFO Registry LogManager register status true, url : http://log.inf.dev.sankuai.com/api/register?appkey=com.example.com\u0026amp;ip=172.18.151.200\u0026amp;port=12315\u0026amp;env=TEST\u0026amp;version=2.2.0 Local Appenv: deployenv [qa], env [test] AppEnv{deployenv=\u0026#39;qa\u0026#39;, env=\u0026#39;test\u0026#39;, swimlane=\u0026#39;tmp-test\u0026#39;, cell=\u0026#39;null\u0026#39;, grouptags=\u0026#39;null\u0026#39;} Fri Sep 19 10:13:48 CST 2025 WARN: Establishing SSL connection without server\u0026#39;s identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn\u0026#39;t set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to \u0026#39;false\u0026#39;. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. 在显示了一些组件初始化信息和ASCII图案后，程序就完全停止输出了。控制台光标在闪烁，但是什么都不会发生。\n二、真相往往出人意料 数字不会说谎：56.7%的CPU使用率 首先使用 ps 命令查看进程信息：\n1 2 $ ps aux | grep ApplicationLoader majiang 97897 56.7 2.5 443940128 633216 ?? R 11:34AM 1:19.25 这个结果让我震惊：\n进程状态: R (Running) — 程序在运行，不是阻塞状态 CPU使用率: 56.7% — 程序在积极工作 运行时间: 1小时19分钟 — 程序一直在持续运行 我原本以为程序卡住了，如果真是这样，CPU使用率应该接近0%。但现在56.7%的CPU使用率说明程序在正常运行！\n深入检查：使用各种分析工具 为了彻底搞清楚程序在做什么，我使用了多种分析手段：\n线程状态分析: 使用jstack反复检查，没有发现死锁或异常阻塞\nProfiler火焰图: 通过JProfiler查看方法调用，程序执行正常\n线程时间线: 观察线程的时间分布，各个线程都在正常工作\n线程快照: 多次获取线程快照对比，线程状态正常工作\n所有这些工具都指向同一个结论：程序运行完全正常，没有任何阻塞或性能问题。\n进一步分析线程堆栈，我发现了关键信息：\n1 2 3 4 5 6 7 8 9 $ jstack 97897 | grep -E \u0026#34;main.*tid\u0026#34; # 没有找到main线程！ $ jstack 97897 | head -20 \u0026#34;qtp1366921090-106\u0026#34; #106 prio=5 ... waiting on condition \u0026#34;qtp1366921090-107\u0026#34; #107 prio=5 ... runnable \u0026#34;RhinoHttpSeverBossGroup-2-1\u0026#34; #128 prio=5 ... runnable \u0026#34;MtthriftServerBossGroup-8-1\u0026#34; #346 prio=5 ... runnable \u0026#34;DestroyJavaVM\u0026#34; #377 prio=5 ... waiting on condition 这些发现彻底改变了我的认知：\n✅ Main线程已经正常执行完毕并退出 ✅ Spring Boot应用完全启动成功 ✅ HTTP服务器线程池（qtp开头的线程）在正常运行 ✅ 各种服务组件的工作线程都在运行 程序根本没有\u0026quot;卡住\u0026quot;！\n真正发生的是：应用在后台正常运行，但是没有任何日志输出。这给我造成了程序停止响应的错觉。\n问题的本质从\u0026quot;程序阻塞\u0026quot;变成了\u0026quot;日志系统失效\u0026quot;。\n三、静默杀手的真面目 SLF4J的选择困难症 既然程序正常运行但没有日志输出，问题就出在日志系统上。回看最初的警告信息：\n1 2 3 4 SLF4J(W): Class path contains multiple SLF4J providers. SLF4J(W): Found provider [org.slf4j.reload4j.Reload4jServiceProvider@64cd705f] SLF4J(W): Found provider [org.apache.logging.slf4j.SLF4JServiceProvider@9225652] SLF4J(I): Actual provider is of type [org.slf4j.reload4j.Reload4jServiceProvider@64cd705f] 这里揭示了关键问题：SLF4J在两个日志实现之间选择了错误的那个。\nSLF4J是一个日志门面，它在运行时从classpath中选择一个具体的日志实现。选择策略很简单：使用第一个发现的提供者。\n两个冲突的提供者：\norg.slf4j.reload4j.Reload4jServiceProvider — 来自slf4j-reload4j包，兼容Log4j 1.x org.apache.logging.slf4j.SLF4JServiceProvider — 来自log4j-slf4j2-impl包，支持Log4j 2.x SLF4J选择了reload4j提供者，但我的项目配置的是Log4j 2.x格式。\n被吞噬的日志去了哪里 我的项目使用的是标准的Log4j 2.x配置文件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \u0026lt;configuration status=\u0026#34;info\u0026#34;\u0026gt; \u0026lt;appenders\u0026gt; \u0026lt;Console name=\u0026#34;Console\u0026#34; target=\u0026#34;SYSTEM_OUT\u0026#34;\u0026gt; \u0026lt;PatternLayout pattern=\u0026#34;${sys:CONSOLE_LOG_PATTERN}\u0026#34; /\u0026gt; \u0026lt;/Console\u0026gt; \u0026lt;XMDFile name=\u0026#34;infoAppender\u0026#34; fileName=\u0026#34;info.log\u0026#34;\u0026gt; \u0026lt;!-- Log4j 2.x特有的appender --\u0026gt; \u0026lt;/XMDFile\u0026gt; \u0026lt;/appenders\u0026gt; \u0026lt;loggers\u0026gt; \u0026lt;root level=\u0026#34;info\u0026#34;\u0026gt; \u0026lt;appender-ref ref=\u0026#34;Console\u0026#34;/\u0026gt; \u0026lt;/root\u0026gt; \u0026lt;/loggers\u0026gt; \u0026lt;/configuration\u0026gt; 问题在于：reload4j只能理解Log4j 1.x的配置格式，无法解析Log4j 2.x的高级语法。\n失败链路：\nreload4j尝试解析log4j2.xml 遇到不认识的\u0026lt;XMDFile\u0026gt;等Log4j 2.x元素 配置解析失败，没有创建任何appender 所有日志调用都被静默丢弃 这就是为什么会出现这个警告：\n1 2 log4j:WARN No appenders could be found for logger (com.sankuai.inf.octo.mns.util.ProcessInfoUtil). log4j:WARN Please initialize the log4j system properly. 完整的\u0026quot;吞噬\u0026quot;过程：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 第1步: 业务代码调用 logger.info(\u0026#34;Spring Boot application started\u0026#34;) ↓ 第2步: SLF4J门面接收调用 org.slf4j.Logger.info(String msg) ↓ 第3步: SLF4J路由到选中的提供者 SLF4J → org.slf4j.reload4j.Reload4jLoggerAdapter ↓ 第4步: reload4j适配器委托给Log4j 1.x Reload4jLoggerAdapter → org.apache.log4j.Logger (reload4j实现) ↓ 第5步: Log4j检查可用的appender logger.getAllAppenders() → 返回空集合 (因为配置解析失败) ↓ 第6步: 没有appender可用 Log4j判断：既然没有appender，就直接丢弃这条消息 ↓ 第7步: 静默返回 方法正常返回，不抛异常，业务代码继续执行 ↓ 结果: 用户完全看不到任何日志输出 Log4j采用\u0026quot;静默失败\u0026quot;策略：当没有可用的appender时，所有日志调用正常执行但输出被完全丢弃，不会抛出异常影响业务逻辑。\n这就是为什么排除slf4j-reload4j能解决问题：去掉错误的提供者后，SLF4J会选择正确的Log4j 2.x实现，日志系统恢复正常。\n四、破解之道 斩断混乱的依赖链 理解了问题本质后，解决方案就很清楚了：消除SLF4J提供者冲突。\n通过Maven依赖排除，将错误的提供者从classpath中移除：\n1 2 3 4 5 6 7 8 9 10 11 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.sankuai.com\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;example-sdk\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0\u0026lt;/version\u0026gt; \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;artifactId\u0026gt;slf4j-reload4j\u0026lt;/artifactId\u0026gt; \u0026lt;groupId\u0026gt;org.slf4j\u0026lt;/groupId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; \u0026lt;/dependency\u0026gt; 修复后的效果：\nClasspath中只剩下log4j-slf4j2-impl提供者 SLF4J自动选择Log4j 2.x实现 Log4j 2.x正确解析log4j2.xml配置文件 Console和File appender正常创建 日志输出恢复正常，应用\u0026quot;复活\u0026quot; 验证修复结果：\n1 2 3 2025-09-19 10:37:00.840 INFO 62045 --- [ main] c.s.h.g.g.ApplicationLoader : Starting ApplicationLoader using Java 17.0.10 2025-09-19 10:37:00.840 INFO 62045 --- [ main] c.s.h.g.g.ApplicationLoader : The following 1 profile is active: \u0026#34;test\u0026#34; 2025-09-19 10:37:15.234 INFO 62045 --- [ main] c.s.h.g.g.ApplicationLoader : Started ApplicationLoader in 14.628 seconds 没有了SLF4J警告信息，没有了log4j的错误提示，一切都恢复正常。\n为什么会一直使用reald4j? 通过查看项目的pom.xml文件，我发现photo-thrift-client依赖的位置确实影响了SLF4J提供者的选择。\n在hrmdm-globaldata-gateway-infrastructure/pom.xml中，photo-thrift-client依赖位于：\n1 2 3 4 5 6 7 8 9 10 11 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.sankuai.com\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;example-sdk\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0\u0026lt;/version\u0026gt; \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;artifactId\u0026gt;slf4j-reload4j\u0026lt;/artifactId\u0026gt; \u0026lt;groupId\u0026gt;org.slf4j\u0026lt;/groupId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; \u0026lt;/dependency\u0026gt; 这个依赖在文件中的位置相对较前，而且它直接引入了slf4j-reload4j作为传递依赖。由于Maven构建classpath时按照依赖在文件中的声明顺序，slf4j-reload4j的ServiceProvider文件被先加载到ServiceLoader中。\n具体的加载顺序：\nMaven解析依赖树，photo-thrift-client较早被处理 photo-thrift-client → slf4j-reload4j 被添加到classpath靠前位置 ServiceLoader扫描时，先发现slf4j-reload4j的ServiceProvider SLF4J选择第一个发现的提供者：reload4j 这就是为什么slf4j-reload4j被优先选择的根本原因。不同的项目中，如果依赖声明顺序不同，可能就会选择log4j-slf4j2-impl作为提供者。\n验证方法：\n1 mvn dependency:build-classpath 可以查看实际构建的classpath顺序，确认slf4j-reload4j确实在前面。\n尾声：技术世界的启示 这个案例揭示了现代Java应用生态的复杂性。一个看似无害的传递依赖，通过微妙的类加载顺序和配置不匹配，导致了整个日志系统的静默失效。\n表象具有欺骗性。当我最初看到程序\u0026quot;卡住\u0026quot;时，直觉告诉我这是一个阻塞或死锁问题。但深入分析后发现，程序运行完全正常，问题出在一个完全不同的地方。\n工具是解决问题的关键。通过系统命令（ps、jstack）和分析工具，我能够透过现象看到本质，发现程序实际在正常运行。\n依赖管理的重要性。在现代Java项目中，传递依赖可能带来意想不到的问题。一个小小的jar包冲突，就能让整个应用的行为变得诡异。\n静默失败的双刃剑。Log4j的静默失败设计保护了业务逻辑，但也让问题变得更难发现。有时候，适当的\u0026quot;噪音\u0026quot;比安静的错误更有价值。\n最重要的是：不要被第一印象误导。技术问题往往比表面现象更复杂，也更有趣。当遇到无法理解的现象时，保持好奇心，深入挖掘，往往会发现令人惊喜的真相。\n在复杂的技术世界里，最神秘的故障背后，可能隐藏着最简单的原理。\n","date":"2025-09-19T16:30:00+08:00","permalink":"https://cyber-blog.github.io/p/spring-boot/","title":"Spring Boot应用假死现象：SLF4J日志框架冲突深度分析"},{"content":"MySQL InnoDB 索引机制与优化：执行流程、覆盖索引、回表与索引下推 InnoDB 表基于 B+ 树实现聚簇索引（主键）和多个二级索引。查询时，MySQL 优化器会选择合适的索引，然后按照执行流程依次进行索引查找（检索满足条件的索引项）、条件过滤、排序（如有需要）和回表（根据二级索引叶子节点的主键值检索完整行）。如果所有查询列都包含在索引中（覆盖索引），则可避免回表，从而节省随机 IO ；否则需要通过主键到聚簇索引检索整行数据（回表），这增加了额外开销 。此外，InnoDB 支持索引条件下推（Index Condition Pushdown，ICP）优化：在扫描二级索引时，将部分 WHERE 条件在存储引擎层提前过滤，从而减少了无效行的回表次数 。\n以下内容通过实际建表、查询示例和 EXPLAIN 分析，详细阐述索引在各阶段的作用，比较覆盖索引与回表的差异，并说明 ICP 在执行流程中的作用与限制。\nInnoDB 索引查找与执行流程 MySQL 查询经过优化器确定执行计划，若使用索引，则执行引擎依次进行：(1) 索引查找：根据索引 B+ 树定位满足前缀条件的叶子节点；(2) 条件过滤：对于索引未能覆盖的条件，在存储引擎层过滤（ICP 可提前过滤部分条件），其余条件或计算在服务器层过滤；(3) 排序：若 ORDER BY 条件与索引顺序一致，则可以顺序扫描索引避免额外排序；否则会使用额外的排序（Using filesort） ；(4) 回表：如果查询需要的列不全在二级索引中，则根据索引叶子节点的主键值到聚簇索引查完整行，此过程即“回表”。整个流程如下：\n索引查找：优化器选择最合适的索引（如单列索引、多列联合索引）进行查找。InnoDB 二级索引叶子节点存储了对应行的主键值 ，因此定位二级索引时可直接获取主键。 条件过滤：默认情况下，MySQL 服务器层将所有非索引列的过滤条件留到获取完整行后处理；对于索引列上的条件，存储引擎可在索引层过滤。启用 ICP 时，满足条件的行可以在索引层提前淘汰 。 排序：如果查询涉及 ORDER BY，优化器检查索引顺序是否可以按需排序。若索引前缀完全匹配排序列并且无其他排序列，则无需额外排序；否则会产生 Using filesort 。 回表（Bookmark Lookup）：当查询列超出索引范围时，需要回表。InnoDB 的二级索引只能快速定位主键值，故若查询还需其他列数据，就要通过主键到聚簇索引读取完整行，这就是回表过程 。 综上，优化索引以让查询满足覆盖索引条件，可避免回表，提高效率 ；否则回表成为瓶颈。我们下面通过示例演示这两种情况。\n覆盖索引与回表对比 覆盖索引：若查询的所有列都包含在一个索引中（包括二级索引列及隐含的主键列），则查询可以完全从索引读取所需数据，无需访问表数据 。EXPLAIN 中 Extra 字段会显示 Using index 。这意味着 MySQL 引擎使用索引即可得到结果，无须读取表行。 回表：若查询涉及的列不全在索引中，则即使可以使用索引检索行主键，也必须通过主键到聚簇索引读出其余列，这称为回表。回表需要额外的随机 I/O 和时间 ，性能低于纯索引扫描。 我们以实际例子说明。假设有如下表和索引：\n1 2 3 4 5 6 7 8 CREATE TABLE demo ( id INT PRIMARY KEY, name VARCHAR(20), dept VARCHAR(20), salary INT, age INT, INDEX idx_name_dept (name, dept, salary) ) ENGINE=InnoDB; 回表示例：查询非覆盖情况需要回表。比如查询 name=\u0026hellip;、dept=\u0026hellip; 和 age（未加索引列）： 1 2 3 EXPLAIN SELECT id, name, dept, age FROM demo WHERE name=\u0026#39;Alice\u0026#39; AND dept=\u0026#39;Sales\u0026#39;; 说明：索引 idx_name_dept(name,dept,salary) 可以用于查找符合 name 和 dept 的行，但列 age 不在索引中，因此每匹配到一条记录后，需要通过主键 id 回表读取 age。对应的执行计划可能如下（示例）：\nid select_type table type possible_keys key key_len ref rows Extra 1 SIMPLE demo ref idx_name_dept idx_name_dept 256 const,const 10 Using where 解释：type=ref 表示用二级索引范围查找；key=idx_name_dept；Extra 显示 Using where，表明服务器层需要再执行 WHERE 条件过滤（尽管在此例中过滤完毕后还需回表检索列）。注意由于 age 不在索引中，这不是覆盖索引查询，因此 MySQL 会回表获取完整行才能返回 age。回表导致额外开销 。 覆盖索引示例：若查询列都在索引中，则不回表。继续上述表结构，如果查询只取 salary： 1 2 3 EXPLAIN SELECT name, dept, salary FROM demo WHERE name=\u0026#39;Alice\u0026#39; AND dept=\u0026#39;Sales\u0026#39;; 因为 name, dept, salary 恰好是索引 idx_name_dept 的列，且 id（主键）会隐式包含在每个索引叶子节点中，故此查询的所有列（name, dept, salary 及聚簇主键 id）均可从索引中取得，无须回表。执行计划示例：\nid select_type table type possible_keys key key_len ref rows Extra 1 SIMPLE demo ref idx_name_dept idx_name_dept 256 const,const 5 Using index 解释：Extra 显示 Using index ，表示此查询可以由索引覆盖执行，不需要访问表（所谓覆盖索引）。通过覆盖索引，避免了读聚簇索引的随机 I/O ，查询效率更高。 下表总结覆盖索引与回表的区别：\n特性 覆盖索引 回表 列需求 查询列完全包含在索引中（加上主键列） 查询列不全在索引中 I/O 行为 直接从索引树读取数据，无需访问表 需要先查索引取主键值，再到聚簇索引读行 EXPLAIN Extra Using index 通常含 Using where（或 Using index condition） 性能 最优（避免额外随机 I/O） 较差（额外随机 I/O 和 CPU 过滤开销） 适用场景 查询列固定且已编入索引；WHERE/GROUP BY/ORDER BY 列索引前缀 查询列超出索引；无覆盖索引需补列数据 以上示例说明：构造合理的联合索引，尽量让查询成为覆盖索引（如将常用的过滤列和返回列包含进索引），可以显著提高查询性能 。\n索引条件下推（ICP）的作用与使用 **索引条件下推（Index Condition Pushdown, ICP）**是 MySQL 5.6+ 为 InnoDB 和 MyISAM 二级索引设计的优化功能 。当使用二级索引检索行时，如果查询中存在多个条件，其中只有部分条件能利用索引列，则 MySQL 默认会把满足索引的那部分条件下推到存储引擎中进行过滤，减少回表次数 。具体行为如下：\n无 ICP 时：存储引擎按索引查到每一行对应的主键值并回表，然后服务器层再对返回的行执行全部剩余的 WHERE 条件过滤 。这意味着即使某些条件可在索引层判断，也要等回表后才能筛选，导致大量不必要的行读取。 启用 ICP 后：存储引擎在扫描索引时，将能够在索引列上判断的条件提前执行：只有在索引条件也满足的情况下，才回表读取该行 。对于索引层无法判断（非索引列或函数列）的条件，待回表后再在服务器层过滤。 举例说明：继续上述 demo 表，如果我们对 idx_name_dept(name,dept,salary) 既有索引，又对列 age 没有索引，则考虑如下查询：\n1 2 3 EXPLAIN SELECT id, name, dept, salary FROM demo WHERE name=\u0026#39;Alice\u0026#39; AND salary \u0026gt; 50000 AND age \u0026lt; 30; 索引 idx_name_dept 可用于 name=\u0026lsquo;Alice\u0026rsquo;（精确匹配）和 salary \u0026gt; 50000（范围匹配），但 age \u0026lt; 30 无索引参与。启用 ICP 时，存储引擎会首先根据 name 和 salary 在索引上查找主键，然后在索引层判断 age \u0026lt; 30 条件。如果 age 不在索引中，就不能在索引层判断，所以 age \u0026lt; 30 在索引层跳过，而在回表后由服务器层执行。 EXPLAIN 输出示例可能为： id select_type table type possible_keys key key_len ref rows Extra 1 SIMPLE demo range idx_name_dept idx_name_dept 258 const,const 15 Using index condition 这里 Extra 显示 Using index condition ，意味着 ICP 被使用。根据官方文档：“Using index condition 说明索引条件下推已经生效 ”。此时 MySQL 引擎会先在索引层应用 name=\u0026lsquo;Alice\u0026rsquo; AND salary\u0026gt;50000 过滤匹配的行，只有满足这些条件的索引条目才会被读取主键，回表到主表后服务器再检查 age\u0026lt;30。由此避免了对不满足前两条件的行进行回表，从而降低了 I/O 和 CPU 开销 。如果关闭 ICP（例如执行 SET optimizer_switch=\u0026lsquo;index_condition_pushdown=off\u0026rsquo;;），则 EXPLAIN 中 Extra 只会显示 Using where ，意味着索引层只用 name=\u0026lsquo;Alice\u0026rsquo; 等定位，查询返回的所有行都要回表并在服务器层再做 age\u0026lt;30 筛选，效率较低。\n需要注意的是，ICP 只对二级索引有效 。InnoDB 的聚簇主键检索时，整行已经加载到缓冲区，故对主键索引应用 ICP 无意义。ICP 条件要求如文档所述：访问方法为 range/ref/eq_ref/ref_or_null，且表需要访问完整行 。此外，ICP 不能下推子查询和存储函数等复杂条件 。\n以下示例演示 ICP 在 EXPLAIN 输出中的表现：假设 demo 表中数据足够多，我们执行上面查询（启用 ICP）：\n1 2 3 4 -- 启用ICP（默认开启），执行查询 EXPLAIN SELECT id, name, dept, salary FROM demo WHERE name=\u0026#39;Alice\u0026#39; AND salary \u0026gt; 50000 AND age \u0026lt; 30\\G 执行计划解析（示例）：\nkey=idx_name_dept：使用复合索引。 type=range：范围查找（因为对 salary \u0026gt; 50000）。 Extra=Using index condition ：说明引擎在索引层先应用了 name 和 salary 条件过滤，后续再回表检查 age 条件。 总结：ICP 在执行流程中的位置在索引查找后、回表前，能有效减少回表次数 。在 EXPLAIN 输出中，Using index condition 表示 ICP 生效；若额外出现 Using where，说明仍有非索引列的过滤条件需在服务器层处理 。例如上例的 age\u0026lt;30 就因未在索引而留在服务器层。\n索引相关概念对比一览 下表对照了本文讨论的关键概念及其适用场景：\n概念 触发条件 优势 EXPLAIN 标志 覆盖索引 查询所需列全包含在某索引（包括主键） 避免回表，减少随机 IO，提高查询效率 Using index 回表 查询列超出所用索引时 无 Using where（无 Using index） 索引下推 (ICP) 复合索引扫描时，有部分条件非索引列 索引层提前过滤，减少回表次数 Using index condition 排序 (filesort) ORDER BY 列未按索引顺序（或无合适索引） 数据量小或内存足够时开销可接受，小表也可忽略 Using filesort 表中各项分别对应覆盖索引利用索引免回表的优势 ；回表的触发（非覆盖）场景；ICP 适用情况与好处 ；以及排序相关的指标 Using filesort 表示排序在内存或外部进行 。\n实战优化建议（Best Practices） 设计覆盖索引：分析业务查询，针对常见的 WHERE、GROUP BY、ORDER BY 和 SELECT 列，设计联合索引，使其尽量覆盖这些查询。覆盖索引能大幅减少随机读，从而提速 。 注意索引列顺序：联合索引中将筛选条件最常用的列放在最左侧；只有左前缀匹配才能有效使用索引。复合索引建列顺序应符合查询逻辑，切勿盲目多列索引 。 合理限制 SELECT 列：避免 SELECT *，只选取必要列。若查询返回较少列，能降低回表概率，或可让索引覆盖更多查询需求 。 利用 EXPLAIN 分析执行计划：定期检查慢查询或关键查询的 EXPLAIN 输出，关注 type、key、rows、Extra 等字段。若出现 Using filesort 或全表扫描（type=ALL）等提示，要考虑创建索引或调整查询逻辑。 启用 ICP（默认开启）：对于范围查询或多条件过滤，EXPLAIN 出现 Using index condition 时说明 ICP 正在减少回表。确认 MySQL 版本 ≥5.6，保持 optimizer_switch 中 index_condition_pushdown 为 on。注意ICP只对二级索引生效 。 监控主键设计：主键越长，二级索引叶子节点也会越大（因为每条叶子记录都要存储主键），影响缓存命中和 I/O。避免在高频使用的表上构建过长的主键 。 定期统计与优化：使用 ANALYZE TABLE 保持统计信息准确，确保优化器能正确估算成本。对极少更新的大表，可考虑手动分析。采用 OPTIMIZE TABLE 或在线重建表，以消除碎片、缩减数据页。 通过上述策略，配合对 EXPLAIN 输出的深入理解 和指标观察，可最大化发挥 InnoDB 索引的性能优势，避免不必要的回表和排序开销，从而优化查询性能。\n","date":"2025-08-04T00:00:00Z","permalink":"https://cyber-blog.github.io/p/database-index/","title":"MySQL InnoDB 索引机制与优化"},{"content":"深入理解 Java 内存模型（JMM）与锁机制 一、Java 内存模型（JMM） ​\tJava 内存模型的设计目的是解决多线程下共享内存的一致性问题。不同于强一致性硬件模型，现代 CPU 和编译器会对内存操作进行各种优化和重排序 。在没有规范约束时，不同处理器的缓存和乱序执行会导致线程间看到的共享变量值不一致，严重影响并发正确性 。Java 提供了跨平台的内存模型（JMM）来定义一套规则，使得 volatile、synchronized 等同步原语能在任何硬件架构上实现一致语义，保障“Write Once, Run Anywhere”原则 。\n​\tJava 内存模型示意：每个线程都有自己的工作内存（Local Memory），通过主内存（Main Memory）进行交互 。线程 A 要把修改写入共享变量，必须先把工作内存中的更新刷新到主内存；线程 B 要读取，则必须从主内存将值读取到其工作内存后才使用。JMM 通过控制这个读-写流程，为 Java 程序提供了内存可见性的保证 。下面示例说明了没有同步时指令重排序可能带来的问题：\n1 2 3 4 5 6 7 class ReorderExample { int x = 0, y = 0; // 线程 A 执行 void writer() { x = 1; y = 2; } // 线程 B 执行 void reader() { int a = y; int b = x; } } 若线程 B 在 reader() 中首先读到 y == 2，按程序原序理应读到 x == 1；但由于编译器/CPU 可重排序写操作，上述代码可能实际执行为 y=2 后才 x=1，导致 B 看到 a==2 而 b==0 。JMM 引入 happens-before 规则来避免这类问题。\nHappens-before 原则： 在 JMM 中，若操作 A happens-before 操作 B，则操作 A 的结果对操作 B 可见，并且执行 A 的语义在语句顺序上排在 B 之前 。常见的具体规则包括：\n程序顺序规则（Program Order）： 同一线程内，按代码顺序的每个操作 happens-before 于该线程中任意后续操作 。\n监视器锁规则（Monitor Lock）： 对一个锁的 释放（unlock）操作 happens-before 于随后对同一锁的 获取（lock）操作 。这保证了进入同步块后可见之前在该锁保护下的修改。\nvolatile 变量规则： 对一个 volatile 域的写操作 happens-before 于任意后续对同一 volatile 域的读操作 。这意味着写入 volatile 的值对后续读取立即可见，同时禁止了该写操作与读操作间的重排序。\n线程启动规则： 如果线程 A 调用了 Thread.start() 启动线程 B，则线程 A 在 start() 之后的操作 happens-before 线程 B 中的任何操作 。\n线程终结规则： 如果线程 A 调用了 Thread.join() 等待线程 B 完成，则线程 B 中的任意操作 happens-before 线程 A 从 join() 返回之后的操作 。\n传递性： 如果 A happens-before B 且 B happens-before C，则 A happens-before C 。\n以上规则保证了关键同步操作之间的先后关系。例如，释放锁的写操作通过内存屏障将缓存数据刷新到主内存，随后的获取锁操作则从主内存重新读取，从而保证了锁保护块内存可见性；对 volatile 字段的写也会立刻被刷新，而读操作则从主内存读取最新值 。\n可见性、有序性、原子性问题 JMM 主要解决可见性和有序性问题。可见性 指一个线程对共享变量的修改是否能及时被其他线程看到；_有序性_指在多线程环境下保证特定操作的执行顺序。通过 happens-before 规则，JMM 能保证关键操作的顺序性和可见性。原子性问题方面，JMM 保证对基本类型的单次读/写（除 long/double 在旧版本中）的操作具有原子性，但复合操作（如 i++）并非原子。若需保证原子性，就必须使用同步手段或原子类。Java 提供了 java.util.concurrent.atomic 包中的原子类（如 AtomicInteger），它们使用 CAS + volatile 来实现线程安全的原子更新 。\n1 2 3 4 5 6 7 8 9 10 11 12 // 示例：使用 volatile 保证可见性 class VolatileExample { int a = 0; // 普通变量 volatile int flag = 0; void writer() { a = 10; flag = 1; } void reader() { if (flag == 1) { // 如果读到 flag=1，则能保证看到 a=10 System.out.println(a); } } } 在上例中，flag 是 volatile 字段，写线程先写 a=10 再写 flag=1，读线程读取到 flag==1 时，通过 happens-before 规则可见先前写的 a 值 。若去掉 volatile，这种可见性就不能被保障，可能读到过时的 a 值。\n二、HotSpot JVM 的锁升级机制 Java 的 synchronized 锁在 HotSpot JVM 中有多种状态优化，以减少无竞争情况下的加锁开销 。JDK6 之后，锁主要有四种状态：无锁、偏向锁、轻量级锁、重量级锁 。对象头（Object Header）中的 Mark Word 用于标记当前锁状态和相关指针（如线程 ID、锁记录指针等） 。\n无锁状态： 对象未被任何线程锁定。Mark Word 保存的是对象哈希码和年龄等信息 。\n偏向锁： 适用于对象只被一个线程反复访问的场景。第一次加锁时，JVM 会将该线程的 ID 写入对象头，之后该线程可无竞争地多次进入同步块而无需 CAS 自旋 。如果其他线程竞争此锁（或出现必须撤销偏向的情况），JVM 会撤销偏向锁并升级为轻量级锁 。\n轻量级锁（自旋锁）： 当偏向锁被撤销后，或并发程度较低时使用。线程获取锁时通过 CAS 将对象头指向自己栈帧中的 Lock Record（锁记录），若失败则在用户态进行自旋重试 。轻量级锁可以在短期竞争中避免进入内核阻塞，提升性能。\n重量级锁（监视器锁）： 当自旋仍无法获得锁（如自旋次数超过阈值或阻塞时间过长）时，JVM 会将锁升级为重量级锁。此时线程挂起，转入内核态由操作系统调度 。重量级锁的获取成本较高，但适用于高度竞争场景。\n下面的示意图展示了典型的锁状态转换流程：\n对象锁状态转换图：HotSpot JVM 中对象锁的状态升级通常按无锁→偏向锁→轻量级锁→重量级锁的顺序进行 。例如，新对象创建后处于无锁；第一次线程访问若开启偏向锁，则进入偏向锁状态；当另一线程竞争时，偏向锁被撤销并升级为轻量级锁；若自旋多次仍未获取锁，则进一步膨胀为重量级锁 。\n对象头与 Mark Word 每个对象头（Object Header）包含 Mark Word 和类型指针（klass pointer）等元数据。Mark Word 存储对象运行时数据：平时包含对象的哈希码、GC 年龄等 ；加锁时则被修改为锁标记和相关指针 。下表示意了 Mark Word 的不同锁态编码：\n锁状态 Mark Word 内容 低位标志位 无锁 23bit 保存哈希码等，age，最低2位为 01 01 偏向锁 线程ID、Epoch、age，高位标志位为 1，最低2位 01 01 轻量级锁 指向栈上 Lock Record 的指针，最低2位 00 00 重量级锁 指向 Monitor 对象的指针，最低2位 10 10 （上述表格源自 HotSpot 对象头结构，简化说明）\n锁升级触发条件与过程 锁的升级（膨胀）通常由以下条件触发：\n无锁→偏向锁： 在对象首次被线程访问时，若启用偏向锁，JVM 会将线程 ID 写入对象头标记为偏向 。\n偏向锁→轻量级锁： 若在偏向锁状态下有第二线程尝试获取该锁，则检测到竞争，偏向锁被撤销并升级为轻量级锁 。撤销时需要安全点（safepoint），此过程将对象头转为指向申请锁的线程的 Lock Record，实现 CAS 自旋锁 。\n轻量级锁→重量级锁： 如果在轻量级锁状态下竞争仍然激烈（例如自旋等待超过一定次数或线程挂起等待），JVM 会膨胀为重量级锁 。一般来说，超过处理器核心数的一半或固定自旋阈值时，就会升级为重量级锁 。\n下面是锁升级过程的简单 Java 示例：\n1 2 3 4 5 6 class Counter { private int count = 0; public synchronized void syncIncrement() { count++; } } 上例中，syncIncrement 初次被线程调用时可能处于无锁或偏向锁（只记录线程ID），当另一个线程并发调用时，偏向锁将撤销并切换到轻量级锁；若后续竞争继续，可能升级为重量级锁。读者可以使用 -XX:+PrintSafepointStatistics 等 JVM 参数或 Profiling 工具观察实际执行过程。\n偏向锁的撤销与回退 偏向锁撤销过程复杂：当检测到竞争需要撤销时，JVM 必须在所有线程停止（safepoint）时完成撤销操作 。撤销时一般会将对象头恢复为无锁状态或直接升级为轻量级锁状态 。常见触发情形包括：当前线程调用 Object.hashCode()、执行 wait/notify/monitorexit、JNI 的 MonitorEnter/MonitorExit、Unsafe.monitorEnter 等，都可能强制撤销偏向锁并转为重量级锁 。例如，在撤销偏向锁后，如果发现原偏向线程还在同步块中，则转换为轻量级锁；若偏向线程已经退出同步块，则可将其恢复为无锁或轻量级锁 。\n需要注意的是，锁一旦升级为重量级锁后不会自动回退为轻量级或偏向锁；只有在类级别触发批量重偏向（rebias）或关闭偏向锁时，未来新对象才能再次使用偏向锁 。在需要性能分析时，可使用 JVM 提供的工具（如 jstack、Java Flight Recorder、JVMTI 接口或 JVisualVM 的锁监控功能）来查看锁竞争和升级情况，直观了解不同锁状态下的行为。\n三、各种锁的关系与适用场景 Java 提供了多种锁机制：内置的 synchronized 关键字、java.util.concurrent.locks 包下的锁（如 ReentrantLock、ReentrantReadWriteLock）以及原子操作（CAS）等。它们之间的关系和适用场景如下：\nSynchronized 与 ReentrantLock： 两者本质都是可重入的互斥锁。synchronized 是 Java 关键字，在 JVM 层面通过监视器（monitorenter/monitorexit）实现，不需要显式释放锁，执行完同步块后自动解锁；而 ReentrantLock 是 JDK 1.5 引入的锁实现，需要手动 lock() / unlock() 。从底层实现看，synchronized 利用了 JDK6+ 的锁优化（偏向锁、自旋、重量级锁等），而 ReentrantLock 基于 AQS 框架，使用 CAS + 自旋和队列实现 。 ReentrantLock 支持公平锁和非公平锁模式（构造函数中可选），而 synchronized 只支持非公平调度 。\nReentrantLock 可以响应中断（ lockInterruptibly() ）或尝试超时获取锁（ tryLock(long, TimeUnit) ），而 synchronized 不支持线程中断获取锁 。\nReentrantLock 支持绑定 Condition（条件队列），可实现更灵活的等待/通知，而 synchronized 只能使用 Object.wait()/notify() 机制 。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // synchronized 示例 public synchronized void incrementSync() { // 自动加锁和释放 } // ReentrantLock 示例 private Lock lock = new ReentrantLock(true); // true 表示公平锁 public void incrementLock() { lock.lock(); try { // 需要手动释放锁 } finally { lock.unlock(); } } 在高并发场景下，ReentrantLock 提供了更灵活的特性，因此通常推荐使用 。例如，可以选择非公平模式以提高吞吐量，也可以在需要时使用超时或可中断锁。但 synchronized 使用更简单，JVM 优化极好，在无竞争或轻度竞争时性能也非常高 。 非公平锁为什么比公平锁吞吐量高：\n非公平锁减少了线程切换： 新线程直接竞争成功就进入临界区，少一次上下文切换。 公平锁需要： 判断自己是否是队列头（多一次检查）。 如果不是，就必须等待——导致线程更多地陷入等待/唤醒，增加系统负担。 非公平锁利用随机性：系统调度新线程拿到锁后，可能比排队线程更快执行完，从而总体提高吞吐量。 乐观锁 vs 悲观锁（CAS vs 同步锁）： 悲观锁假设会发生冲突，每次访问共享数据前都会上锁（如 synchronized、ReentrantLock） ；乐观锁则假设冲突很少，仅在提交时通过某种机制验证（如 CAS）是否成功 。Java 的原子类（如 AtomicInteger）即使用 CAS + volatile 来实现非阻塞的乐观并发。 1 2 3 // CAS 乐观锁示例：AtomicInteger AtomicInteger counter = new AtomicInteger(0); counter.incrementAndGet(); // 底层使用 CAS 保证原子性 在 AtomicInteger 内部，核心实现依赖于 sun.misc.Unsafe 类（JDK 9+ 是 jdk.internal.misc.Unsafe）。\nUnsafe 提供了底层的 compareAndSwapInt 等方法。\n来看一个简化版伪代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class AtomicInteger { private volatile int value; public final int incrementAndGet() { for (;;) { // 无限循环直到成功 int current = get(); int next = current + 1; if (compareAndSet(current, next)) return next; } } public final boolean compareAndSet(int expect, int update) { // compare and swap 缩写 CAS return unsafe.compareAndSwapInt(this, valueOffset, expect, update); } } 具体过程：\n当线程 A 调用 incrementAndGet() 时： 会先 get() 当前值：这里要读取 value 另一个线程 B 也可能修改 value 因为 value 是 volatile： 保证了线程 A 读取 value 时，一定能读到主内存中最新的值（而不是线程本地工作内存的旧值） 当线程 B 更新 value 后，立即对线程 A 可见 然后 CAS 操作： compareAndSwapInt 在比较时，也能保证看到最新的值 为什么保证原子性:\ncompareAndSwapInt 是 CPU 的原子指令，不可被打断 无论多少线程同时执行，只有一个线程的 CAS 能成功 失败的线程检测到值变化，就重试，直到成功 volatile 保证 value 的读取永远是最新值 CAS 的优点是不涉及操作系统阻塞，适合竞争不激烈、多核场景下的高吞吐量操作 ；缺点是循环重试可能导致 CPU 空转，并存在 ABA 问题。悲观锁则通过阻塞等待保证安全，多用于写操作频繁的场景 。实际应用中，可视情况混合使用：读多写少时可采用读写锁，更新计数等操作可使用 LongAdder/AtomicInteger 等原子类。\n自旋锁与阻塞锁： 轻量级锁本质是一种自旋锁，它让等待的线程在用户态循环尝试获取锁而不进入内核阻塞 。自旋锁适用于锁持有时间极短的场景，可减少线程切换开销，但若持锁时间较长或线程数过多，会浪费大量 CPU。与之相对，重量级锁采用阻塞（park）机制，适合长时间等待但会有上下文切换开销。\n读写锁（ReentrantReadWriteLock）： 读写锁维护一对锁，一个读锁一个写锁。多个读线程可同时持有读锁（共享），而写锁是独占的。持有写锁的线程可以降级为读锁（即先获取写锁再获取读锁，然后释放写锁） 。读写锁通常用于读多写少的场景，可显著提高并发性能。但若写操作很多，则读写锁的开销可能高于普通互斥锁，因为需要维护更多状态。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 // ReentrantReadWriteLock 示例：读多写少场景 ReentrantReadWriteLock rwLock = new ReentrantReadWriteLock(); Lock readLock = rwLock.readLock(); Lock writeLock = rwLock.writeLock(); // 读方法 public void doRead() { readLock.lock(); try { // 读共享资源 } finally { readLock.unlock(); } } // 写方法 public void doWrite() { writeLock.lock(); try { // 写共享资源 } finally { writeLock.unlock(); } } 公平锁 vs 非公平锁： 公平锁（new ReentrantLock(true)）按照线程请求顺序授予锁，避免饥饿；非公平锁则可能让新线程“插队”抢锁，从而通常具有更高的吞吐量。synchronized 以及默认的 ReentrantLock 为非公平策略。高并发场景下，非公平锁性能更优，但在需要严格顺序时应使用公平锁 。 综上所述，在高并发/低延迟系统中，选择合适的锁策略非常关键：volatile 适合于简单的状态标志；原子类（CAS）适合于无锁计数；synchronized 简洁高效、自动释放，适合锁竞争较轻或场景简单；ReentrantLock 则提供更多特性和可调性；读写锁适用于读多写少场景；自旋锁（如轻量级锁）适合短时锁定，否则应切换到阻塞锁避免资源浪费。具体使用时应权衡吞吐量与延迟开销，比如在超低延迟场合尽量避免阻塞、使用非公平锁；在公平性要求高的场合选择公平锁或其他协调方式。\n参考资料： 对于 JMM 和锁机制的深入原理，可参考 JSR-133 提案和 OpenJDK 文档，以及相关文章 等。上述讨论结合实际例子和 HotSpot 实现机制。\n","date":"2025-07-12T00:00:00Z","permalink":"https://cyber-blog.github.io/p/jmm/","title":"JMM"},{"content":"在分布式系统的设计与实现中，如何有效地管理数据分布、处理节点变动，并保持系统的高可用性与负载均衡，一直是一个核心挑战。随着云计算和微服务架构的广泛应用，传统的哈希方法已显得力不从心，特别是在节点动态变动的情况下。一致性哈希（Consistent Hashing）算法作为一种创新的解决方案，因其显著的优势而被广泛应用于分布式缓存、负载均衡、分布式存储等场景中。\n本文将深度解析一致性哈希算法的工作原理、应用优势与局限性，并探讨它在实际应用中的挑战及潜在的优化方向。\n一、哈希算法的传统挑战与一致性哈希的提出 1.1 传统哈希算法的局限性 哈希算法在分布式系统中的主要目的是将数据均匀地分配到多个节点上，从而保证数据的高可用性与系统的负载均衡。然而，传统哈希算法在处理节点变动时存在几个显著的缺点：\n节点变动时的数据重分配：在传统哈希算法中，每次节点增加或减少时，数据需要重新分配。假设我们有N个节点，每个节点承载M个数据，节点数量变化时，可能需要重新映射大量数据。这种数据迁移不仅增加了系统的负担，还会导致性能下降。\n负载不均问题：节点的负载不均可能导致某些节点超负荷运行，而其他节点则空闲，影响系统的响应速度和稳定性。\n节点动态性问题：分布式系统中，节点的数量通常是动态的，如何高效地处理节点的增删，成为了分布式架构中的一个挑战。\n1.2 一致性哈希的提出 为了应对这些挑战，David Karger 等人于1997年提出了一致性哈希算法。该算法旨在通过减少节点变动时的数据迁移，降低系统的负担，实现平滑的数据迁移和高效的负载均衡。其核心思想是将整个哈希空间映射为一个环，每个节点和数据项都通过哈希函数映射到环上，数据项总是被存储到第一个顺时针方向上哈希值大于或等于数据项哈希值的节点上。\n二、一致性哈希算法的核心原理 一致性哈希的核心思想通过哈希环来实现节点与数据的映射，从而减少节点增减时的数据迁移量。\n2.1 哈希环与节点分布 一致性哈希的工作原理基于一个哈希环，将哈希空间视为一个大环（例如0到2^32-1的整数区间）。每个节点和数据项都通过哈希函数映射到这个环上。数据项会被存储在哈希值大于或等于数据项哈希值的第一个节点上。节点的哈希值可以通过hash(node)计算得到，而数据项的哈希值则通过hash(data)来计算。\n节点映射：每个节点被映射到哈希环上一个位置，假设节点的哈希值为hash(node)，则节点在哈希环上的位置就是hash(node) % 2^32。\n数据映射：每个数据项（例如用户请求的key）会通过hash(data)计算其哈希值，然后顺时针方向找到第一个哈希值大于或等于该数据项哈希值的节点，将数据存储到该节点。\n2.2 节点增减的处理 一致性哈希的优势之一就是对节点增删的高效处理。传统哈希方法在节点变动时需要重新分配所有数据，但一致性哈希只会影响与新增或删除节点相邻的数据项。\n节点增加：当新增一个节点时，只有距离其哈希值最近的数据项会被迁移到该节点。其他节点上的数据不受影响，极大地减少了数据的迁移量。\n节点删除：当一个节点被删除时，它所存储的数据会由顺时针方向上的下一个节点接管，迁移的数据量也被限制在相邻节点之间。\n2.3 虚拟节点的引入 为了避免哈希环上节点分布不均带来的负载不均问题，一致性哈希引入了虚拟节点的概念。每个物理节点可以映射到多个虚拟节点，这样即使物理节点数量较少，虚拟节点的数量足够多，依然能保证数据的均匀分布。\n虚拟节点映射：每个物理节点通过哈希函数生成多个虚拟节点，每个虚拟节点占有哈希环上的一个位置。虚拟节点可以帮助平衡负载，避免某些节点因为物理节点数量较少而导致负载不均。 三、一致性哈希的优缺点 3.1 优点 减少数据迁移：一致性哈希通过节点的局部调整，显著减少了数据迁移量。例如，新增节点时，只需迁移一小部分数据，避免了全量数据迁移的开销。\n负载均衡：虚拟节点的引入使得数据的分布更加均匀，能够有效解决节点负载不均的问题。\n高扩展性和容错性：一致性哈希能够平滑处理节点的增减，使得系统具有很高的扩展性。在节点失败时，数据的丢失范围也较小，具备较强的容错能力。\n3.2 缺点 虚拟节点的管理复杂性：尽管虚拟节点在负载均衡方面发挥了重要作用，但它们的管理需要额外的计算与存储。特别是在节点频繁变化时，如何合理分配和管理虚拟节点成为了一个挑战。\n数据热点问题：尽管一致性哈希减少了数据的迁移，但在某些情况下，特定节点的负载可能仍然过高，导致出现热点问题。这个问题可以通过更加智能的虚拟节点管理来解决，但它仍然是一个需要优化的方向。\n四、一致性哈希与其他算法的比较 一致性哈希算法相较于其他常见的哈希或负载均衡算法，在节点变动处理上的效率显著更高。以下是几种常见哈希算法的比较：\n随机哈希：随机哈希将数据随机分配到多个节点上，无法保证数据的均匀分布，并且在节点增减时会产生大量数据迁移。\n目标哈希：目标哈希通过节点与数据之间的目标值映射来进行负载均衡。尽管其在节点变动时的表现不如一致性哈希高效，但它在一些场景下仍有应用价值。\n一致性哈希通过哈希环和虚拟节点的引入，优化了数据迁移问题，实现了更高效的数据分配和负载均衡。\n五、一致性哈希的实际应用案例 5.1 分布式缓存 在分布式缓存系统中，一致性哈希被广泛应用。例如，Memcached 和 Redis 等缓存系统使用一致性哈希来优化节点增减时的数据迁移。当缓存节点的数量发生变化时，仅需迁移相邻的数据，而非全量迁移，极大地降低了成本。\n5.2 分布式存储与数据库 分布式数据库如Amazon DynamoDB 和 Cassandra 使用一致性哈希来实现高效的分布式存储。通过一致性哈希，数据分布可以平滑地处理节点的增删，减少了扩展过程中的数据迁移和负载不均问题。\n5.3 负载均衡 在Nginx 和 HAProxy 等负载均衡器中，一致性哈希通过客户端请求特征（如IP地址）实现请求分配。这样，当新增后端服务器时，仅会影响与新节点相邻的请求，减少了重新分配的开销。\n六、未来发展与优化 尽管一致性哈希已经在多个领域取得了成功，但仍然有进一步的优化空间。以下是几个潜在的优化方向：\n虚拟节点的自适应管理：可以引入自适应算法，根据节点负载和变动情况动态调整虚拟节点的数量与分布。\n混合算法：结合一致性哈希与其他负载均衡算法（如目标哈希、最小负载等），通过智能调度进一步提升数据分布的均衡性和系统的效率。\n七、总结 一致性哈希算法为分布式系统提供了一个高效、低成本的数据分配和负载均衡解决方案。通过哈希环和虚拟节点的创新设计，它能够在节点增减时显著减少数据迁移，保证系统的高可用性和扩展性。尽管存在虚拟节点管理和数据热点等挑战，但一致性哈希在分布式缓存、存储、负载均衡等领域的广泛应用，已经证明了它的重要性。随着技术的发展，一致性哈希算法有望进一步优化，提升其在更复杂分布式环境下的适应能力。\n","date":"2025-07-08T00:00:00Z","permalink":"https://cyber-blog.github.io/p/consistent-hash/","title":"一致性哈希的简单介绍"},{"content":"分布式锁的实现原理与方式详解 一、基于 Redis 的分布式锁实现 Redis 是目前使用最广泛的分布式锁方案之一，通常通过如下命令实现：\n1 SET lock_key unique_id NX PX 30000 参数说明 lock_key：键（Key），表示分布式锁在 Redis 中的标识名；\nunique_id：值（Value），代表当前请求加锁的客户端唯一标识（例如 UUID + ThreadID），用于确保释放锁时身份验证；\nNX：表示仅在键不存在时才进行设置，防止覆盖其他客户端已持有的锁；\nPX 30000：设置键的过期时间为 30 秒，防止因客户端异常退出而产生死锁。\n加锁失败的返回值 如果 lock_key 已存在，说明已有其他线程持有锁，命令将返回 null，表示加锁失败。\n解锁 Lua 脚本示例 1 2 3 4 5 if redis.call(\u0026#34;get\u0026#34;, KEYS[1]) == ARGV[1] then return redis.call(\u0026#34;del\u0026#34;, KEYS[1]) else return 0 end 该脚本判断当前请求者是否为锁的持有者（通过比对 unique_id），只有持有者才能安全释放锁。\n二、基于 Redisson 的分布式锁实现 Redisson 是 Redis 官方推荐的 Java 客户端之一，封装了丰富的分布式对象，其中对分布式锁的支持较为完整。\n主要特性 支持可重入锁（同一线程可多次加锁）；\nWatchdog 自动续期机制，防止锁自动过期；\nLua 脚本保障操作原子性；\n支持公平锁、读写锁、联锁等高级功能。\n加锁 Lua 脚本 1 2 3 4 5 6 7 8 9 10 11 12 13 if (redis.call(\u0026#39;exists\u0026#39;, KEYS[1]) == 0) then redis.call(\u0026#39;hset\u0026#39;, KEYS[1], ARGV[2], 1); redis.call(\u0026#39;pexpire\u0026#39;, KEYS[1], ARGV[1]); return nil; end; if (redis.call(\u0026#39;hexists\u0026#39;, KEYS[1], ARGV[2]) == 1) then redis.call(\u0026#39;hincrby\u0026#39;, KEYS[1], ARGV[2], 1); redis.call(\u0026#39;pexpire\u0026#39;, KEYS[1], ARGV[1]); return nil; end; return redis.call(\u0026#39;pttl\u0026#39;, KEYS[1]); 使用 Hash 存储锁持有者及重入次数；\nARGV[2] 为唯一标识（如线程 ID）；\n若当前线程已持有锁，则重入计数 +1 并续期。\n解锁 Lua 脚本 1 2 3 4 5 6 7 8 9 10 11 12 if (redis.call(\u0026#39;hexists\u0026#39;, KEYS[1], ARGV[1]) == 0) then return nil; end; local counter = redis.call(\u0026#39;hincrby\u0026#39;, KEYS[1], ARGV[1], -1); if (counter \u0026gt; 0) then redis.call(\u0026#39;pexpire\u0026#39;, KEYS[1], ARGV[2]); return 0; else redis.call(\u0026#39;del\u0026#39;, KEYS[1]); return 1; end; 减少持有者的重入计数；\n若减为 0，释放锁。\nWatchdog 自动续期机制 默认锁的 TTL 为 30 秒；\n若未显式设置 TTL，Redisson 会启动“看门狗”线程，每隔 10 秒自动续期，直至业务逻辑完成；\n避免因执行时间较长导致锁意外过期。\nJava 使用示例 1 2 3 4 5 6 7 8 9 10 RLock lock = redissonClient.getLock(\u0026#34;myLock\u0026#34;); try { if (lock.tryLock(5, 30, TimeUnit.SECONDS)) { // 执行业务逻辑 } } finally { if (lock.isHeldByCurrentThread()) { lock.unlock(); } } 三、Lua 脚本为何具备原子性？ 核心机制 Redis 执行 Lua 脚本时具备以下特性：\nRedis 使用单线程模型；\nEVAL 命令整体作为一个事务执行；\n执行脚本期间不会响应其他客户端请求；\n脚本内部多个 Redis 命令会被整体提交执行，无法被打断。\n因此，在 Redis 中执行 Lua 脚本具有天然的原子性，能够有效避免竞态条件。\n四、分布式锁的其他实现方式 1. 基于数据库（如 MySQL/PostgreSQL） 实现方式 利用唯一约束：INSERT INTO lock_table (key) VALUES ('lockKey')；\n或使用悲观锁：SELECT ... FOR UPDATE。\n优缺点 优点：实现简单、依赖较少、与业务数据库一致性强；\n缺点：性能较差，不适合高并发场景，可能引发死锁或阻塞。\n2. 基于 ZooKeeper 的分布式锁 实现方式 利用临时顺序节点（Ephemeral Sequential）；\n所有客户端在某节点下创建顺序子节点；\n排序最小的节点获得锁，其他客户端监听前一个节点。\n优缺点 优点：可靠性高、天然支持顺序和事件通知机制；\n缺点：部署复杂、性能受限于 ZooKeeper 的吞吐能力。\n3. 基于 Etcd 的分布式锁 实现方式 利用 Etcd 的租约（Lease）和事务（Txn）；\n客户端通过 CAS 机制创建唯一锁键，并绑定租约；\n续期机制保障锁的持有。\n优缺点 优点：一致性强、适用于容器编排等场景；\n缺点：相对复杂，需要管理租约续期和连接状态。\n五、小结 实现方式 优点 缺点 Redis 高性能、实现简单、生态丰富 容易出现锁失效或误删 Redisson 功能丰富、支持自动续期 相对重量级，需客户端支持 数据库 简单易用、无需引入中间件 性能瓶颈明显，存在死锁风险 ZooKeeper 高可靠性、顺序性好 部署复杂、性能有限 Etcd 一致性强、支持租约机制 运维成本高，使用门槛较高 选择分布式锁实现方案时，应根据系统性能需求、可靠性要求和技术栈综合考虑。\n","date":"2025-06-07T00:00:00Z","permalink":"https://cyber-blog.github.io/p/distributed-lock/","title":"分布式锁的实现原理与方式详解"},{"content":"Java 线程池详解 Java 中的线程池是并发编程中非常重要的一部分，用于提高程序的性能和资源利用率，减少频繁创建和销毁线程的开销。本文将循序渐进地介绍 Java 线程池的构造参数、常见类型及其适用场景，帮助开发者选择和配置合适的线程池。\n一、线程池基础：ThreadPoolExecutor 构造函数 Java 提供了 ThreadPoolExecutor 类用于自定义线程池，其构造函数如下：\n1 2 3 4 5 6 7 8 9 public ThreadPoolExecutor( int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue\u0026lt;Runnable\u0026gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler ) 参数详解 1. corePoolSize（核心线程数） 线程池中始终保留的线程数量，即使空闲也不会被销毁。\n当任务提交时，如果当前线程数小于 corePoolSize，会创建新线程处理任务。\n2. maximumPoolSize（最大线程数） 线程池允许的最大线程数量。\n当队列已满且线程数小于 maximumPoolSize 时，线程池会创建新线程处理任务。\n3. keepAliveTime + unit（空闲线程最大存活时间） 非核心线程在空闲时间超过此值时会被回收。\n默认对核心线程无效，可通过 allowCoreThreadTimeOut(true) 启用。\n4. workQueue（任务等待队列） 用于缓存等待执行的任务，常见实现：\n类型 特点 适用场景 ArrayBlockingQueue 有界，FIFO 控制内存使用，防止 OOM LinkedBlockingQueue 默认无界 默认队列类型，需防范内存泄漏风险 SynchronousQueue 不缓存任务 高并发、短任务 PriorityBlockingQueue 支持任务优先级 有优先级需求的场景 5. threadFactory（线程工厂） 用于自定义线程创建逻辑，如命名、守护线程设置等。 1 2 3 4 5 6 new ThreadFactory() { private AtomicInteger count = new AtomicInteger(1); public Thread newThread(Runnable r) { return new Thread(r, \u0026#34;MyThread-\u0026#34; + count.getAndIncrement()); } } 6. handler（拒绝策略） 当线程池和队列已满时，任务提交会触发拒绝策略。 策略类 行为 是否抛异常 AbortPolicy 抛出异常 ✅ 是 CallerRunsPolicy 由调用者线程执行 ❌ 否 DiscardPolicy 直接丢弃任务 ❌ 否 DiscardOldestPolicy 丢弃队列中最旧的任务 ❌ 否 二、任务处理流程（简化逻辑） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 +------------------------------+ | 提交任务 executor.execute() | +---------------+--------------+ | v +-------------+--------------+ | 当前线程数 \u0026lt; corePoolSize ? | +-------------+--------------+ | 是 v 创建线程立即执行任务 | 否 v +-------------+--------------+ | 队列未满？（workQueue） | +-------------+--------------+ | 是 v 放入队列等待 | 否 v +--------------+-------------+ | 当前线程数 \u0026lt; maximumPoolSize ? | +--------------+-------------+ | 是 v 创建线程立即执行任务 | 否 v 触发拒绝策略（handler） 三、Executors 提供的线程池工厂方法 除了手动创建线程池，Java 提供了 Executors 工具类简化常见线程池的创建方式：\n类型 方法 特点 适用场景 固定线程池 Executors.newFixedThreadPool(n) 固定线程数，无界队列 稳定任务量，控制并发 缓存线程池 Executors.newCachedThreadPool() 无限线程数，空闲线程回收 高并发、短任务 单线程池 Executors.newSingleThreadExecutor() 单线程顺序执行 日志、事务顺序 定时线程池 Executors.newScheduledThreadPool(n) 支持定时与周期任务 定时任务处理 单线程定时池 Executors.newSingleThreadScheduledExecutor() 单线程定时任务 严格顺序定时任务 各线程池使用示例与说明 1. FixedThreadPool 1 ExecutorService pool = Executors.newFixedThreadPool(4); 固定线程数，线程不会被销毁。\n使用无界队列，需注意任务积压可能导致 OOM。\n2. CachedThreadPool 1 ExecutorService pool = Executors.newCachedThreadPool(); 无限线程创建，适合短期高并发任务。\n使用 SynchronousQueue，不会缓存任务。\n3. SingleThreadExecutor 1 ExecutorService pool = Executors.newSingleThreadExecutor(); 单线程执行所有任务，保证顺序。\n可用于串行化控制任务执行顺序。\n4. ScheduledThreadPool 1 ScheduledExecutorService pool = Executors.newScheduledThreadPool(2); 支持延迟和周期性执行任务： 1 2 pool.schedule(task, 1, TimeUnit.SECONDS); pool.scheduleAtFixedRate(task, 0, 2, TimeUnit.SECONDS); 5. SingleThreadScheduledExecutor 1 ScheduledExecutorService pool = Executors.newSingleThreadScheduledExecutor(); 单线程定时任务调度器，适用于需要顺序和定时的任务。 四、Executors 工厂方法的潜在风险 FixedThreadPool 和 SingleThreadExecutor 使用 无界队列，任务堆积可能导致内存溢出；\nCachedThreadPool 最大线程数无限制，线程创建过多可能引发 OOM；\n✅ 推荐实践 明确指定线程池参数，避免默认配置带来的不确定性。\n1 2 3 4 5 6 ThreadPoolExecutor executor = new ThreadPoolExecutor( 4, 8, 60, TimeUnit.SECONDS, new ArrayBlockingQueue\u0026lt;\u0026gt;(100), Executors.defaultThreadFactory(), new ThreadPoolExecutor.AbortPolicy() ); 五、总结与建议 对于一般项目，推荐根据任务特性合理配置 ThreadPoolExecutor；\nExecutors 提供的快捷工厂方法方便但存在默认值陷阱；\n线程池需监控运行状态，避免资源耗尽或任务堆积；\n定期评估线程池的负载情况，调整配置参数。\n使用线程池是一项系统工程，需要根据任务类型、系统资源、服务目标进行细致规划与调优。\n","date":"2025-04-01T00:00:00Z","permalink":"https://cyber-blog.github.io/p/java-threadpool-details/","title":"Java 线程池详解"},{"content":"Java HashMap 和 Redis Hash 的区别 • Redis 的 hash:\n• Redis 的 hash 是基于自定义的哈希表实现，支持高效的键值对操作。\n• 其内部实现利用了 dict 数据结构，具体是通过动态哈希表和链地址法处理冲突。\n• Redis 的 hash 可以存储较大规模的数据，但一般会限制在单个键值对（Field 和 Value）的大小范围内。\n• Java 的 HashMap:\n• Java 的 HashMap 是基于 数组 + 链表 + 红黑树 实现的哈希表。\n• 当链表长度超过阈值（默认 8）时，会将链表转换为红黑树以优化性能。\n在 Redis 中，dict 是 Redis 内部用来实现哈希表的一种核心数据结构。它为 Redis 的很多功能提供了基础支持，包括键值存储、集合和哈希类型等。\ndict • dict 是 Redis 的哈希表实现。\n• 它的主要目的是提供快速的键值对存储和检索。\n• 由两个哈希表组成，用于支持渐进式 rehash 操作（扩容或缩容时逐步迁移数据）。\n结构组成 Redis 的 dict 主要由以下几个部分组成：\ndict 结构 dict 是一个封装了哈希表的结构，它包含两个 dictEntry 数组和一些辅助信息。定义如下： 1 2 3 4 5 6 7 typedef struct dict { dictType *type; // 类型特定函数（如计算哈希值、键比较等） void *privdata; // 私有数据指针，供用户使用 dictht ht[2]; // 两个哈希表，ht[0] 和 ht[1] long rehashidx; // rehash 索引，-1 表示未进行 rehash unsigned long iterators; // 当前运行的安全迭代器数量 } dict; dictht 结构 dictht 是具体的哈希表结构，定义如下： 1 2 3 4 5 6 typedef struct dictht { dictEntry **table; // 哈希表数组 unsigned long size; // 哈希表大小 unsigned long sizemask; // 用于计算索引的掩码 (size - 1) unsigned long used; // 已使用的节点数量 } dictht; table 是一个指针数组，每个元素指向一个 dictEntry 链表的头结点。 sizemask 和 size 用于快速计算哈希值的存储位置。 dictEntry 结构 dictEntry 是存储具体键值对的结构： 1 2 3 4 5 6 7 8 9 10 typedef struct dictEntry { void *key; // 键 union { void *val; // 值（通用指针） uint64_t u64; // 64 位无符号整数值 int64_t s64; // 64 位有符号整数值 double d; // 浮点值 } v; // 值 struct dictEntry *next; // 指向下一个节点，用于处理哈希冲突（链表法） } dictEntry; dict的特性 双哈希表支持:\nRedis 的 dict 拥有两个哈希表（ht[0] 和 ht[1]），正常情况下只使用 ht[0]。 在扩容或缩容时，ht[1] 会被用来暂存新数据，然后逐步将数据从 ht[0] 迁移到 ht[1]。 渐进式 Rehash:\n扩容或缩容时，Redis 不会一次性完成哈希表的重建。 每次对哈希表进行读/写操作时，Redis 会将 ht[0] 中的一部分数据迁移到 ht[1]，从而避免性能抖动。 动态调整大小\nRedis 会根据哈希表的负载因子（used / size）动态调整哈希表的大小： 当负载因子超过 1（即元素数量大于桶数量）时，触发扩容。 当 Redis 处于高内存压力时（maxmemory 限制），负载因子可以放宽到 5。 冲突解决:\n使用链地址法解决哈希冲突。 同一索引的多个键值对通过 dictEntry 链表连接。 Redis Dict 和 Java Hash表有何不同 数据结构 Redis Dict\n数组：哈希槽的核心存储，每个槽位可以指向一个链表。初始大小通常根据实际数据动态调整，避免空间浪费。 链表：用于处理哈希冲突，存储冲突的键值对。Redis 的链表比较简单，只是一个线性链表，没有复杂的数据结构。 渐进式 Rehash： Redis 通过维护两张哈希表（ht[0] 和 ht[1]）来实现动态扩容或缩容。扩容时避免一次性拷贝，通过增量方式分批迁移数据。 数据迁移是逐步完成的，不会阻塞服务。 优势：不会造成显著的性能抖动。 劣势：Rehash 完成前需要维护两张哈希表，稍微增加了内存开销。 Java HashMap\n数组：存储哈希桶，每个桶可以指向一个链表或树结构。 链表：用于解决冲突，链表长度超过阈值后会转换为红黑树。 链表转红黑树： 在链表长度超过 8 时，链表自动转化为红黑树，降低最坏情况下的查找复杂度（从 O(n) 降到 O(log n)）。 当节点数量少于 6 时，会回退为链表。 优势：极大优化了高冲突情况下的性能。 劣势：引入红黑树增加了实现复杂性。 直接扩容： HashMap 在负载因子达到阈值时（默认 0.75）触发扩容，直接分配新的数组，将所有数据重新哈希并迁移。会导致明显的性能开销，尤其在存储大量数据时。 总结\n特性 Redis Dict Java HashMap 核心结构 数组 + 链表 数组 + 链表/红黑树 扩容机制 渐进式 Rehash 直接扩容，性能可能抖动 冲突处理 链地址法（链表） 链地址法 + 链表转树 并发支持 单线程操作 线程不安全，多线程需额外支持 实现复杂度 简单，轻量化 较复杂，支持更多功能 适用场景 高性能内存数据库 通用本地存储 ","date":"2024-12-24T00:00:00Z","permalink":"https://cyber-blog.github.io/p/redis-hash/","title":"Redis Hash 结构的底层实现"},{"content":"它是一只鸟吗？ 从您自己的数据创建模型 1 2 3 4 5 6 7 8 9 #NB: Kaggle requires phone verification to use the internet or a GPU. If you haven\u0026#39;t done that yet, the cell below will fail # This code is only here to check that your internet is enabled. It doesn\u0026#39;t do anything else. # Here\u0026#39;s a help thread on getting your phone number verified: https://www.kaggle.com/product-feedback/135367 import socket,warnings try: socket.setdefaulttimeout(1) socket.socket(socket.AF_INET, socket.SOCK_STREAM).connect((\u0026#39;1.1.1.1\u0026#39;, 53)) except socket.error as ex: raise Exception(\u0026#34;STOP: No internet. Click \u0026#39;\u0026gt;|\u0026#39; in top right and set \u0026#39;Internet\u0026#39; switch to on\u0026#34;) 1 2 3 4 5 6 7 8 # It\u0026#39;s a good idea to ensure you\u0026#39;re running the latest version of any libraries you need. # `!pip install -Uqq \u0026lt;libraries\u0026gt;` upgrades to the latest version of \u0026lt;libraries\u0026gt; # NB: You can safely ignore any warnings or errors pip spits out about running as root or incompatibilities import os iskaggle = os.environ.get(\u0026#39;KAGGLE_KERNEL_RUN_TYPE\u0026#39;, \u0026#39;\u0026#39;) if iskaggle: !pip install -Uqq fastai \u0026#39;duckduckgo_search\u0026gt;=6.2\u0026#39; 2015 年，创建一个可以识别鸟类的计算机系统的想法被认为非常具有挑战性，以至于它成为这个 XKCD 笑话的基础：\n但今天，我们只需几分钟就可以使用完全免费的资源做到这一点！ 我们将采取的基本步骤是：\n使用 DuckDuckGo 搜索“鸟照片”的图片 使用 DuckDuckGo 搜索“森林照片”的图片 微调预训练的神经网络以识别这两组 尝试在鸟的图片上运行此模型，看看它是否有效。 Step 1: 下载鸟类和非鸟类的图像 1 2 3 4 5 from duckduckgo_search import DDGS #DuckDuckGo has changed the api so we need to update from fastcore.all import * def search_images(keywords, max_images=200): return L(DDGS().images(keywords, max_results=max_images)).itemgot(\u0026#39;image\u0026#39;) import time, json 让我们从搜索鸟照片开始，看看我们得到什么样的结果。我们首先从搜索中获取 URL：\n1 2 3 4 #NB: `search_images` depends on duckduckgo.com, which doesn\u0026#39;t always return correct responses. # If you get a JSON error, just try running it again (it may take a couple of tries). urls = search_images(\u0026#39;bird photos\u0026#39;, max_images=1) urls[0] 1 \u0026#39;https://images.pexels.com/photos/326900/pexels-photo-326900.jpeg?cs=srgb\u0026amp;dl=wood-flight-bird-326900.jpg\u0026amp;fm=jpg\u0026#39; \u0026hellip;然后下载图片并查看它：\n1 2 3 4 5 6 7 from fastdownload import download_url dest = \u0026#39;bird.jpg\u0026#39; download_url(urls[0], dest, show_progress=False) from fastai.vision.all import * im = Image.open(dest) im.to_thumb(256,256) 现在让我们下载“森林照片”并压缩事情：\n1 2 download_url(search_images(\u0026#39;forest photos\u0026#39;, max_images=1)[0], \u0026#39;forest.jpg\u0026#39;, show_progress=False) Image.open(\u0026#39;forest.jpg\u0026#39;).to_thumb(256,256) 我们搜索到了想要的结果，所以让我们再多下载一些“鸟”和“森林”的照片，并将它们保存到不同的文件夹中：\n1 2 3 4 5 6 7 8 9 searches = \u0026#39;forest\u0026#39;,\u0026#39;bird\u0026#39; path = Path(\u0026#39;bird_or_not\u0026#39;) for o in searches: dest = (path/o) dest.mkdir(exist_ok=True, parents=True) download_images(dest, urls=search_images(f\u0026#39;{o} photo\u0026#39;)) time.sleep(5) resize_images(path/o, max_size=400, dest=path/o) Step 2: 训练我们的模型 有些照片可能无法正确下载，这可能会导致模型训练失败，因此需要删除它们：\n1 2 3 failed = verify_images(get_image_files(path)) failed.map(Path.unlink) len(failed) 为了训练模型，我们需要 DataLoaders，它是一个包含训练集（用于创建模型的图像）和验证集（用于检查模型准确性的图像\u0026ndash;训练期间未使用的数据）。 在 fastai 中，我们可以使用 DataBlock 轻松创建它，并从中查看示例图像：\n1 2 3 4 5 6 7 8 9 dls = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=[Resize(192, method=\u0026#39;squish\u0026#39;)] ).dataloaders(path, bs=32) dls.show_batch(max_n=6) 每个 DataBlock 参数的含义如下：\n1 blocks=(ImageBlock, CategoryBlock), 我们模型的输入是图像，输出是类别（在本例中为“鸟”或“森林”）。\n1 get_items=get_image_files, 要查找模型的所有输入，请运行 get_image_files 函数（它返回路径中所有图像文件的列表）。\n1 splitter=RandomSplitter(valid_pct=0.2, seed=42), 将数据随机分为训练集和验证集，使用 20% 的数据作为验证集。seed是随机数种子，一般是一个固定的整数，它用来初始化生成随机数的算法。通过设置相同的种子，你可以确保在不同的机器或者不同的时间执行相同的代码时，得到相同的随机结果。42 是一个常见的随机数种子（也许有些人觉得它是一个“神秘的”数字），其实可以使用任何整数值，只要它们固定，就能保证一致的划分结果。\n1 get_y=parent_label, 标签（y 值）是每个文件的 parent 的名称（即它们所在的文件夹的名称，可以是 bird 或 forest）。\n1 item_tfms=[Resize(192, method=\u0026#39;squish\u0026#39;)] Before training, resize each image to 192x192 pixels by \u0026ldquo;squishing\u0026rdquo; it (as opposed to cropping it).\n在训练之前，通过squish图像（而不是裁剪图像）将每个图像的大小调整为 192x192 像素。\n现在我们准备好训练我们的模型了。 最快广泛使用的计算机视觉模型是resnet18。 即使在 CPU 上，也可以在几分钟内对其进行训练！ （在 GPU 上，通常需要不到 10 秒\u0026hellip;\u0026hellip;）\nfastai 附带了一个有用的 fine_tune() 方法，该方法自动使用最佳实践来微调预训练模型，因此我们将使用它。\n1 2 learn = vision_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(3) • 参数3表示: 这个参数表示微调的训练轮数（epochs）。在这里，模型会在数据集上进行 3 个训练周期（epochs）的微调。\n1 2 Downloading: \u0026#34;https://download.pytorch.org/models/resnet18-f37072fd.pth\u0026#34; to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth 100%|██████████| 44.7M/44.7M [00:00\u0026lt;00:00, 84.4MB/s] epoch train_loss valid_loss error_rate time 0 0.783741 0.634016 0.214286 00:01 epoch train_loss valid_loss error_rate time 0 0.045509 0.020604 0.000000 00:01 1 0.026428 0.000642 0.000000 00:01 2 0.017754 0.000184 0.000000 00:01 一般来说，当我运行此程序时，我会看到验证集的准确性为 100%（尽管每次运行可能会有所不同）。 “微调”模型意味着我们从其他人使用其他数据集训练的模型（称为预训练模型）开始，并稍微调整权重，以便模型学会识别您的特定数据集。 在本例中，预训练模型经过训练，可以识别 imagenet 中的照片以及广泛使用的计算机视觉数据集，其中图像涵盖 1000 个类别）有关微调的详细信息及其重要性，请查看免费的 fast.ai课程。\nStep 3: 第 3 步：使用我们的模型（构建您自己的模型！） 让我们看看我们的模型对我们一开始下载的那只鸟有什么看法：\n1 2 3 is_bird,_,probs = learn.predict(PILImage.create(\u0026#39;bird.jpg\u0026#39;)) print(f\u0026#34;This is a: {is_bird}.\u0026#34;) print(f\u0026#34;Probability it\u0026#39;s a bird: {probs[0]:.4f}\u0026#34;) 1 2 This is a: bird. Probability it\u0026#39;s a bird: 1.0000 Good job, resnet18\n因此，正如所看到的，在几年的时间里，创建计算机视觉分类模型已经从“难得像一个笑话”变成了“非常简单且免费”！ 这不仅仅是在计算机视觉领域。 得益于深度学习，计算机现在可以做许多几年前看似不可能的事情，包括创作令人惊叹的艺术品和解释笑话。 它的发展速度如此之快，以至于即使是该领域的专家也很难预测它在未来几年将如何影响社会。 有一点很明确——我们所有人都必须尽力理解这项技术，这一点很重要，否则我们就会落后！\n现在轮到你了。 单击此页面的“Copy \u0026amp; Edit”并尝试使用您自己的图像搜索创建您自己的图像分类器！\n如果您喜欢这个，请考虑点击右上角的“投票”按钮——知道人们何时欣赏我们的工作对我们笔记本作者来说是非常鼓舞人心的。\n","date":"2024-12-19T00:00:00Z","permalink":"https://cyber-blog.github.io/p/practicaldeeplearning-pdl-part1-gettingstarted/","title":"实用深度学习 Part1-1.入门"},{"content":"背景 公司的一台 Freeswitch 软交换机在之前开发了一个自定义的基于 Websocket 的 ASR 模块。\n这个模块会在每次拨打机器人外呼时运行，监听电话的语音流，通过 Websocket 发送给 ASR 接口。并同时监听 WebSocket 的返回，如果有返回则通过 Freeswitch 的 ASR 模块扩展接口触发 Lua 的回调函数，Lua 回调会发送 Event 事件通知 Java 服务。\n这个模块在运行一段时间之后发现 Freeswitch 就会崩溃。通过 Zabbix 发现机器的可用内存会持续降低。\n并且排查系统日志，看到相同时间 Freeswitch 被系统 Kill 掉。可以确认是因为 Freeswitch内存泄漏导致的。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 [root@wyn-fs-02 ~]# dmesg | grep -i \u0026#34;kill\u0026#34; [87068.056478] in:imjournal invoked oom-killer: gfp_mask=0x201da, order=0, oom_score_adj=0 [87068.060698] [\u0026lt;ffffffffa49cd3c5\u0026gt;] oom_kill_process+0x2d5/0x4a0 [87068.061095] [\u0026lt;ffffffffa49ccd8d\u0026gt;] ? oom_unkillable_task+0xcd/0x120 [87068.132781] Out of memory: Kill process 1705 (freeswitch) score 482 or sacrifice child [87068.133635] Killed process 1705 (freeswitch), UID 0, total-vm:3770896kB, anon-rss:1925484kB, file-rss:0kB, shmem-rss:0kB [87068.140076] in:imjournal invoked oom-killer: gfp_mask=0x201da, order=0, oom_score_adj=0 [87068.147021] [\u0026lt;ffffffffa49cd3c5\u0026gt;] oom_kill_process+0x2d5/0x4a0 [87068.147658] [\u0026lt;ffffffffa49ccd8d\u0026gt;] ? oom_unkillable_task+0xcd/0x120 [87068.236909] Out of memory: Kill process 1727 (freeswitch) score 483 or sacrifice child [87068.237622] Killed process 1727 (freeswitch), UID 0, total-vm:3770896kB, anon-rss:1928716kB, file-rss:0kB, shmem-rss:0kB [100910.662047] gmain invoked oom-killer: gfp_mask=0x201da, order=0, oom_score_adj=0 [100910.668484] [\u0026lt;ffffffffa49cd3c5\u0026gt;] oom_kill_process+0x2d5/0x4a0 [100910.669139] [\u0026lt;ffffffffa49ccd8d\u0026gt;] ? oom_unkillable_task+0xcd/0x120 [100910.782806] Out of memory: Kill process 6691 (freeswitch) score 489 or sacrifice child [100910.784072] Killed process 6691 (freeswitch), UID 0, total-vm:3826532kB, anon-rss:1952492kB, file-rss:0kB, shmem-rss:0kB [203440.310492] in:imjournal invoked oom-killer: gfp_mask=0x201da, order=0, oom_score_adj=0 [203440.316740] [\u0026lt;ffffffffa49cd3c5\u0026gt;] oom_kill_process+0x2d5/0x4a0 [203440.317352] [\u0026lt;ffffffffa49ccd8d\u0026gt;] ? oom_unkillable_task+0xcd/0x120 [203440.390917] Out of memory: Kill process 16512 (freeswitch) score 476 or sacrifice child [203440.391793] Killed process 16512 (freeswitch), UID 0, total-vm:3896204kB, anon-rss:1902280kB, file-rss:0kB, shmem-rss:0kB 并且用 htop 监控 Freeswitch 的进程发现只要一开启 ASR 模块，内存占用就开始往上涨并且不会下降。\n解决过程 看到上述问题，初步怀疑是模块使用的 Websocket 库 Websocketpp 的相关对象等可能没有手动回收内存。开始着手检查。\n问题1 链接关闭时没有销毁相关的消息列表 经过检查，发现Websocket 库的 Client 及相关链接对象都是使用的C++11的shared_ptr或 weak_ptr等智能指针标准库进行管理。\n不过，在 Websocket Client 对象当中有一个自定义属性std::vector\u0026lt;std::string\u0026gt; m_messages; 这个队列记录了接收到的所有消息，但是在链接关闭时没有清空，需要手动清空。在重启 Freeswitch 后发现内存还是会一直增长，但是会有一些回收，虽然造成了一些内存泄漏，但是不是主要原因。这个问题也比较容易发现，可以直接查找代码发现。\n问题2 CJSON 的 CJSON_Print 方法转换的字符串需要手动清除 在发现还有问题后，检查了一遍代码，感觉只是看代码是检查不出来了。通过调研，先是使用的valgrind来查内存泄漏。\nValgrind 是一款综合性的程序分析工具，主要用于内存管理的调试、内存泄漏检测、并发错误检测和性能分析。它适用于 C、C++、Fortran 等语言编写的程序，帮助开发者发现程序中的各种隐蔽问题。Valgrind 本质上是一个虚拟化的执行环境，它将你的程序加载进一个动态二进制翻译器中执行，并对内存的使用情况进行监控。\nvalgrind Contenos直接使用yum install valgrind进行安装。\n安转完成后，运行下面的指令检查\n1 valgrind --leak-check=full freeswitch 检查结果\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 ==11979== Memcheck, a memory error detector ==11979== Copyright (C) 2002-2017, and GNU GPL\u0026#39;d, by Julian Seward et al. ==11979== Using Valgrind-3.15.0 and LibVEX; rerun with -h for copyright info ==11979== Command: freeswitch ==11979== Cannot lock pid file /usr/local/freeswitch/run/freeswitch.pid. ==11979== ==11979== HEAP SUMMARY: ==11979== in use at exit: 46,272 bytes in 25 blocks ==11979== total heap usage: 29 allocs, 4 frees, 120,232 bytes allocated ==11979== ==11979== LEAK SUMMARY: ==11979== definitely lost: 0 bytes in 0 blocks ==11979== indirectly lost: 0 bytes in 0 blocks ==11979== possibly lost: 24,576 bytes in 3 blocks ==11979== still reachable: 21,696 bytes in 22 blocks ==11979== suppressed: 0 bytes in 0 blocks ==11979== Rerun with --leak-check=full to see details of leaked memory ==11979== ==11979== For lists of detected and suppressed errors, rerun with: -s ==11979== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0) 可以看到没有检测出来 definitely 的明确内存泄漏，都是一些“一次性”的泄漏。。所以猜测是一些“still reachable”的内存越来越多导致的，“still reachable”指的是能被引用的内存，内存泄漏是内存无法被引用到，但是能被引用的内存不一定是还在使用的内存，比如不断向一个链表里面写数据而没有删除动作，那么这个链表就会一直增大，虽然可能只有最新的10个数据有在用，但是没有把旧的数据释放，这个虽然不是严格的内存泄漏，但也会造成了内存不断增大。\n目前来看就是借助其他办法分析现在 Freeswitch 的内存分布。\njemalloc Google 了一番，找到一个C++内存分析工具 jemalloc。该工具能分析 C++程序的内存分布，帮助检查哪个环节分配了过多的内存。\njemalloc 是一个高效的内存分配库，设计用于提升系统性能、减少内存碎片和优化多线程程序的内存管理。最初由 Facebook 开发，jemalloc 被广泛应用于需要高性能和高并发支持的环境，如数据库系统、Web 服务器和缓存服务。以下是 jemalloc 的一些关键特点：\n内存碎片优化：jemalloc 使用分级分配策略和延迟归还内存技术，以有效减少内存碎片问题，特别是在需要频繁分配和释放内存的应用中，表现出更稳定的内存使用率。\n多线程优化：jemalloc 针对多线程环境进行了深度优化，通过线程本地缓存（thread-local caching）技术大幅减少了内存锁竞争，提升了多线程程序的并发性能。\n分配效率高：jemalloc 使用多级分配池（arenas）和分级桶（bins）来高效管理不同大小的内存分配需求，减少了内存分配器的开销。\n诊断工具支持：jemalloc 提供了丰富的调试和诊断功能，可以生成内存分配日志和统计数据，帮助开发人员分析内存分配情况和优化程序内存使用。\n与 glibc 兼容：在 Linux 系统上，jemalloc 可作为 glibc 的替代内存分配器使用，应用程序只需简单配置即可使用 jemalloc 而无需代码修改。\n安装步骤 1 2 3 4 5 6 7 8 wget https://github.com/jemalloc/jemalloc/releases/download/5.3.0/jemalloc-5.3.0.tar.bz2 bzip2 -d jemalloc-5.3.0.tar.bz2 tar -xf jemalloc-5.3.0.tar cd jemalloc-5.3.0 ./autogen.sh ./configure --prefix=/usr/local --enable-prof make make install 内存分布接转出 PDF 需要依赖这两个工具，直接通过yum安装。\n1 2 yum install graphviz yum install ghostscript 使用 要在应用中使用 jemalloc，一般可以通过 LD_PRELOAD 将其加载，或在编译阶段直接链接 jemalloc 库。例如，在 Linux 上可以使用以下命令强制应用使用 jemalloc：\n1 MALLOC_CONF=prof:true,lg_prof_interval:27 LD_PRELOAD=/usr/local/lib/libjemalloc.so.2 freeswitch 命令详解\n1. MALLOC_CONF 环境变量\njemalloc 支持通过 MALLOC_CONF 环境变量传递配置选项，这里使用了以下参数：\n​\t•prof:true：开启内存分配分析（profiling）功能，帮助检测和分析应用中的内存使用情况。\n​\t•lg_prof_interval:27：设置内存分配采样间隔的对数值。27 表示间隔为 2^27 字节，即每分配 128MB 内存进行一次采样。\n2. LD_PRELOAD 环境变量\n​\t•LD_PRELOAD=/usr/local/lib/libjemalloc.so.2：指定在程序启动时预加载 jemalloc 库，以替换默认的内存分配函数 (malloc、free 等)。/usr/local/lib/libjemalloc.so.2 是 jemalloc 库的路径。\n3. freeswitch\n​\t•freeswitch：启动 Freeswitch 应用程序，应用上述 jemalloc 设置。\n运行下面的命令可以把单次采样文件转换为 PDF 内存报告\n1 jeprof --show_bytes --pdf /usr/local/freeswitch/bin/freeswitch jeprof.1842.42.i42.heap \u0026gt; output1.pdf 注意这里有一个坑，可执行文件的路径必须是执行文件的绝对路径，不能是 bin 目录下的软连接。\n报告 下面是报告的完整截图，可以看到每个方法分配的内存大小还有层级调用关系。\n可以看到 Print(inline) 方法分配了 86% 的内存。该方法的调用栈是 cJson_Print -\u0026gt; asr_feed。asr_feed方法会通过 Websocket 发送所有音频数据，每帧音频都是通过使用 JSON 格式发送，通过cJSON_Print 将 JSON 数据结构转换为一个字符串，并返回一个动态分配的字符串指针。但是由于这个返回的字符串是通过 malloc 或 realloc 直接分配在堆上，所以必须使用 free() 函数手动释放，否则，这部分内存将会造成内存泄露。\n即使 cJSON_Print 的指针是局部变量，并且已经销毁了这个指针，内存仍然会保留在堆上，并且依然是标记为“可达到的”（reachable）状态。这就是内存泄露的主要原因。\n可达到内存 “Reachable（可达到）” 是内存的概念，通常出现在内存泄漏检查工具（例如 Valgrind）的报告中，表示虽然程序不再需要使用这部分内存，或者已经没有明确的指针指向它，但从技术上讲，内存管理系统仍然可以找到它并访问它。\n要理解这个问题，我们可以从内存管理和指针的生命周期两个方面来分析：\n1. 堆内存的分配和释放\n在调用 cJSON_Print() 时，它返回一个动态分配的字符串，内存分配是在 堆 上完成的。这意味着，程序员必须手动释放这块内存（通过 free()）。即使局部变量指针已经销毁，这块堆内存仍然会存在，直到被明确释放或程序结束。\n1 2 3 // json_str 是局部变量，指向堆上的内存 char *json_str = cJSON_Print(json_data); // 堆内存分配 当你离开这个函数或作用域时，json_str 指针本身可能会被销毁或失效，但堆上的内存依然存在。这就是为什么它仍然会被视为reachable：因为堆上分配的内存没有释放，操作系统或内存管理器可以通过堆的内存结构访问它。\n2. 可达到 vs 不可达到\n​\t•可达到 (reachable)：指分配的内存块在程序中尚未被释放，并且从技术上，内存管理器仍然可以通过某些方式找到它们，尽管它们可能不再被局部变量或指针直接引用。\n​\t•不可达到 (unreachable)：如果没有任何指针或引用指向这块内存，而且你无法通过正常的编程手段找到这块内存，那么这部分内存就变成“不可达到”的了。此时，它会被标记为未使用内存，等到程序结束时才会回收。\n","date":"2024-10-25T00:00:00Z","image":"https://cyber-blog.github.io/p/cpp-memoryleak/cover_hu11979990975044714774.png","permalink":"https://cyber-blog.github.io/p/cpp-memoryleak/","title":"记一次C++内存泄漏的排查经过"},{"content":"VITS 是一种用于英语文本转语音 （TTS） 的轻量级、低延迟模型。 大规模多语言语音 （MMS） 是 VITS 的多语言 TTS 扩展，支持 1100 多种语言。\n两者都使用相同的底层 VITS 架构，由一个鉴别器和一个用于基于 GAN 的训练的生成器组成。它们的标记器不同：VITS 标记器将英语输入文本转换为音素，而 MMS 标记器将输入文本转换为基于字符的标记。\n如果要使用宽松的英语 TTS 模型，则应微调基于 VITS 的 checkpoint，并针对所有其他情况微调基于 MMS 的检查点。 针对印尼语的训练选择 mms-tts-ind-traincheckpoint\n结合正确的数据和以下训练方法，您可以在 20 分钟内获得每个 VITS/MMS 检查点的出色微调版本，只需 80 到 150 个样本。\n微调 VITS 或 MMS 需要按连续顺序完成多个阶段：\nInstall requirements Choose or create the initial model Finetune the model Optional - how to use the finetuned model 安装 requirements 克隆仓库并 install，确保python \u0026gt;= 3.10 1 2 3 git clone git@github.com:ylacombe/finetune-hf-vits.git cd finetune-hf-vits pip install -r requirements.txt 链接您的 Hugging Face 帐户，以便您可以在 Hub 上拉取/推送模型仓库。这将使您能够在 Hub 上保存微调的权重，以便您可以与社区共享它们并轻松重复使用它们。运行以下命令： 1 2 git config --global credential.helper store huggingface-cli login 然后输入 https://huggingface.co/settings/tokens 的身份验证令牌。如果还没有令牌，请创建一个新令牌。您应确保此令牌具有“写入”权限。\n使用 Cython 构建monotonic alignment search function。这是绝对必要的，因为 Python 原生版本非常慢。\n1 2 3 4 5 # Cython-version Monotonoic Alignment Search cd monotonic_align mkdir monotonic_align python setup.py build_ext --inplace cd .. （可选）如果您使用的是原始 VITS 检查点，而不是 MMS 检查点，请安装phonemizer。 按照此处指示的步骤操作。 例如，如果你在 Debian/Unbuntu 上： 1 2 3 4 # Install dependencies sudo apt-get install festival espeak-ng mbrola # Install phonemizer pip install phonemizer 更多细节 某些语言要求在将文本提供给 `VitsTokenizer` 之前使用 `uroman`，因为目前分词器本身不支持执行预处理。 为此，您需要将 uroman 仓库克隆到本地计算机，并将 bash 变量 UROMAN 设置为本地路径： 1 2 3 git clone https://github.com/isi-nlp/uroman.git cd uroman export UROMAN=$(pwd) 剩下的就是由训练脚本来处理了。 选择模型 如果需要的 checkpoint 已经存在。\n目前一些checkpoint已经可用，参照如下列表可以在 Hugging Face 搜索 可以列表\nEnglish ylacombe/vits-ljs-with-discriminator (确保 phonemizer已安装) - 非常适合单一声音微调 ylacombe/vits-vctk-with-discriminator (确保 phonemizer已安装) - 适用于多声音英语微调。 ylacombe/mms-tts-eng-train - 如果您想避免使用 phonemizer 包。 Spanish - ylacombe/mms-tts-spa-train 西班牙语 Korean - ylacombe/mms-tts-kor-train 韩语 Marathi - ylacombe/mms-tts-mar-train 马拉地语 Tamil - ylacombe/mms-tts-tam-train 泰米尔语 Gujarati - ylacombe/mms-tts-guj-train 古吉拉特语 在这种情况下，您找到了正确的检查点，记下存储库名称并直接传递到下一步🤗。 在这里我选择了 fadhilamri/mms-tts-ind-train🥳。 如果需要的需要微调的语言 checkpoint 不存在，请参考这里创建对应语言的checkpoint。\n微调 使用 json 配置文件可以运行微调脚本，两种方法都使用命令行。请注意，您只需要一个 GPU 即可微调 VITS/MMS，因为模型非常轻巧（83M 参数）,根据数据集的大小对显存的需求有较大变化，我的数据集大小是602MB、需要 21GB 显存左右。\nNote 使用配置文件是使用微调脚本的首选方式，因为它包含要考虑的最重要的参数。有关参数的完整列表，请运行 python run_vits_finetuning.py --help。请注意，训练脚本不会忽略某些参数。\ntraining_config_examples文件夹包含配置文件的示例。一旦对您的配置文件感到满意，您就可以微调模型。\n要考虑的重要参数：\n与工件相关的所有内容：project_name 和输出目录 （hub_model_id， output_dir），用于跟踪模型。 需要微调的模型：model_name_or_path。 这里，填写需要进行微调的模型（checkpoint）。 例如，如果选择已存在的检查点：ylacombe/vits-ljs-with-discriminator，或者转换了自己的检查点：\u0026lt;repo-id-you-want\u0026gt; 或 \u0026lt;local-folder\u0026gt;。 数据集使用的 dataset_name 及其详细信息：dataset_config_name、列名等。 如果有多个声音，而您只想保留一个声音，请注意 speaker_id_column_name、override_speaker_embeddings 和 filter_on_speaker_id。后者允许只保留一个声音，但您也可以使用多个声音进行训练。 例如，finetune_english.json 中默认使用的数据集是 British Isles accents 数据集的子集，使用 welsh_female 配置的单个威尔士女声，由 speaker_id=5223 标识。 如何打包本地数据到上传到 HuggingFace 的 Datasets 请参考我写的上一篇博客 超级重要的 hyperparameters‼ learning_rate batch_size 各种损失权重：weight_duration、weight_kl、weight_mel、weight_disc、weight_gen 、weight_fmaps 可以参考我进行微调的配置文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 { \u0026#34;project_name\u0026#34;: \u0026#34;vits_finetuned_ind_female\u0026#34;, \u0026#34;push_to_hub\u0026#34;: true, \u0026#34;hub_model_id\u0026#34;: \u0026#34;dhavalgala/mms-tts-ind-train\u0026#34;, \u0026#34;overwrite_output_dir\u0026#34;: true, \u0026#34;output_dir\u0026#34;: \u0026#34;./tmp/vits_finetuned_ind_female\u0026#34;, \u0026#34;dataset_name\u0026#34;: \u0026#34;Majiang213/ind_famal\u0026#34;, \u0026#34;dataset_config_name\u0026#34;: \u0026#34;data\u0026#34;, \u0026#34;audio_column_name\u0026#34;: \u0026#34;audio\u0026#34;, \u0026#34;text_column_name\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;train_split_name\u0026#34;: \u0026#34;train\u0026#34;, \u0026#34;eval_split_name\u0026#34;: \u0026#34;train\u0026#34;, \u0026#34;speaker_id_column_name\u0026#34;: \u0026#34;speaker_id\u0026#34;, \u0026#34;override_speaker_embeddings\u0026#34;: true, \u0026#34;filter_on_speaker_id\u0026#34;: \u0026#34;12\u0026#34;, \u0026#34;max_duration_in_seconds\u0026#34;: 50, \u0026#34;min_duration_in_seconds\u0026#34;: 1.0, \u0026#34;max_tokens_length\u0026#34;: 5000, \u0026#34;model_name_or_path\u0026#34;: \u0026#34;dhavalgala/mms-tts-ind-train\u0026#34;, \u0026#34;preprocessing_num_workers\u0026#34;: 4, \u0026#34;do_train\u0026#34;: true, \u0026#34;num_train_epochs\u0026#34;: 200, \u0026#34;gradient_accumulation_steps\u0026#34;: 1, \u0026#34;gradient_checkpointing\u0026#34;: false, \u0026#34;per_device_train_batch_size\u0026#34;: 16, \u0026#34;learning_rate\u0026#34;: 2e-5, \u0026#34;adam_beta1\u0026#34;: 0.8, \u0026#34;adam_beta2\u0026#34;: 0.99, \u0026#34;warmup_ratio\u0026#34;: 0.01, \u0026#34;group_by_length\u0026#34;: false, \u0026#34;do_eval\u0026#34;: true, \u0026#34;eval_steps\u0026#34;: 50, \u0026#34;per_device_eval_batch_size\u0026#34;: 16, \u0026#34;do_step_schedule_per_epoch\u0026#34;: true, \u0026#34;weight_disc\u0026#34;: 3, \u0026#34;weight_fmaps\u0026#34;: 1, \u0026#34;weight_gen\u0026#34;: 1, \u0026#34;weight_kl\u0026#34;: 1.5, \u0026#34;weight_duration\u0026#34;: 1, \u0026#34;weight_mel\u0026#34;: 35, \u0026#34;fp16\u0026#34;: true, \u0026#34;seed\u0026#34;: 456 } 推理 只需几行代码，即可通过文本转语音 （TTS） 管道使用微调的模型！只需将 ylacombe/vits_ljs_welsh_female_monospeaker_2 替换为您自己的模型 ID （hub_model_id） 或模型的路径 （output_dir）。\n1 2 3 4 5 6 7 8 9 from transformers import pipeline import scipy model_id = \u0026#34;ylacombe/vits_ljs_welsh_female_monospeaker_2\u0026#34; synthesiser = pipeline(\u0026#34;text-to-speech\u0026#34;, model_id) # add device=0 if you want to use a GPU speech = synthesiser(\u0026#34;Hello, my dog is cooler than you!\u0026#34;) scipy.io.wavfile.write(\u0026#34;finetuned_output.wav\u0026#34;, rate=speech[\u0026#34;sampling_rate\u0026#34;], data=speech[\u0026#34;audio\u0026#34;][0]) 问题 目前在推理时有一个关于 speaker_id的代码兼容性问题，需要将run_vits_finetuning.py文件里的所有 speaker_id=batch[\u0026quot;speaker_id\u0026quot;] 代码注释掉。 注释后参考\n1 2 3 4 5 6 7 8 9 model_outputs_train = model( input_ids=batch[\u0026#34;input_ids\u0026#34;], attention_mask=batch[\u0026#34;attention_mask\u0026#34;], labels=batch[\u0026#34;labels\u0026#34;], labels_attention_mask=batch[\u0026#34;labels_attention_mask\u0026#34;], # speaker_id=batch[\u0026#34;speaker_id\u0026#34;], return_dict=True, monotonic_alignment_function=maximum_path, ) PS MMS 是由 Vineel Pratap、Andros Tjandra、Bowen Shi 等人在《 Scaling Speech Technology to 1,000+ Languages》中提出的。您可以在MMS Language Coverage Overview中找到有关受支持语言及其 ISO 639-3 代码的更多详细信息，并在 Hugging Face Hub 上查看所有 MMS-TTS 检查点：facebook/mms-tts。 Hugging Face 🤗 Transformers 用于模型集成，Hugging Face 🤗 Accelerate 用于分布式代码，Hugging Face 🤗 datasets用于方便数据集访问。 ","date":"2024-07-12T00:00:00Z","image":"https://cyber-blog.github.io/p/finetune-vits-mms-id/cover_hu5113326763321136349.png","permalink":"https://cyber-blog.github.io/p/finetune-vits-mms-id/","title":"基于 MMS 模型的印尼语微调"},{"content":"打包Dataset 之前在使用 finetune-hf-vits 项目微调 MMS-TTS 的印尼语模型的时候，该项目会读取 HuggingFace 的数据集，需要将本地的数据集上传到 HuggingFace。 HugginFace 的数据集是 parquet 格式。可以使用 Hugging Face 官方提供的包生成。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 path = \u0026#34;your-path\u0026#34; # 读取 Excel 文件 excel_file_path = os.path.join(path, \u0026#39;FEMALE VOICE 2 .xlsx\u0026#39;) df = pd.read_excel(excel_file_path) # 处理音频路径 df[\u0026#39;audio\u0026#39;] = df[\u0026#39;audio\u0026#39;].apply(lambda x: load_audio_file_to_bytes(x) if isinstance(x, str) and x else None) # 转换 DataFrame 为字典格式 data_dict = df[[\u0026#39;text\u0026#39;, \u0026#39;audio\u0026#39;, \u0026#39;speaker_id\u0026#39;]].to_dict(orient=\u0026#39;list\u0026#39;) # 定义数据集特征，使用 Audio 类型 features = Features({ \u0026#39;text\u0026#39;: Value(\u0026#39;string\u0026#39;), \u0026#39;audio\u0026#39;: Audio(sampling_rate=None), # 这里定义音频特征 \u0026#39;speaker_id\u0026#39;: Value(\u0026#39;string\u0026#39;) }) # 创建 Dataset 对象 dataset = Dataset.from_dict(data_dict, features=features) # 保存为 Hugging Face 的数据集格式 dataset.push_to_hub(\u0026#34;Majiang213/ind_female\u0026#34;) 通过代码可以看到，我的数据保存格式是一个 Excel 文件，里面有 text 列，为音频的文本，audio 列为文件路径，speaker_id 列为声音 id。在读取 Excel 之后，将音频路径转换为了实际的音频数据。然后将 Pandas DataFrame 对象转换为了一个字典，通过 Hugging Face 包的 Dataset 对象，进行转换，并上传。\n另外不要忘记使用该命令登录\n1 huggingface-cli login 这是读取音频数据的代码和需要引入的包\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 import io import os import librosa import pandas as pd import soundfile as sf from datasets import Dataset, Audio, Value, Features from pydub import AudioSegment def load_audio_file_to_bytes(audio_path): audio_path = os.path.join(path, audio_path) # 获取文件扩展名 file_extension = audio_path.split(\u0026#39;.\u0026#39;)[-1].lower() # 创建一个字节流对象 audio_bytes = io.BytesIO() # 处理不同格式的音频文件 if file_extension in [\u0026#39;mp3\u0026#39;, \u0026#39;m4a\u0026#39;]: # 使用 pydub 读取 MP3 或 M4A 文件 audio = AudioSegment.from_file(audio_path, format=file_extension) # 直接保存音频数据到字节流 audio.export(audio_bytes, format=\u0026#39;wav\u0026#39;) elif file_extension in [\u0026#39;wav\u0026#39;, \u0026#39;flac\u0026#39;, \u0026#39;ogg\u0026#39;]: # 使用 librosa 直接读取音频文件 y, sr = librosa.load(audio_path, sr=None) # 将 NumPy 数组转换为 int16 类型并写入字节流 sf.write(audio_bytes, y, sr, format=\u0026#39;WAV\u0026#39;) else: raise ValueError(f\u0026#34;Unsupported file format: {file_extension}\u0026#34;) # 移动流的指针到起始位置，以便后续读取 audio_bytes.seek(0) return audio_bytes.getvalue() PS 另外推荐一个处理音频文件的 Python 小工具 audio-preprocess。 这个 Repo 包含了一些用于处理音频的脚本. 主要包含以下功能:\n视频/音频转 wav 音频人声分离 音频自动切片 音频响度匹配 音频数据统计（支持判断音频长度） 音频重采样 音频打标 (.lab) 音频打标 FunASR（使用 --model-type funasr 开启, 详细使用方法可查看代码） 音频打标 WhisperX .lab 标注合并为 .list 文件 (示例: fap merge-lab ./dataset list.txt \u0026quot;{PATH}|spkname|JP|{TEXT}\u0026quot;) ([ ] 表示未完成, [x] 表示已完成)\n使用方式参考 1 2 pip install -e . fap --help ","date":"2024-07-01T00:00:00Z","permalink":"https://cyber-blog.github.io/p/dataset-upload2hugginface/","title":"音频文件打包 Dataset 上传 HugginFace"},{"content":"定义 ASR 模块 在当前的外呼平台中，实时 ASR（语音识别）已成为一种常见的基础功能。尽管接触 FreeSWITCH 已有一段时间，但此前一直未进行 ASR 模块的定制化开发，仅通过 GitHub 上的开源项目实现需求。然而，这些方案往往无法完全满足业务需求，例如需要对接不同厂商的 ASR 接口、支持定制化开发（如实时 ASR、文件 ASR、打断功能等）。因此，基于具体需求进行定制开发势在必行。\n目前的一个需求是实现 FreeSWITCH 的语音实时识别，采用的方案是通过 WebSocket 传输语音帧进行识别，具体可以通过以下方式实现。\n基础代码 Freeswitch 的 ASR 模块也是通过 media_bug 来实现的。 FreeSWITCH 定义的模块，需要定义下面的这 3 个方法。分别定义模块加载、卸载和模块定义。\n主要关注模块加载方法，定义了这个模块在 ASR 生命周期里的接口实现。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 extern \u0026#34;C\u0026#34; { // 模块定义，分别是模块加载、模块卸载 SWITCH_MODULE_LOAD_FUNCTION(mod_iflytekasr_load); SWITCH_MODULE_SHUTDOWN_FUNCTION(mod_iflytekasr_shutdown); SWITCH_MODULE_DEFINITION(mod_iflytekasr, mod_iflytekasr_load, mod_iflytekasr_shutdown, NULL); } // 模块加载 SWITCH_MODULE_LOAD_FUNCTION(mod_iflytekasr_load) { switch_asr_interface_t *asr_interface; switch_mutex_init(\u0026amp;MUTEX, SWITCH_MUTEX_NESTED, pool); globals.pool = pool; switch_mutex_lock(MUTEX); iflytekasr_load_config(); switch_mutex_unlock(MUTEX); /* connect my internal structure to the blank pointer passed to me */ *module_interface = switch_loadable_module_create_module_interface(pool, modname); asr_interface = (switch_asr_interface_t *)switch_loadable_module_create_interface(*module_interface, SWITCH_ASR_INTERFACE); asr_interface-\u0026gt;interface_name = \u0026#34;iflytekasr\u0026#34;; asr_interface-\u0026gt;asr_open = asr_open; asr_interface-\u0026gt;asr_close = asr_close; asr_interface-\u0026gt;asr_load_grammar = asr_load_grammar; asr_interface-\u0026gt;asr_unload_grammar = asr_unload_grammar; asr_interface-\u0026gt;asr_feed = asr_feed; asr_interface-\u0026gt;asr_pause = asr_pause; asr_interface-\u0026gt;asr_resume = asr_resume; asr_interface-\u0026gt;asr_check_results = asr_check_results; asr_interface-\u0026gt;asr_get_results = asr_get_results; asr_interface-\u0026gt;asr_start_input_timers = asr_start_input_timers; asr_interface-\u0026gt;asr_text_param = asr_text_param; asr_interface-\u0026gt;asr_numeric_param = asr_numeric_param; asr_interface-\u0026gt;asr_float_param = asr_float_param; /* indicate that the module should continue to be loaded */ return SWITCH_STATUS_SUCCESS; *module_interface = switch_loadable_module_create_module_interface(pool, modname); switch_log_printf(SWITCH_CHANNEL_LOG, SWITCH_LOG_INFO, \u0026#34;iflytekasr mod loaded version 1.1 since 20241024.\\n\u0026#34;); return SWITCH_STATUS_SUCCESS; } // 模块卸载 SWITCH_MODULE_SHUTDOWN_FUNCTION(mod_iflytekasr_shutdown) { switch_log_printf(SWITCH_CHANNEL_LOG, SWITCH_LOG_WARNING, \u0026#34;iflytekasr shutdown\\n\u0026#34;); return SWITCH_STATUS_UNLOAD; } 接口实现 下列接口定义了一个 ASR 模块的完整生命周期及其功能。在 FreeSWITCH 中，这些接口可用于与第三方 ASR API（如科大讯飞、Google Speech-to-Text 等）对接，负责从初始化到关闭的各种操作。以下是各接口的具体作用：\n​\t1.asr_open：ASR 启动时执行，初始化 ASR 模块并建立连接。\n​\t2.asr_close：ASR 关闭时执行，关闭 ASR 模块并释放资源。\n​\t3.asr_load_grammar：在初始化资源之后执行，加载语法或语言模型。在开始 ASR 开启命令，可以加载一些外部传递来的参数，最终会传递到该接口解析。命令格式为 detect_speech \u0026lt;mod_name\u0026gt; \u0026lt;gram_name\u0026gt; \u0026lt;gram_path\u0026gt; [\u0026lt;addr\u0026gt;]\n​\t4.asr_unload_grammar：在 close 方法前执行，卸载语法或语言模型，释放资源。\n​\t5.asr_feed：在拨打过程中，向 ASR API 传递语音数据帧。\n​\t6.asr_pause：暂停当前语音识别任务。\n​\t7.asr_resume：恢复暂停的语音识别任务。\n​\t8.asr_check_results：检查识别结果是否已生成。注意：这个方法的实现要保证幂等性。在asr_get_results方法被调用之前，这个方法多次调用的结果要保持一致\n​\t9.asr_get_results：获取语音识别的最终结果。\n​\t10.asr_start_input_timers：启动输入计时器控制时间窗口。\n​\t11.asr_text_param：设置文本类型的配置参数。\n​\t12.asr_numeric_param：设置数值类型的配置参数。\n​\t13.asr_float_param：设置浮点类型的配置参数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 // asr 模块开启时执行的方法，做一些初始化操作 static switch_status_t test_speech_open(switch_speech_handle_t *sh, const char *voice_name, int rate, int channels, switch_speech_flag_t *flags) { test_tts_t *context = switch_core_alloc(sh-\u0026gt;memory_pool, sizeof(test_tts_t)); switch_assert(context); context-\u0026gt;samples = sh-\u0026gt;samplerate; sh-\u0026gt;private_info = context; return SWITCH_STATUS_SUCCESS; } // close 时执行的方法，释放一些资源 static switch_status_t test_speech_close(switch_speech_handle_t *sh, switch_speech_flag_t *flags) { return SWITCH_STATUS_SUCCESS; } // 通过查阅官方文档可以看到这个指令的格式为 detect_speech \u0026lt;mod_name\u0026gt; \u0026lt;gram_name\u0026gt; \u0026lt;gram_path\u0026gt; [\u0026lt;addr\u0026gt;] /*! function to load a grammar to the asr interface */ static switch_status_t asr_load_grammar(switch_asr_handle_t *ah, const char *grammar, const char *name) { test_asr_t *context = (test_asr_t *)ah-\u0026gt;private_info; if (switch_test_flag(ah, SWITCH_ASR_FLAG_CLOSED)) { switch_log_printf(SWITCH_CHANNEL_LOG, SWITCH_LOG_ERROR, \u0026#34;asr_open attempt on CLOSED asr handle\\n\u0026#34;); return SWITCH_STATUS_FALSE; } switch_log_printf(SWITCH_CHANNEL_UUID_LOG(context-\u0026gt;channel_uuid), SWITCH_LOG_INFO, \u0026#34;load grammar %s\\n\u0026#34;, grammar); context-\u0026gt;grammar = switch_core_strdup(ah-\u0026gt;memory_pool, grammar); return SWITCH_STATUS_SUCCESS; } /*! function to unload a grammar to the asr interface */ static switch_status_t asr_unload_grammar(switch_asr_handle_t *ah, const char *name) { return SWITCH_STATUS_SUCCESS; } // 接收语音流的方法 // ASR 的 media_bug 会在 SWITCH_ABC_TYPE_READ 状态下调用这个方法。 /*! function to feed audio to the ASR */ static switch_status_t asr_feed( switch_asr_handle_t *ah, void *data, unsigned int len, switch_asr_flag_t *flags) { test_asr_t *context = (test_asr_t *) ah-\u0026gt;private_info; switch_status_t status = SWITCH_STATUS_SUCCESS; switch_vad_state_t vad_state; if (switch_test_flag(ah, SWITCH_ASR_FLAG_CLOSED)) { return SWITCH_STATUS_BREAK; } if (switch_test_flag(context, ASRFLAG_RETURNED_RESULT) \u0026amp;\u0026amp; switch_test_flag(ah, SWITCH_ASR_FLAG_AUTO_RESUME)) { switch_log_printf(SWITCH_CHANNEL_UUID_LOG(context-\u0026gt;channel_uuid), SWITCH_LOG_DEBUG, \u0026#34;Auto Resuming\\n\u0026#34;); test_asr_reset(context); } if (switch_test_flag(context, ASRFLAG_READY)) { vad_state = switch_vad_process(context-\u0026gt;vad, (int16_t *)data, len / sizeof(uint16_t)); if (vad_state == SWITCH_VAD_STATE_STOP_TALKING) { switch_set_flag(context, ASRFLAG_RESULT); switch_log_printf(SWITCH_CHANNEL_UUID_LOG(context-\u0026gt;channel_uuid), SWITCH_LOG_INFO, \u0026#34;Talking stopped, have result.\\n\u0026#34;); switch_vad_reset(context-\u0026gt;vad); switch_clear_flag(context, ASRFLAG_READY); } else if (vad_state == SWITCH_VAD_STATE_START_TALKING) { switch_set_flag(context, ASRFLAG_START_OF_SPEECH); context-\u0026gt;speech_time = switch_micro_time_now(); } } return status; } // detect_speech resume 命令执行时会调用这个方法 /*! function to resume recognizer */ static switch_status_t asr_resume(switch_asr_handle_t *ah) { test_asr_t *context = (test_asr_t *) ah-\u0026gt;private_info; if (switch_test_flag(ah, SWITCH_ASR_FLAG_CLOSED)) { switch_log_printf(SWITCH_CHANNEL_LOG, SWITCH_LOG_ERROR, \u0026#34;asr_resume attempt on CLOSED asr handle\\n\u0026#34;); return SWITCH_STATUS_FALSE; } switch_log_printf(SWITCH_CHANNEL_UUID_LOG(context-\u0026gt;channel_uuid), SWITCH_LOG_DEBUG, \u0026#34;Resuming\\n\u0026#34;); test_asr_reset(context); return SWITCH_STATUS_SUCCESS; } // detect_speech pause 命令执行时会调用这个方法 /*! function to pause recognizer */ static switch_status_t asr_pause(switch_asr_handle_t *ah) { test_asr_t *context = (test_asr_t *) ah-\u0026gt;private_info; if (switch_test_flag(ah, SWITCH_ASR_FLAG_CLOSED)) { switch_log_printf(SWITCH_CHANNEL_LOG, SWITCH_LOG_ERROR, \u0026#34;asr_pause attempt on CLOSED asr handle\\n\u0026#34;); return SWITCH_STATUS_FALSE; } switch_log_printf(SWITCH_CHANNEL_UUID_LOG(context-\u0026gt;channel_uuid), SWITCH_LOG_DEBUG, \u0026#34;Pausing\\n\u0026#34;); context-\u0026gt;flags = 0; return SWITCH_STATUS_SUCCESS; } // 检查是否获取到 ASR 结果的方法。每个语音帧都会调用。 // **注意：这个方法的实现要保证幂等性。在asr_get_results方法被调用之前，这个方法多次调用的结果要保持一致 /*! function to read results from the ASR*/ static switch_status_t asr_check_results(switch_asr_handle_t *ah, switch_asr_flag_t *flags) { test_asr_t *context = (test_asr_t *) ah-\u0026gt;private_info; if (switch_test_flag(context, ASRFLAG_RETURNED_RESULT) || switch_test_flag(ah, SWITCH_ASR_FLAG_CLOSED)) { return SWITCH_STATUS_BREAK; } if (!switch_test_flag(context, ASRFLAG_RETURNED_START_OF_SPEECH) \u0026amp;\u0026amp; switch_test_flag(context, ASRFLAG_START_OF_SPEECH)) { return SWITCH_STATUS_SUCCESS; } if ((!switch_test_flag(context, ASRFLAG_RESULT)) \u0026amp;\u0026amp; (!switch_test_flag(context, ASRFLAG_NOINPUT_TIMEOUT))) { if (switch_test_flag(context, ASRFLAG_INPUT_TIMERS) \u0026amp;\u0026amp; !(switch_test_flag(context, ASRFLAG_START_OF_SPEECH)) \u0026amp;\u0026amp; context-\u0026gt;no_input_timeout \u0026gt;= 0 \u0026amp;\u0026amp; (switch_micro_time_now() - context-\u0026gt;no_input_time) / 1000 \u0026gt;= context-\u0026gt;no_input_timeout) { switch_log_printf(SWITCH_CHANNEL_UUID_LOG(context-\u0026gt;channel_uuid), SWITCH_LOG_DEBUG, \u0026#34;NO INPUT TIMEOUT %\u0026#34; SWITCH_TIME_T_FMT \u0026#34;ms\\n\u0026#34;, (switch_micro_time_now() - context-\u0026gt;no_input_time) / 1000); switch_set_flag(context, ASRFLAG_NOINPUT_TIMEOUT); } else if (switch_test_flag(context, ASRFLAG_START_OF_SPEECH) \u0026amp;\u0026amp; context-\u0026gt;speech_timeout \u0026gt; 0 \u0026amp;\u0026amp; (switch_micro_time_now() - context-\u0026gt;speech_time) / 1000 \u0026gt;= context-\u0026gt;speech_timeout) { switch_log_printf(SWITCH_CHANNEL_UUID_LOG(context-\u0026gt;channel_uuid), SWITCH_LOG_DEBUG, \u0026#34;SPEECH TIMEOUT %\u0026#34; SWITCH_TIME_T_FMT \u0026#34;ms\\n\u0026#34;, (switch_micro_time_now() - context-\u0026gt;speech_time) / 1000); if (switch_test_flag(context, ASRFLAG_START_OF_SPEECH)) { switch_set_flag(context, ASRFLAG_RESULT); } else { switch_set_flag(context, ASRFLAG_NOINPUT_TIMEOUT); } } } return switch_test_flag(context, ASRFLAG_RESULT) || switch_test_flag(context, ASRFLAG_NOINPUT_TIMEOUT) ? SWITCH_STATUS_SUCCESS : SWITCH_STATUS_BREAK; } // 获取识别加过的方法，识别结果复制给 xmlstr 这个指针。返回 true 时 Freeswitch 会对外发送 detected-speech 事件 /*! function to read results from the ASR */ static switch_status_t asr_get_results(switch_asr_handle_t *ah, char **xmlstr, switch_asr_flag_t *flags) { udp_info_t *udp_info = (udp_info_t *)ah-\u0026gt;private_info; switch_log_printf(SWITCH_CHANNEL_UUID_LOG(udp_info-\u0026gt;uuid), SWITCH_LOG_INFO, \u0026#34;[[iflytekasr_callback--\u0026gt;asr_get_results]] get result start [%s] \\n\u0026#34;, udp_info-\u0026gt;uuid); connection_metadata::ptr metadata_it = globals.endpoint-\u0026gt;get_metadata(udp_info-\u0026gt;uuid); switch_mutex_lock(udp_info-\u0026gt;flag_mutex); int index = metadata_it-\u0026gt;get_m_last_index(); if (udp_info-\u0026gt;seq == index) { return SWITCH_STATUS_FALSE; } udp_info-\u0026gt;seq = index; switch_log_printf(SWITCH_CHANNEL_UUID_LOG(udp_info-\u0026gt;uuid), SWITCH_LOG_INFO, \u0026#34;[[iflytekasr_callback--\u0026gt;asr_get_results]] get result get mutex [%s] \\n\u0026#34;, udp_info-\u0026gt;uuid); *xmlstr = switch_mprintf(\u0026#34;%s\u0026#34;, get_switch_buffer_ptr(udp_info-\u0026gt;text_buffer)); switch_log_printf(SWITCH_CHANNEL_LOG, SWITCH_LOG_INFO, \u0026#34;[[iflytekasr_callback--\u0026gt;asr_get_result]] asr results is [%s] \\n\u0026#34;, *xmlstr); switch_buffer_zero(udp_info-\u0026gt;text_buffer); switch_mutex_unlock(udp_info-\u0026gt;flag_mutex); return SWITCH_STATUS_SUCCESS; } // 设置超时时间接口 /*! function to start input timeouts */ static switch_status_t asr_start_input_timers(switch_asr_handle_t *ah) { test_asr_t *context = (test_asr_t *) ah-\u0026gt;private_info; if (switch_test_flag(ah, SWITCH_ASR_FLAG_CLOSED)) { switch_log_printf(SWITCH_CHANNEL_LOG, SWITCH_LOG_ERROR, \u0026#34;asr_start_input_timers attempt on CLOSED asr handle\\n\u0026#34;); return SWITCH_STATUS_FALSE; } switch_log_printf(SWITCH_CHANNEL_UUID_LOG(context-\u0026gt;channel_uuid), SWITCH_LOG_DEBUG, \u0026#34;start_input_timers\\n\u0026#34;); if (!switch_test_flag(context, ASRFLAG_INPUT_TIMERS)) { switch_set_flag(context, ASRFLAG_INPUT_TIMERS); context-\u0026gt;no_input_time = switch_micro_time_now(); } else { switch_log_printf(SWITCH_CHANNEL_UUID_LOG(context-\u0026gt;channel_uuid), SWITCH_LOG_INFO, \u0026#34;Input timers already started\\n\u0026#34;); } return SWITCH_STATUS_SUCCESS; } // 加载参数 /*! set text parameter */ static void asr_text_param(switch_asr_handle_t *ah, char *param, const char *val) { test_asr_t *context = (test_asr_t *) ah-\u0026gt;private_info; if (!zstr(param) \u0026amp;\u0026amp; !zstr(val)) { int nval = atoi(val); double fval = atof(val); if (!strcasecmp(\u0026#34;no-input-timeout\u0026#34;, param) \u0026amp;\u0026amp; switch_is_number(val)) { context-\u0026gt;no_input_timeout = nval; switch_log_printf(SWITCH_CHANNEL_UUID_LOG(context-\u0026gt;channel_uuid), SWITCH_LOG_DEBUG, \u0026#34;no-input-timeout = %d\\n\u0026#34;, context-\u0026gt;no_input_timeout); } else if (!strcasecmp(\u0026#34;speech-timeout\u0026#34;, param) \u0026amp;\u0026amp; switch_is_number(val)) { context-\u0026gt;speech_timeout = nval; switch_log_printf(SWITCH_CHANNEL_UUID_LOG(context-\u0026gt;channel_uuid), SWITCH_LOG_DEBUG, \u0026#34;speech-timeout = %d\\n\u0026#34;, context-\u0026gt;speech_timeout); } else if (!strcasecmp(\u0026#34;start-input-timers\u0026#34;, param)) { context-\u0026gt;start_input_timers = switch_true(val); if (context-\u0026gt;start_input_timers) { switch_set_flag(context, ASRFLAG_INPUT_TIMERS); } else { switch_clear_flag(context, ASRFLAG_INPUT_TIMERS); } switch_log_printf(SWITCH_CHANNEL_UUID_LOG(context-\u0026gt;channel_uuid), SWITCH_LOG_DEBUG, \u0026#34;start-input-timers = %d\\n\u0026#34;, context-\u0026gt;start_input_timers); } else if (!strcasecmp(\u0026#34;vad-mode\u0026#34;, param)) { switch_log_printf(SWITCH_CHANNEL_UUID_LOG(context-\u0026gt;channel_uuid), SWITCH_LOG_DEBUG, \u0026#34;vad-mode = %s\\n\u0026#34;, val); if (context-\u0026gt;vad) switch_vad_set_mode(context-\u0026gt;vad, nval); } else if (!strcasecmp(\u0026#34;vad-voice-ms\u0026#34;, param) \u0026amp;\u0026amp; nval \u0026gt; 0) { context-\u0026gt;voice_ms = nval; switch_vad_set_param(context-\u0026gt;vad, \u0026#34;voice_ms\u0026#34;, nval); } else if (!strcasecmp(\u0026#34;vad-silence-ms\u0026#34;, param) \u0026amp;\u0026amp; nval \u0026gt; 0) { context-\u0026gt;silence_ms = nval; switch_vad_set_param(context-\u0026gt;vad, \u0026#34;silence_ms\u0026#34;, nval); } else if (!strcasecmp(\u0026#34;vad-thresh\u0026#34;, param) \u0026amp;\u0026amp; nval \u0026gt; 0) { context-\u0026gt;thresh = nval; switch_vad_set_param(context-\u0026gt;vad, \u0026#34;thresh\u0026#34;, nval); } else if (!strcasecmp(\u0026#34;channel-uuid\u0026#34;, param)) { context-\u0026gt;channel_uuid = switch_core_strdup(ah-\u0026gt;memory_pool, val); switch_log_printf(SWITCH_CHANNEL_UUID_LOG(context-\u0026gt;channel_uuid), SWITCH_LOG_DEBUG, \u0026#34;channel-uuid = %s\\n\u0026#34;, val); } else if (!strcasecmp(\u0026#34;result\u0026#34;, param)) { context-\u0026gt;result_text = switch_core_strdup(ah-\u0026gt;memory_pool, val); switch_log_printf(SWITCH_CHANNEL_UUID_LOG(context-\u0026gt;channel_uuid), SWITCH_LOG_DEBUG, \u0026#34;result = %s\\n\u0026#34;, val); } else if (!strcasecmp(\u0026#34;confidence\u0026#34;, param) \u0026amp;\u0026amp; fval \u0026gt;= 0.0) { context-\u0026gt;result_confidence = fval; switch_log_printf(SWITCH_CHANNEL_UUID_LOG(context-\u0026gt;channel_uuid), SWITCH_LOG_DEBUG, \u0026#34;confidence = %f\\n\u0026#34;, fval); } else if (!strcasecmp(\u0026#34;partial\u0026#34;, param) \u0026amp;\u0026amp; switch_true(val)) { context-\u0026gt;partial = 3; switch_log_printf(SWITCH_CHANNEL_UUID_LOG(context-\u0026gt;channel_uuid), SWITCH_LOG_DEBUG, \u0026#34;partial = %d\\n\u0026#34;, context-\u0026gt;partial); } } } /*! set numeric parameter */ static void asr_numeric_param(switch_asr_handle_t *ah, char *param, int val) { } /*! set float parameter */ static void asr_float_param(switch_asr_handle_t *ah, char *param, double val) { } 配置文件 自定义模块的配置文件参考，可以在加载模块时读取一些配置，保存到模块的全局变量当中\n1 2 3 4 5 6 7 \u0026lt;configuration name=\u0026#34;iflytekasr.conf\u0026#34; description=\u0026#34;ybs ws asr configuration\u0026#34;\u0026gt; \u0026lt;settings\u0026gt; \u0026lt;param name=\u0026#34;auto-reload\u0026#34; value=\u0026#34;true\u0026#34;/\u0026gt; \u0026lt;!-- save the wave file for debug --\u0026gt; \u0026lt;param name=\u0026#34;wav-file-dir\u0026#34; value=\u0026#34;/data/tmp/iflytekasr/\u0026#34;/\u0026gt; \u0026lt;/settings\u0026gt; \u0026lt;/configuration\u0026gt; WebSocketPP 模块 WebSocket 模块经过多种调研，选择使用了 WebSocketPP。\nWebsocketpp 需要依赖 boost，CentOS 安装方法如下，其他分发版本可以参考官方文档\n1 2 3 4 5 6 7 8 9 10 11 12 # 安装 boost yum install boost yum install boost-devel # 安装 websocketpp git clone https://github.com/zaphoyd/websocketpp.git cd websocketpp #进入目录 mkdir build \u0026amp;\u0026amp; cd build cmake .. #执行cmake sudo make sudo make install 测试 Demo 你可以进入该项目运行下面的脚本编译代码，启动 Server 端和 Client 端进行简单测试\n1 2 3 4 5 6 7 8 9 10 11 cd ../examples/echo_server g++ -o echo_server echo_server.cpp -lboost_system -lpthread -std=c++11 #编译链接echo_server ./echo_server #启动服务器端 cd ../examples/echo_client #编译链接echo_client g++ -o echo_client echo_client.cpp -lboost_system -lpthread -std=c++11 #启动客户端 ./echo_client 实现代码 下面代码定义了一个 endpoint 对象，提供了一些基础方法，如send、close等。该对象还维持了多个 WebSocket 链接对象，这些对象包含一些 如on_message的钩子方法。代码如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 #include \u0026lt;websocketpp/config/asio_no_tls_client.hpp\u0026gt; #include \u0026lt;websocketpp/client.hpp\u0026gt; #include \u0026lt;websocketpp/common/thread.hpp\u0026gt; #include \u0026lt;websocketpp/common/memory.hpp\u0026gt; typedef websocketpp::client\u0026lt;websocketpp::config::asio_client\u0026gt; client; static switch_mutex_t *MUTEX = NULL; class connection_metadata { public: typedef websocketpp::lib::shared_ptr\u0026lt;connection_metadata\u0026gt; ptr; connection_metadata(std::string uuid, websocketpp::connection_hdl hdl, std::string uri) : m_uuid(uuid), m_hdl(hdl), m_status(\u0026#34;Connecting\u0026#34;), m_uri(uri), m_server(\u0026#34;N/A\u0026#34;) { } void on_open(client *c, websocketpp::connection_hdl hdl) { m_status = \u0026#34;Open\u0026#34;; client::connection_ptr con = c-\u0026gt;get_con_from_hdl(hdl); m_server = con-\u0026gt;get_response_header(\u0026#34;Server\u0026#34;); } void on_fail(client *c, websocketpp::connection_hdl hdl) { m_status = \u0026#34;Failed\u0026#34;; client::connection_ptr con = c-\u0026gt;get_con_from_hdl(hdl); m_server = con-\u0026gt;get_response_header(\u0026#34;Server\u0026#34;); m_error_reason = con-\u0026gt;get_ec().message(); } void on_close(client *c, websocketpp::connection_hdl hdl) { m_status = \u0026#34;Closed\u0026#34;; client::connection_ptr con = c-\u0026gt;get_con_from_hdl(hdl); std::stringstream s; s \u0026lt;\u0026lt; \u0026#34;close code: \u0026#34; \u0026lt;\u0026lt; con-\u0026gt;get_remote_close_code() \u0026lt;\u0026lt; \u0026#34; (\u0026#34; \u0026lt;\u0026lt; websocketpp::close::status::get_string(con-\u0026gt;get_remote_close_code()) \u0026lt;\u0026lt; \u0026#34;), close reason: \u0026#34; \u0026lt;\u0026lt; con-\u0026gt;get_remote_close_reason(); m_error_reason = s.str(); } void on_message(websocketpp::connection_hdl, client::message_ptr msg) { if (msg-\u0026gt;get_opcode() == websocketpp::frame::opcode::text) { m_messages.push_back(msg-\u0026gt;get_payload()); } else { m_messages.push_back(websocketpp::utility::to_hex(msg-\u0026gt;get_payload())); } // std::cout \u0026lt;\u0026lt; \u0026#34;\u0026gt; recived message is: \u0026#34; \u0026lt;\u0026lt; msg-\u0026gt;get_payload() \u0026lt;\u0026lt; \u0026#34;index is: \u0026#34; \u0026lt;\u0026lt; m_messages.size() \u0026lt;\u0026lt; std::endl; } websocketpp::connection_hdl get_hdl() const { return m_hdl; } std::string get_uuid() const { return m_uuid; } std::string get_status() const { return m_status; } int get_size() { return m_messages.size(); } std::string get_message(int index) { return m_messages[index]; } friend std::ostream \u0026amp;operator\u0026lt;\u0026lt;(std::ostream \u0026amp;out, connection_metadata const \u0026amp;data); private: std::string m_uuid; websocketpp::connection_hdl m_hdl; std::string m_status; std::string m_uri; std::string m_server; std::string m_error_reason; std::vector\u0026lt;std::string\u0026gt; m_messages; }; std::ostream \u0026amp;operator\u0026lt;\u0026lt;(std::ostream \u0026amp;out, connection_metadata const \u0026amp;data) { out \u0026lt;\u0026lt; \u0026#34;\u0026gt; URI: \u0026#34; \u0026lt;\u0026lt; data.m_uri \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34; \u0026lt;\u0026lt; \u0026#34;\u0026gt; Status: \u0026#34; \u0026lt;\u0026lt; data.m_status \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34; \u0026lt;\u0026lt; \u0026#34;\u0026gt; Remote Server: \u0026#34; \u0026lt;\u0026lt; (data.m_server.empty() ? \u0026#34;None Specified\u0026#34; : data.m_server) \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34; \u0026lt;\u0026lt; \u0026#34;\u0026gt; Error/close reason: \u0026#34; \u0026lt;\u0026lt; (data.m_error_reason.empty() ? \u0026#34;N/A\u0026#34; : data.m_error_reason) \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; out \u0026lt;\u0026lt; \u0026#34;\u0026gt; Messages Processed: (\u0026#34; \u0026lt;\u0026lt; data.m_messages.size() \u0026lt;\u0026lt; \u0026#34;) \\n\u0026#34;; std::vector\u0026lt;std::string\u0026gt;::const_iterator it; for (it = data.m_messages.begin(); it != data.m_messages.end(); ++it) { out \u0026lt;\u0026lt; *it \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; } return out; } class websocket_endpoint { public: websocket_endpoint() : m_next_id(0) { m_endpoint.clear_error_channels(websocketpp::log::elevel::none); m_endpoint.set_access_channels(websocketpp::log::alevel::none); m_endpoint.clear_access_channels(websocketpp::log::alevel::none); // m_endpoint.set_access_channels(websocketpp::log::alevel::all); // m_endpoint.clear_access_channels(websocketpp::log::alevel::frame_payload); m_endpoint.init_asio(); m_endpoint.start_perpetual(); m_thread = websocketpp::lib::make_shared\u0026lt;websocketpp::lib::thread\u0026gt;(\u0026amp;client::run, \u0026amp;m_endpoint); } ~websocket_endpoint() { m_endpoint.stop_perpetual(); for (con_list::const_iterator it = m_connection_list.begin(); it != m_connection_list.end(); ++it) { if (it-\u0026gt;second-\u0026gt;get_status() != \u0026#34;Open\u0026#34;) { // Only close open connections continue; } std::cout \u0026lt;\u0026lt; \u0026#34;\u0026gt; Closing connection \u0026#34; \u0026lt;\u0026lt; it-\u0026gt;second-\u0026gt;get_uuid() \u0026lt;\u0026lt; std::endl; websocketpp::lib::error_code ec; m_endpoint.close(it-\u0026gt;second-\u0026gt;get_hdl(), websocketpp::close::status::going_away, \u0026#34;\u0026#34;, ec); if (ec) { std::cout \u0026lt;\u0026lt; \u0026#34;\u0026gt; Error closing connection \u0026#34; \u0026lt;\u0026lt; it-\u0026gt;second-\u0026gt;get_uuid() \u0026lt;\u0026lt; \u0026#34;: \u0026#34; \u0026lt;\u0026lt; ec.message() \u0026lt;\u0026lt; std::endl; } } m_thread-\u0026gt;join(); } int connect(std::string uuid, std::string const \u0026amp;uri) { websocketpp::lib::error_code ec; client::connection_ptr con = m_endpoint.get_connection(uri, ec); if (ec) { std::cout \u0026lt;\u0026lt; \u0026#34;\u0026gt; Connect initialization error: \u0026#34; \u0026lt;\u0026lt; ec.message() \u0026lt;\u0026lt; std::endl; return -1; } connection_metadata::ptr metadata_ptr = websocketpp::lib::make_shared\u0026lt;connection_metadata\u0026gt;(uuid, con-\u0026gt;get_handle(), uri); m_connection_list[uuid] = metadata_ptr; con-\u0026gt;set_open_handler(websocketpp::lib::bind( \u0026amp;connection_metadata::on_open, metadata_ptr, \u0026amp;m_endpoint, websocketpp::lib::placeholders::_1)); con-\u0026gt;set_fail_handler(websocketpp::lib::bind( \u0026amp;connection_metadata::on_fail, metadata_ptr, \u0026amp;m_endpoint, websocketpp::lib::placeholders::_1)); con-\u0026gt;set_close_handler(websocketpp::lib::bind( \u0026amp;connection_metadata::on_close, metadata_ptr, \u0026amp;m_endpoint, websocketpp::lib::placeholders::_1)); con-\u0026gt;set_message_handler(websocketpp::lib::bind( \u0026amp;connection_metadata::on_message, metadata_ptr, websocketpp::lib::placeholders::_1, websocketpp::lib::placeholders::_2)); m_endpoint.connect(con); return 1; } void close(std::string uuid, websocketpp::close::status::value code, std::string reason) { websocketpp::lib::error_code ec; con_list::iterator metadata_it = m_connection_list.find(uuid); if (metadata_it == m_connection_list.end()) { std::cout \u0026lt;\u0026lt; \u0026#34;\u0026gt; No connection found with uuid \u0026#34; \u0026lt;\u0026lt; uuid \u0026lt;\u0026lt; std::endl; return; } m_endpoint.close(metadata_it-\u0026gt;second-\u0026gt;get_hdl(), code, reason, ec); if (ec) { std::cout \u0026lt;\u0026lt; \u0026#34;\u0026gt; Error initiating close: \u0026#34; \u0026lt;\u0026lt; ec.message() \u0026lt;\u0026lt; std::endl; } m_connection_list.erase(uuid); } void send(std::string uuid, std::string message) { websocketpp::lib::error_code ec; con_list::iterator metadata_it = m_connection_list.find(uuid); if (metadata_it == m_connection_list.end()) { std::cout \u0026lt;\u0026lt; \u0026#34;\u0026gt; No connection found with uuid \u0026#34; \u0026lt;\u0026lt; uuid \u0026lt;\u0026lt; std::endl; return; } m_endpoint.send(metadata_it-\u0026gt;second-\u0026gt;get_hdl(), message, websocketpp::frame::opcode::text, ec); if (ec) { std::cout \u0026lt;\u0026lt; \u0026#34;\u0026gt; Error sending message: \u0026#34; \u0026lt;\u0026lt; ec.message() \u0026lt;\u0026lt; std::endl; return; } } void sendBinary(std::string uuid, std::string message) { websocketpp::lib::error_code ec; con_list::iterator metadata_it = m_connection_list.find(uuid); if (metadata_it == m_connection_list.end()) { std::cout \u0026lt;\u0026lt; \u0026#34;\u0026gt; No connection found with uuid \u0026#34; \u0026lt;\u0026lt; uuid \u0026lt;\u0026lt; std::endl; return; } m_endpoint.send(metadata_it-\u0026gt;second-\u0026gt;get_hdl(), message, websocketpp::frame::opcode::binary, ec); if (ec) { std::cout \u0026lt;\u0026lt; \u0026#34;\u0026gt; Error sending message: \u0026#34; \u0026lt;\u0026lt; ec.message() \u0026lt;\u0026lt; std::endl; return; } } connection_metadata::ptr get_metadata(std::string uuid) const { con_list::const_iterator metadata_it = m_connection_list.find(uuid); if (metadata_it == m_connection_list.end()) { return connection_metadata::ptr(); } else { return metadata_it-\u0026gt;second; } } private: typedef std::map\u0026lt;std::string, connection_metadata::ptr\u0026gt; con_list; client m_endpoint; websocketpp::lib::shared_ptr\u0026lt;websocketpp::lib::thread\u0026gt; m_thread; con_list m_connection_list; int m_next_id; }; 安装 mod_iflytekasr Makefile 参考如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 FREESWITCH_MOD_PATH=/usr/local/freeswitch/mod MODNAME = mod_iflytekasr.so MODOBJ = mod_iflytekasr.o base64.o MODCFLAGS = -Wall -Wno-unused-function -I/usr/src/freeswitch/src/include -I/usr/src/freeswitch/libs/libteletone/src -lboost_system -lpthread CC = g++ CPPFLAGS = -fPIC -std=c++11 -g $(MODCFLAGS) LDFLAGS = $(MODLDFLAGS) all: $(MODNAME) $(MODNAME): $(MODOBJ) @$(CC) -shared $(CPPFLAGS) -o $@ $(MODOBJ) $(LDFLAGS) .PHONY: all clean clean: rm -f $(MODNAME) $(MODOBJ) install: $(MODNAME) install -d $(FREESWITCH_MOD_PATH) install $(MODNAME) $(FREESWITCH_MOD_PATH) mod_iflytekasr.o: mod_iflytekasr.cpp base64.o: base64.cpp base64.h 在 ${FREESWITCH_SOURCE_ROOT}/modules.conf 添加以下两个模块 1 asr_tts/mod_iflytekasr 通过将以下行添加到 ${FREESWITCH_INSTALLATION_ROOT}/conf/autoload_configs/modules.conf.xml 来激活 mod_iflytekasr\n1 \u0026lt;load module=\u0026#34;mod_iflytekasr\u0026#34;/\u0026gt; 将模块复制到 ${FREESWITCH_SOURCE_ROOT}/src/mod/asr_tts/ 目录下 1 2 cd mod_iflytekasr make install 重启 freeswitch 加载模块\n复制以下lua脚本到${FREESWITCH_INSTALLATION_ROOT}/scripts/里\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 -- asr text for one round local asr_text = nil; -- 通话的事件都会回调这个接口 -- This is the input callback used by dtmf or any other events on this session such as ASR. function onInput(s, type, obj) freeswitch.consoleLog(\u0026#34;info\u0026#34;, \u0026#34;Callback with type \u0026#34; .. type .. \u0026#34;\\n\u0026#34;); -- freeswitch.consoleLog(\u0026#34;info\u0026#34;, \u0026#34;s=\u0026#34; .. s .. \u0026#34;\\n\u0026#34;); freeswitch.consoleLog(\u0026#34;info\u0026#34;, \u0026#34;obj=\u0026#34; .. obj:serialize() .. \u0026#34;\\n\u0026#34;); if (type == \u0026#34;dtmf\u0026#34;) then freeswitch.consoleLog(\u0026#34;info\u0026#34;, \u0026#34;DTMF Digit: \u0026#34; .. obj.digit .. \u0026#34;\\n\u0026#34;); elseif (type == \u0026#34;event\u0026#34;) then local event = obj:getHeader(\u0026#34;Speech-Type\u0026#34;); if (event == \u0026#34;begin-speaking\u0026#34;) then freeswitch.consoleLog(\u0026#34;info\u0026#34;, \u0026#34;speaking=\u0026#34; .. obj:serialize() .. \u0026#34;\\n\u0026#34;); -- Return break on begin-speaking events to stop playback of the fire or tts. return \u0026#34;break\u0026#34;; end if (event == \u0026#34;detected-speech\u0026#34;) then -- freeswitch.consoleLog(\u0026#34;info\u0026#34;, \u0026#34;\\n\u0026#34; .. obj:serialize() .. \u0026#34;\\n\u0026#34;); local text = obj:getBody(); if (text ~= \u0026#34;(null)\u0026#34;) then -- Pause speech detection (this is on auto but pausing it just in case) session:execute(\u0026#34;detect_speech\u0026#34;, \u0026#34;pause\u0026#34;); -- Parse the results from the event into the results table for later use. -- results = getResults(obj:getBody()); -- set the global asr text for later use asr_text = text; end -- return \u0026#34;break\u0026#34;; end end end session:answer(); -- Register the input callback session:setInputCallback(\u0026#34;onInput\u0026#34;); -- Sleep a little bit to get media time to be fully up session:sleep(100); session:streamFile(\u0026#34;hello.wav\u0026#34;); session:execute(\u0026#34;detect_speech\u0026#34;, \u0026#34;test directory directory\u0026#34;); -- keep the thread alive while (session:ready() == true) do if (asr_text == nil) then session:sleep(20); elseif (asr_text == \u0026#34;\u0026#34;) then session:streamFile(\u0026#34;I didn\u0026#39;t hear you.wav\u0026#34;); asr_text = nil; session:execute(\u0026#34;detect_speech\u0026#34;, \u0026#34;resume\u0026#34;); else -- do your NLU here ? -- echo back the recognition result session:streamFile(\u0026#34;Yes.wav\u0026#34;); asr_text = nil; session:execute(\u0026#34;detect_speech\u0026#34;, \u0026#34;resume\u0026#34;); end end -- stop the detect_speech and hangup session:execute(\u0026#34;detect_speech\u0026#34;, \u0026#34;stop\u0026#34;); session:sleep(1000); session:hangup(); ","date":"2024-03-25T00:00:00Z","image":"https://cyber-blog.github.io/p/fs-asr-api-connection/cover_hu5846421448488347028.png","permalink":"https://cyber-blog.github.io/p/fs-asr-api-connection/","title":"FreeSWITCH ASR 接口对接"},{"content":"MySQL 索引结构与执行流程：从 B+ 树到联合索引与索引下推全解析 本篇文章从 MySQL 为什么选择 B+ 树索引开始，逐步引出联合索引的最左匹配原则、索引下推（ICP）、回表机制和覆盖索引等关键执行细节，帮助你从结构、原理到实际执行路径深入理解索引背后的逻辑。\n一、为什么 MySQL 选择 B+ 树而非 B 树？ MySQL 中使用最广泛的 InnoDB 存储引擎采用 B+ 树 作为索引结构，这种选择并非偶然。\n1. B 树 vs B+ 树 的结构差异 特性 B 树 B+ 树 数据存储 所有节点都存数据 仅叶子节点存数据，内部节点只存键 查询路径 查询可能在中间节点结束 查询必须走到叶子节点 范围扫描 需要中序遍历整棵树 叶子节点通过链表串联，天然支持范围查询 2. 为什么 B+ 树更适合数据库索引？ 磁盘访问效率更高：非叶子节点更小，内存缓存命中率更高，减少磁盘 IO；\n范围查询更高效：链表串联叶子节点，直接顺序扫描即可；\n结构稳定、查询均衡：所有查找都走到叶子节点，路径统一，优化更容易；\n排序与全表扫描友好：天然有序，支持快速 ORDER BY；\n📌 总结：B+ 树对磁盘 I/O、范围查、缓存友好，全面优于 B 树，是关系型数据库的首选。\n二、联合索引与最左匹配原则 1. 什么是联合索引？ 联合索引是将多个字段作为一个整体创建的复合索引，例如：\n1 CREATE INDEX idx_abc ON user(a, b, c); 在使用联合索引 (a, b, c) 时，会把每一行的 a、b、c 作为组合键 (a, b, c) 排序成一棵 B+ 树，其比较顺序就是字典序。它在 B+ 树的叶子节点上，按照 a -\u0026gt; b -\u0026gt; c 的顺序排列。\nB+树索引示例\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 Root Node +-----------------+ | (2,1,1) | +-----------------+ / \\ / \\ +------------------+ +------------------+ ← 中间节点 | (1,2,1) | | (2,2,1) | +------------------+ +------------------+ / \\ / \\ / \\ / \\ Leaf1 Leaf2 Leaf3 Leaf4 ← 叶子节点（有序链表） +----------+ +----------+ +----------+ +----------+ |1,1,1 | → |1,1,2 | →|1,2,1 | →|2,1,1 | +----------+ +----------+ +----------+ +----------+ ↓ +----------+ |2,2,1 | → (尾部 null) +----------+ 2. 为什么要遵循最左匹配原则？ 只有从最左侧字段开始连续使用，索引才能被利用。例如：\nWHERE 条件 是否命中 idx_abc WHERE a = 1 ✅ 使用 a WHERE a = 1 AND b = 2 ✅ 使用 a、b WHERE b = 2 ❌ 不使用联合索引 WHERE a = 1 AND c = 3 ✅ 使用 a，但 c 不触发 联合索引中的比较（如 \u0026lt; (2,1,1)）是基于联合键的字典序，而最左前缀原则则是限制你必须从前往后匹配字段，否则 B+ 树无法正确判断搜索路径。\n三、联合索引查询执行流程（含回表、索引下推、覆盖索引） 我们通过一个实际查询来讲解完整的执行流程：\n1. 表结构与查询示例 1 2 3 4 5 6 7 8 9 10 CREATE TABLE user ( id INT PRIMARY KEY, a INT, b INT, c INT, d VARCHAR(100), KEY idx_abc (a, b, c) ); SELECT c FROM user WHERE a = 1 AND b = 2 AND c \u0026gt; 3; 2. 执行流程（含 Server 层与 InnoDB 层分工） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 【1】客户端发送 SQL 查询： SELECT d FROM t WHERE a = 1 AND c = 2; 【2】Server 层： ├─ 解析 SQL，生成执行计划 ├─ 确认可以使用联合索引 idx_a_b_c（a 是最左字段） ├─ 判断：c 不满足最左前缀 → 不能用于定位 └─ 但是 ✅ 可以使用 Index Condition Pushdown（ICP） ↓ 【3】Server 层下发执行计划 + 下推过滤条件 (c=2) 到 InnoDB ↓ 【4】InnoDB 二级索引扫描 idx_a_b_c： ├─ 根据 a=1 扫描对应索引区间 ├─ 对每一个记录，**在扫描索引的同时检查 c=2** └─ ✅ 满足 c=2 的主键 id 被保留，其他跳过（减少回表） ↓ 【5】返回匹配的主键 id（如 id=123、id=456）给 Server ↓ 【6】Server 层判断： └─ 字段 d 不在索引中 ❌ → 必须回表 ↓ 【7】InnoDB 根据主键 ID 回表： ├─ 检查主键页是否在 Buffer Pool（缓存） │ └─ ✅ 在内存 → 直接读取 │ └─ ❌ 不在内存 → 触发磁盘 IO └─ 返回完整行数据（含 d 字段） ↓ 【8】Server 层组装最终结果，返回给客户端 3. 术语解析 ✅ 索引下推（ICP）：将 c \u0026gt; 3 的判断提前到 InnoDB 层，在遍历索引时就过滤不符合的行，避免大量无效回表；\n✅ 覆盖索引：查询字段 c 已经在索引 idx_abc 中，直接从叶子节点读取，无需访问主键页（聚簇索引），提高效率；\n❌ 回表：如果 SELECT 中包含非索引字段（如 d），则需根据主键 id 再去聚簇索引中查找整行数据，增加开销；\n四、Hash 索引对比补充：适合极端等值查询 1. Hash 索引特点（多用于 Memory 引擎） 特性 Hash 索引 B+ 树索引 查找效率 ✅ 等值查询最快 ✅ 稍慢但功能更全 范围查询 ❌ 不支持 ✅ 支持 排序 ❌ 不支持 ✅ 支持（叶子链表） 支持引擎 仅 Memory，或自适应哈希 InnoDB、MyISAM、Memory 等 2. InnoDB 的自适应哈希索引（AHI） InnoDB 会自动监控某些 B+ 树热点页，自动为其建立 Hash 索引以优化等值查询，这对开发者透明、不可控制。\n五、InnoDB 无主键表时会如何查询 自动生成隐藏主键：\n若表无显式主键或唯一非空索引，InnoDB 会自动添加一个 6字节隐藏 RowID，作为聚簇索引的 key。\n二级索引指向 RowID：\n所有二级索引将指向这个隐藏 RowID，而不是显式主键。\n查询过程与回表：\n查询时先通过二级索引查出 RowID；\n然后根据 RowID 去聚簇索引中“回表”获取整行数据；\n返回所需字段。\n性能与维护影响：\n无法控制 RowID 排序与结构；\n查询效率和调优难度高于有主键表；\n不利于数据管理和故障排查。\n六、Mysql 深分页的查询优化总结\n五、总结与优化建议 ✅ B+ 树索引是数据库查询的核心结构，范围查、排序查都依赖它；\n✅ 联合索引需注意“最左前缀”匹配，字段顺序设计极关键；\n✅ ICP（索引下推）可减少回表成本，提高查询效率；\n✅ 覆盖索引是查询最优形态，能避免访问主键页；\n✅ Hash 索引虽快，但仅适合极简等值查找场景。\n📌 优化索引设计的核心策略：\n通过合理设计联合索引顺序 + 使用覆盖索引 + 启用 ICP\n从根源上减少回表次数，才是提升 MySQL 查询性能的核心。\n","date":"2023-10-24T00:00:00Z","permalink":"https://cyber-blog.github.io/p/mysql-%E6%9F%A5%E8%AF%A2%E6%B5%81%E7%A8%8B/","title":"Mysql 查询流程"},{"content":"MVCC（多版本并发控制） MVCC（Multi-Version Concurrency Control，多版本并发控制）是一种用于解决数据库中并发事务问题的机制。它通过为每个事务提供数据库中数据的一个快照（Snapshot），允许多个事务同时读取和写入数据，从而实现高效的并发控制和事务隔离。\n事务是由 MySQL 的引擎来实现的，我们常见的 InnoDB 引擎它是支持事务的。\n不过并不是所有的引擎都能支持事务，比如 MySQL 原生的 MyISAM 引擎就不支持事务，也正是这样，所以大多数 MySQL 的引擎都是用 InnoDB。\n事务看起来感觉简单，但是要实现事务必须要遵守 4 个特性，分别如下：\n原子性（Atomicity）：一个事务中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节，而且事务在执行过程中发生错误，会被回滚到事务开始前的状态，就像这个事务从来没有执行过一样，就好比买一件商品，购买成功时，则给商家付了钱，商品到手；购买失败时，则商品在商家手中，消费者的钱也没花出去。 一致性（Consistency）：是指事务操作前和操作后，数据满足完整性约束，数据库保持一致性状态。 隔离性（Isolation）：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致，因为多个事务同时使用相同的数据时，不会相互干扰，每个事务都有一个完整的数据空间，对其他并发事务是隔离的。也就是说，消费者购买商品这个事务，是不影响其他消费者购买的。 持久性（Durability）：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 InnoDB 引擎通过以下技术来保证事务的这四个特性\n持久性是通过 redo log （重做日志）来保证的； 原子性是通过 undo log（回滚日志） 来保证的； 隔离性是通过 MVCC（多版本并发控制） 或锁机制来保证的； 一致性则是通过持久性+原子性+隔离性来保证； 数据库的四种隔离级别 隔离级别 脏读 不可重复读 幻读 READ UNCOMMITTED：未提交读 可能发生 可能发生 可能发生 READ COMMITTED：已提交读 解决 可能发生 可能发生 REPEATABLE READ：可重复读 解决 解决 可能发生 SERIALIZABLE：可串行化 解决 解决 解决 为什么没有脏写？\n四种问题按照严重性排序：脏写 \u0026gt; 脏读 \u0026gt; 不可重复读 \u0026gt; 幻读\n脏写这个问题太严重了，不论是哪种隔离级别，都不允许脏写的情况发生。\nSQL 标准提出了四种隔离级别来规避这些现象，隔离级别越高，性能效率就越低，这四个隔离级别如下：\n读未提交（read uncommitted），指一个事务还没提交时，它做的变更就能被其他事务看到； 读提交（read committed），指一个事务提交之后，它做的变更才能被其他事务看到； 可重复读（repeatable read），指一个事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，MySQL InnoDB 引擎的默认隔离级别； 串行化（serializable ）；会对记录加上读写锁，在多个事务对这条记录进行读写操作时，如果发生了读写冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行； 所以，要解决脏读现象，就要升级到「读提交」以上的隔离级别；要解决不可重复读现象，就要升级到「可重复读」的隔离级别，要解决幻读现象不建议将隔离级别升级到「串行化」。\n不同的数据库厂商对 SQL 标准中规定的 4 种隔离级别的支持不一样，有的数据库只实现了其中几种隔离级别，我们讨论的 MySQL 虽然支持 4 种隔离级别，但是与SQL 标准中规定的各级隔离级别允许发生的现象却有些出入。\nMySQL 在「可重复读」隔离级别下，可以很大程度上避免幻读现象的发生（注意是很大程度避免，并不是彻底避免），所以 MySQL 并不会使用「串行化」隔离级别来避免幻读现象的发生，因为使用「串行化」隔离级别会影响性能。\nMySQL InnoDB 引擎的默认隔离级别虽然是「可重复读」，但是它很大程度上避免幻读现象（并不是完全解决了。解决的方案有两种：\n针对快照读（普通 select 语句），是通过 MVCC 方式解决了幻读，因为可重复读隔离级别下，事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，即使中途有其他事务插入了一条数据，是查询不出来这条数据的，所以就很好了避免幻读问题。 针对当前读（select \u0026hellip; for update 等语句），是通过 next-key lock（记录锁+间隙锁）方式解决了幻读，因为当执行 select \u0026hellip; for update 语句的时候，会加上 next-key lock，如果有其他事务在 next-key lock 锁范围内插入了一条记录，那么这个插入语句就会被阻塞，无法成功插入，所以就很好了避免幻读问题。 隔离级别实现方式 对于「读未提交」隔离级别的事务来说，因为可以读到未提交事务修改的数据，所以直接读取最新的数据就好了； 对于「串行化」隔离级别的事务来说，通过加读写锁的方式来避免并行访问； 对于「读提交」和「可重复读」隔离级别的事务来说，它们是通过 Read View 来实现的，它们的区别在于创建 Read View 的时机不同，大家可以把 Read View 理解成一个数据快照，就像相机拍照那样，定格某一时刻的风景。「读提交」隔离级别是在「每个语句执行前」都会重新生成一个 Read View，而「可重复读」隔离级别是「启动事务时」生成一个 Read View，然后整个事务期间都在用这个 Read View**。 Read View Read View 主要有以下两部分组成：\nRead View 中四个字段作用； 聚簇索引记录中两个跟事务有关的隐藏列； Read View 有四个重要的字段：\nm_ids ：指的是在创建 Read View 时，当前数据库中「活跃事务」的事务 id 列表，注意是一个列表，“活跃事务”指的就是，启动了但还没提交的事务。 min_trx_id ：指的是在创建 Read View 时，当前数据库中「活跃事务」中事务 id 最小的事务，也就是 m_ids 的最小值。 max_trx_id ：这个并不是 m_ids 的最大值，而是创建 Read View 时当前数据库中应该给下一个事务的 id 值，也就是全局事务中最大的事务 id 值 + 1； creator_trx_id ：指的是创建该 Read View 的事务的事务 id。 对于使用 InnoDB 存储引擎的数据库表，它的聚簇索引记录中都包含下面两个隐藏列：\ntrx_id，当一个事务对某条聚簇索引记录进行改动时，就会把该事务的事务 id 记录在 trx_id 隐藏列里； roll_pointer，每次对某条聚簇索引记录进行改动时，都会把旧版本的记录写入到 undo 日志中，然后这个隐藏列是个指针，指向每一个旧版本记录，于是就可以通过它找到修改前的记录。 row_id，行ID，唯一标识一条记录（如果定义主键，它就没有啦） 在创建 Read View 后，我们可以将记录中的 trx_id 划分这三种情况：\n一个事务去访问记录的时候，除了自己的更新记录总是可见之外，还有这几种情况：\n如果记录的 trx_id 值小于 Read View 中的 min_trx_id 值，表示这个版本的记录是在创建 Read View 前已经提交的事务生成的，所以该版本的记录对当前事务可见。 如果记录的 trx_id 值大于等于 Read View 中的 max_trx_id 值，表示这个版本的记录是在创建 Read View 后才启动的事务生成的，所以该版本的记录对当前事务不可见。 如果记录的 trx_id 值在 Read View 的 min_trx_id 和 max_trx_id 之间，需要判断 trx_id 是否在 m_ids 列表中： 如果记录的 trx_id 在 m_ids 列表中，表示生成该版本记录的活跃事务依然活跃着（还没提交事务），所以该版本的记录对当前事务不可见。 如果记录的 trx_id 不在 m_ids列表中，表示生成该版本记录的活跃事务已经被提交，所以该版本的记录对当前事务可见。 这种通过「版本链」来控制并发事务访问同一个记录时的行为就叫 MVCC（多版本并发控制）。\n对于可重复读 可重复读隔离级别是启动事务时生成一个 Read View，然后整个事务期间都在用这个 Read View。\n对于读已提交 读提交隔离级别是在每次读取数据时，都会生成一个新的 Read View。\n","date":"2023-08-24T00:00:00Z","permalink":"https://cyber-blog.github.io/p/mvcc%E6%80%BB%E7%BB%93/","title":"MVCC总结"},{"content":"M1 macOS Monterey 12.6 (21G115) Xcode版本14.0.1\nJDK版本17\n初始化配置\n1 bash configure --enable-debug --with-jvm-variants=client,server --disable-warnings-as-errors 编译\n1 make 遇到的坑 https://bugs.openjdk.org/browse/JDK-8272700 https://github.com/tarantool/tarantool/issues/6576 切换为jdk17u-dev之后的问题 https://bugs.openjdk.org/browse/JDK-8283221\n详细配置和步骤参考官网\nXcode版本过新参考\nProblems with the Build Environment Make sure your configuration is correct. Re-run configure, and look for any warnings. Warnings that appear in the middle of the configure output is also repeated at the end, after the summary. The entire log is stored in $BUILD/configure.log.\nVerify that the summary at the end looks correct. Are you indeed using the Boot JDK and native toolchain that you expect?\nBy default, the JDK has a strict approach where warnings from the compiler is considered errors which fail the build. For very new or very old compiler versions, this can trigger new classes of warnings, which thus fails the build. Run configure with \u0026ndash;disable-warnings-as-errors to turn of this behavior. (The warnings will still show, but not make the build fail.)\n","date":"2022-12-01T00:00:00Z","image":"https://cyber-blog.github.io/p/macos-compile-jdk/cover_hu16901052935209888649.png","permalink":"https://cyber-blog.github.io/p/macos-compile-jdk/","title":"MacOS自行编译JDK"},{"content":"Java 与 C++之间有一堵由内存动态分配和垃圾收集技术所围成的高墙，墙外面的人 想进去，墙里面的人却想出来。\n对于从事 C、C++程序开发的开发人员来说，在内存管理领域，他们既是拥有最高 权力的“皇帝”，又是从事最基础工作的劳动人民——既拥有每一个对象的“所有权”，又 担负着每一个对象生命从开始到终结的维护责任。\n对于 Java 程序员来说，在虚拟机自动内存管理机制的帮助下，不再需要为每一个 new 操作去写配对的 delete/free 代码，不容易出现内存泄漏和内存溢出问题，看起来由 虚拟机管理内存一切都很美好。不过，也正是因为 Java 程序员把控制内存的权力交给了 Java 虚拟机，一旦出现内存泄漏和溢出方面的问题，如果不了解虚拟机是怎样使用内存 的，那排查错误、修正问题将会成为一项异常艰难的工作。\nJava内存区域及相关异常 Java 虚拟机在执行 Java 程序的过程中会把它所管理的内存划分为若干个不同的数据 区域。这些区域有各自的用途，以及创建和销毁的时间，有的区域随着虚拟机 进程的启 动而一直存在，有些区域则是依赖用户线程的启动和结束而建立和销毁。根据《Java虚拟机规范》的规定，Java 虚拟机所管理的内存将会包括以下几个运行时数据区域，如图\n程序计数器 程序计数器（Program Counter Register）是一块较小的内存空间，它可以看作是当 前线程所执行的字节码的行号指示器。在 Java 虚拟机的概念模型里 ，字节码解释器工 作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，它是程序控制流的指示器，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数 器来完成。\n由于 Java 虚拟机的多线程是通过线程轮流切换、分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理器（对于多核处理器来说是一个内核）都只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需 要有一个独立的程序计数器，各条线程之间计数器互不影响，独立存储，我们称这类内 存区域为“线程私有”的内存。\n如果线程正在执行的是一个 Java 方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行的是本地（Native）方法，这个计数器值则应为空（Undefined）。此内存区域是唯一一个在《Java虚拟机规范》中没有规定任何 OutOfMemoryError 情况的区域。\nJava虚拟机栈 与程序计数器一样，Java 虚拟机栈（Java Virtual Machine Stack）也是线程私有的， 它的生命周期与线程相同。虚拟机栈描述的是 Java 方法执行的线程内存模型：每个方法被执行的时候，Java 虚拟机都会同步创建一个栈帧（Stack Frame）用于存储局部变量表、操作数栈、动态连接、方法出口等信息。每一个方法被调用直至执行完毕的过程， 就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。也就是说， 栈帧随着方法调用而创建，随着方法结束而销毁。无论方法正常完成还是异常完成都算作方法结束。\n经常有人把 Java 内存区域笼统地划分为堆内存（Heap）和栈内存（Stack），虽然这种在 Java 语言里就显得有些粗糙了，实际的内存区域划分要比这更复杂。不过这种划分方式的流行也间接说明了程序员最关注的、与对象内存分配关系最密切的区域是“堆”和“栈”两块。其中，“堆”在稍后会讲述，而“栈”通常就是指这里讲的虚拟机栈，或者更多的情况下只是指虚拟机栈中局部变量表部分。\n局部变量表存放了编译期可知的各种 Java 虚拟机基本数据类型（boolean、byte、 char、short、int、 float、long、double）、对象引用（reference 类型，它并不等同于对象 本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或者其他与此对象相关的位置）和 returnAddress类型（指向了一条字节码指令的地址）。\n这些数据类型在局部变量表中的存储空间以局部变量槽（Slot）来表示，其中 64 位 长度的 long 和 double 类型的数据会占用两个变量槽，其余的数据类型只占用一个。局 部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在栈帧中分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表的变量槽数量。请注意，实际上虚拟机真正使用多大的内存空间 （譬如按照 1 个变量槽占用 32 个比特、64 个比特，或者更多）来实现一个变量槽，这是完全由具体的虚拟机实现自行决定，不同的虚拟机或不同虚拟机版本都有可能不一致（也可通过-Xss参数手动设置）。\n在《Java虚拟机规范》中，对这个内存区域规定了两类异常状况：如果线程请求的栈深度大于虚拟机所允许的深度，将抛出 StackOverflowError 异常；当线程无法申请到足够的栈空间会抛出OutOfMemoryError 异常。\n本地方法栈 本地方法栈（Native Method Stacks）与虚拟机栈所发挥的作用是非常相似的，其区别只是虚拟机栈为虚拟机执行 Java 方法（也就是字节码）服务，而本地方法栈则是为虚拟机使用到的本地（Native）方法服务。\n本地方法被执行的时候，在本地方法栈也会创建一个栈帧，用于存放该本地方法的局部变量表、操作数栈、动态链接、出口信息。\n与虚拟机栈一样，本地方法栈也会在栈深度溢出或者栈扩展失败时分别抛出 StackOverflowError 和 OutOfMemoryError 异常。\nJava 堆 对于 Java 应用程序来说，Java 堆（Java Heap）是虚拟机所管理的内存中最大的一块。Java 堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯 一目的就是存放对象实例，Java 世界里“几乎”所有的对象实例都在这里分配内存。在 《Java 虚拟机规范》中对 Java 堆的描述是：“所有的对象实例以及数组都应当在堆上分配”。\nJava 堆是垃圾收集器管理的内存区域，因此一些资料中它也被称作“GC 堆” （Garbage Collected Heap）。\n在 JDK 7 版本及 JDK 7 版本之前，堆内存被通常分为下面三部分\n新生代内存(Young Generation) 老生代(Old Generation) 永久代(Permanent Generation) 下图所示的 Eden 区、两个 Survivor 区 S0 和 S1 都属于新生代，中间一层属于老年代，最下面一层属于永久代。\nJDK 8 版本之后 PermGen(永久) 已被 Metaspace(元空间) 取代，元空间使用的是直接内存\n大部分情况，对象都会首先在 Eden 区域分配，在一次新生代垃圾回收后，如果对象还存活，则会进入 S0 或者 S1，并且对象的年龄还会加 1(Eden 区-\u0026gt;Survivor 区后对象的初始年龄变为 1)，当它的年龄增加到一定程度（默认为 15 岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。\n在十年之前（以 G1 收集器的出现为分界），作为业界绝对主 流的 HotSpot 虚拟机，它内部的垃圾收集器全部都基于“经典分代”来设计，需要新生代、老年代收集器搭配才能工作，在这种背景下，上述说法还算是不会产生太大歧义。 但是到了今天，垃圾收集器技术与十年前已不可同日而语，HotSpot 里面也出现了不采用分代设计的新垃圾收集器，再按照上面的提法就有很多需要商榷的地方了。\n堆这里最容易出现的就是 OutOfMemoryError 错误，并且出现这种错误之后的表现形式还会有几种，比如：\njava.lang.OutOfMemoryError: GC Overhead Limit Exceeded ： 当 JVM 花太多时间执行垃圾回收并且只能回收很少的堆空间时，就会发生此错误。 java.lang.OutOfMemoryError: Java heap space :假如在创建新的对象时, 堆内存中的空间不足以存放新创建的对象, 就会引发此错误。(和配置的最大堆内存有关，且受制于物理内存大小。最大堆内存可通过-Xmx参数配置，若没有特别配置，将会使用默认值，详见：Default Java 8 max heap sizeopen in new window) 方法区 方法区（Method Area）与 Java 堆一样，属于是 JVM 运行时数据区域的一块逻辑区域，是各个线程共享的内存区域。\n虽然《Java 虚拟机规范》中把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫作“非堆”（Non-Heap），目的是与 Java 堆区分开来。\n《Java 虚拟机规范》只是规定了有方法区这么个概念和它的作用，方法区到底要如何实现那就是虚拟机自己要考虑的事情了。也就是说，在不同的虚拟机实现上，方法区的实现是不同的。\n当虚拟机要使用一个类时，它需要读取并解析 Class 文件获取相关信息，再将信息存入到方法区。方法区会存储已被虚拟机加载的 类信息、字段信息、方法信息、常量、静态变量、即时编译器编译后的代码缓存等数据。\n说到方法区，不得不提一下“永久代”这个概念，尤其是在 JDK 8 以前，许多 Java 程序员都习惯在 HotSpot 虚拟机上开发、部署程序，很多人都更愿意把方法区称呼为“永久 代”（Permanent Generation），或将两者混为一谈。本质上这两者并不是等价的，因为仅 仅是当时的 HotSpot 虚拟机设计团队选择把收集器的分代设计扩展至方法区，或者说使 用永久代来实现方法区而已，这样使得 HotSpot 的垃圾收集器能够像管理 Java 堆一样管 理这部分内存，省去专门为方法区编写内存管理代码的工作。但是对于其他虚拟机实 现，譬如 BEA JRockit、IBM J9 等来说，是不存在永久代的概念的。原则上如何实现方 法区属于虚拟机实现细节，不受《Java 虚拟机规范》管束，并不要求统一。\n但现在回头来看，当年使用永久代来实现方法区的决定并不是一个好主意。\n这种设计导致了 Java 应 用更容易遇到内存溢出的问题（永久代有-XX：MaxPermSize 的上限，即使不设置也有 默认大小，而 J9 和 JRockit 只要没有触碰到进程可用内存的上限，例如 32 位系统中的 4GB 限制，就不会出问题）。 当 Oracle 收购 BEA 获得了 JRockit （JRockit 从来没有一个叫永久代的东西）的所有权后， 准备把 JRockit 中的优秀功能，譬如 Java Mission Control 管理工具，移植到 HotSpot 虚拟机时，但因为两者对方法区实现的差异导致两者的代码合并起来较为困难。 考虑到 HotSpot 未来的发 展，在 JDK 6 的时候 HotSpot 开发团队就有放弃永久代，逐步改为采用本地内存 （Native Memory）来实现方法区的计划了(JEP 122-Remove the Permanent Generation) ，到了 JDK 7 的 HotSpot，已经把原本放在 永久代的字符串常量池、静态变量等移出，而到了 JDK 8，终于完全废弃了永久代的概念，改用与 JRockit、J9 一样在本地内存中实现的元空间（Metaspace）来代替，把 JDK 7 中永久代还剩余的内容（主要是类型信息）全部移到元空间中。 造成以上问题的原因是：\n《Java 虚拟机规范》对方法区的约束是非常宽松的，除了和 Java 堆一样不需要连续 的内存和可以选择固定大小或者可扩展外，甚至还可以选择不实现垃圾收集。相对而言，垃圾收集行为在这个区域的确是比较少出现的，但并非数据进入了方法区就如永久代的名字一样“永久”存在了。这区域的内存回收目标主要是针对常量池的回收和对类型的卸载，一般来说这个区域的回收效果比较难令人满意，尤其是类型的卸载，条件相当苛刻，但是这部分区域的回收有时又确实是必要的。以前 Sun 公司的 Bug 列表中，曾出现过的若干个严重的 Bug 就是由于低版本的 HotSpot 虚拟机对此区域未完全回收而导致内存泄漏。\n方法区常用参数有哪些？\nJDK 1.8 之前永久代还没被彻底移除的时候通常通过下面这些参数来调节方法区大小。\n1 2 -XX:PermSize=N //方法区 (永久代) 初始大小 -XX:MaxPermSize=N //方法区 (永久代) 最大大小,超过这个值将会抛出 OutOfMemoryError 异常:java.lang.OutOfMemoryError: PermGen 相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入方法区后就“永久存在”了。\nJDK 1.8 的时候，方法区（HotSpot 的永久代）被彻底移除了，取而代之是元空间，元空间使用的是直接内存。下面是一些常用参数：\n1 2 -XX:MetaspaceSize=N //设置 Metaspace 的初始（和最小大小） -XX:MaxMetaspaceSize=N //设置 Metaspace 的最大大小 与永久代很大的不同就是，如果不指定大小的话，随着更多类的创建，虚拟机会耗尽所有可用的系统内存。\n根据《Java 虚拟机规范》的规定，如果方法区无法满足新的内存分配需求时，将抛出 OutOfMemoryError 异常。\n运行时常量池 运行时常量池（Runtime Constant Pool）是方法区的一部分。\nClass 文件中除了有类的版本、字段、方法、接口等描述信息外，还有用于存放编译期生成的各种字面量（Literal）和符号引用（Symbolic Reference）的常量池表(Constant Pool Table)，这部分内容将在类加载后存放到方法区的运行时常量池中。\n字面量是源代码中的固定值的表示法，即通过字面我们就能知道其值的含义。字面量包括整数、浮点数和字符串字面量，符号引用包括类符号引用、字段符号引用、方法符号引用和接口方法符号引用。\n既然运行时常量池是方法区的一部分，自然受到方法区内存的限制，当常量池无法再申请到内存时会抛出 OutOfMemoryError 异常。\n字符串常量池 字符串常量池 是 JVM 为了提升性能和减少内存消耗针对字符串（String 类）专门开辟的一块区域，主要目的是为了避免字符串的重复创建。\n1 2 3 4 5 6 // 在堆中创建字符串对象”ab“ // 将字符串对象”ab“的引用保存在字符串常量池中 String aa = \u0026#34;ab\u0026#34;; // 直接返回字符串常量池中字符串对象”ab“的引用 String bb = \u0026#34;ab\u0026#34;; System.out.println(aa==bb);// true HotSpot 虚拟机中字符串常量池的实现是 src/hotspot/share/classfile/stringTable.cpp ,StringTable 本质上就是一个HashSet\u0026lt;String\u0026gt; ,容量为 StringTableSize（可以通过 -XX:StringTableSize 参数来设置）。\nStringTable 中保存的是字符串对象的引用，字符串对象的引用指向堆中的字符串对象。\nJDK1.7 之前，字符串常量池存放在永久代。JDK1.7 字符串常量池和静态变量从永久代移动了 Java 堆中。\nJDK 1.7 为什么要将字符串常量池移动到堆中？\n主要是因为永久代（方法区实现）的 GC 回收效率太低，只有在整堆收集 (Full GC)的时候才会被执行 GC。Java 程序中通常会有大量的被创建的字符串等待回收，将字符串常量池放到堆中，能够更高效及时地回收字符串内存。\n相关问题：JVM 常量池中存储的是对象还是引用呢？ - RednaxelaFX - 知乎open in new window\n最后再来分享一段周志明老师在《深入理解 Java 虚拟机（第 3 版）》样例代码\u0026amp;勘误open in new window Github 仓库的 issue#112open in new window 中说过的话：\n运行时常量池、方法区、字符串常量池这些都是不随虚拟机实现而改变的逻辑概念，是公共且抽象的，Metaspace、Heap 是与具体某种虚拟机实现相关的物理概念，是私有且具体的。\n直接内存 直接内存（Direct Memory）并不是虚拟机运行时数据区的一部分，也不是《Java 虚 拟机规范》中定义的内存区域。但是这部分内存也被频繁地使用，而且也可能导致 OutOfMemoryError 异常出现，\nJDK1.4 中新加入的 NIO(New Input/Output) 类，引入了一种基于通道（Channel）与缓存区（Buffer）的 I/O 方式，它可以直接使用 Native 函数库直接分配堆外内存，然后通过一个存储在 Java 堆中的 DirectByteBuffer 对象作为这块内存的引用进行操作。这样就能在一些场景中显著提高性能，因为避免了在 Java 堆和 Native 堆之间来回复制数据。\n本机直接内存的分配不会受到 Java 堆的限制，但是，既然是内存就会受到本机总内存大小以及处理器寻址空间的限制。\nHotSpot 虚拟机对象探秘 通过上面的介绍我们大概知道了虚拟机的内存情况，下面我们来详细的了解一下 HotSpot 虚拟机在 Java 堆中对象分配、布局和访问的全过程。\n对象的创建 Java 对象的创建过程我建议最好是能默写出来，并且要掌握每一步在做什么。\nStep1:类加载检查 虚拟机遇到一条 new 指令时，首先将去检查这个指令的参数是否能在常量池中定位到这个类的符号引用，并且检查这个符号引用代表的类是否已被加载过、解析和初始化过。如果没有，那必须先执行相应的类加载过程。\nStep2:分配内存 在类加载检查通过后，接下来虚拟机将为新生对象分配内存。对象所需的内存大小在类加载完成后便可确定，为对象分配空间的任务等同于把一块确定大小的内存从 Java 堆中划分出来。分配方式有 “指针碰撞” 和 “空闲列表” 两种，选择哪种分配方式由 Java 堆是否规整决定，而 Java 堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定（压缩功能取决于收集算法是\u0026quot;标记-清除\u0026quot;，还是\u0026quot;标记-整理\u0026quot;，值得注意的是，复制算法内存也是规整的）。\n内存分配的两种方式 （补充内容，需要掌握）：\n指针碰撞 ： 适用场合 ：堆内存规整（即没有内存碎片）的情况下。 原理 ：用过的内存全部整合到一边，没有用过的内存放在另一边，中间有一个分界指针，只需要向着没用过的内存方向将该指针移动对象内存大小位置即可。 使用该分配方式的 GC 收集器：Serial, ParNew 空闲列表 ： 适用场合 ： 堆内存不规整的情况下。 原理 ：虚拟机会维护一个列表，该列表中会记录哪些内存块是可用的，在分配的时候，找一块儿足够大的内存块儿来划分给对象实例，最后更新列表记录。 使用该分配方式的 GC 收集器：CMS 内存分配并发问题\n在创建对象的时候有一个很重要的问题，就是线程安全，因为在实际开发过程中，创建对象是很频繁的事情，即使仅仅修改一个指针所指向的位置，在并发情况下也并不是线程安 全的，可能出现正在给对象 A 分配内存，指针还没来得及修改，对象 B 又同时使用了 原来的指针来分配内存的情况，作为虚拟机来说，必须要保证线程是安全的，通常来讲，虚拟机采用两种方式来保证线程安全：\nCAS+失败重试： CAS 是乐观锁的一种实现方式。所谓乐观锁就是，每次不加锁而是假设没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止。虚拟机采用 CAS 配上失败重试的方式保证更新操作的原子性。 TLAB： 为每一个线程预先在 Eden 区分配一块儿内存，JVM 在给线程中的对象分配内存时，首先在 TLAB 分配，当对象大于 TLAB 中的剩余内存或 TLAB 的内存已用尽时，再采用上述的 CAS 进行内存分配（虚拟机是否使用 TLAB，可以通过XX：+/-UseTLAB 参数来设定） Step3:初始化零值 内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头），这一步操作保证了对象的实例字段在 Java 代码中可以不赋初始值就直接使用，程序能访问到这些字段的数据类型所对应的零值。\nStep4:设置对象头 初始化零值完成之后，虚拟机要对对象进行必要的设置，例如这个对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的 GC 分代年龄等信息。 这些信息存放在对象头中。 另外，根据虚拟机当前运行状态的不同，如是否启用偏向锁等，对象头会有不同的设置方式。\nStep5:执行 init 方法 在上面工作都完成之后，从虚拟机的视角来看，一个新的对象已经产生了，但从 Java 程序的视角来看，对象创建才刚开始，\u0026lt;init\u0026gt; 方法还没有执行，所有的字段都还为零。所以一般来说，执行 new 指令之后会接着执行 \u0026lt;init\u0026gt; 方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来。\n对象的内存布局 在 HotSpot 虚拟机里，对象在堆内存中的存储布局可以划分为三个部分：对象头 （Header）、实例数据（Instance Data）和对齐填充（Padding）。\n对象头 （Header）\nHotSpot 虚拟机对象的对象头部分包括两类信息。\n用于存储对象自身的运 行时数据，如哈希码（HashCode）、GC 分代年龄、锁状态标志、线程持有的锁、偏向 线程 ID、偏向时间戳等，这部分数据的长度在 32 位和 64 位的虚拟机（未开启压缩指针）中分别为 32 个比特和 64 个比特，官方称它为“Mark Word”。在 32 位的 HotSpot 虚拟机中，如对象未 被同步锁锁定的状态下，Mark Word 的 32 个比特存储空间中的 25 个比特用于存储对象 哈希码，4 个比特用于存储对象分代年龄，2 个比特用于存储锁标志位，1 个比特固定为 0。 对象头的另外一部分是类型指针，即对象指向它的类型元数据的指针，Java 虚拟机通过这个指针来确定该对象是哪个类的实例。此外，如果对象是一个 Java 数组，那在对象头中还必须有一 块用于记录数组长度的数据，因为虚拟机可以通过普通 Java 对象的元数据信息确定 Java 对象的大小，但是如果数组的长度是不确定的，将无法通过元数据中的信息推断出数组的大小。 实例数据（Instance Data）\n实例数据部分是对象真正存储的有效信息，也是在程序中所定义的各种类型的字段内容。\nHotSpot 虚拟机默认的分配顺序为 longs/doubles、ints、shorts/chars、 bytes/booleans、oops（Ordinary Object Pointers，OOPs），从以上默认的分配策略中可以 看到，相同宽度的字段总是被分配到一起存放，在满足这个前提条件的情况下，在父类 中定义的变量会出现在子类之前。如果 HotSpot 虚拟机的+XX：CompactFields 参数值为 true（默认就为 true），那子类之中较窄的变量也允许插入父类变量的空隙之中，以节省出一点点空间。\n对齐填充（Padding）\n这部分不是必然存在的，也没有什么特别的含义，仅仅起占位作用。 因为 Hotspot 虚拟机的自动内存管理系统要求对象起始地址必须是 8 字节的整数倍，换句话说就是对象的大小必须是 8 字节的整数倍。而对象头部分正好是 8 字节的倍数（1 倍或 2 倍），因此，当对象实例数据部分没有对齐时，就需要通过对齐填充来补全。\n对象的访问定位 创建对象自然是为了后续使用该对象。由于 reference 类型在《Java 虚拟机规范》里面只规定了它是 一个指向对象的引用，并没有定义这个引用应该通过什么方式去定位、访问到堆中对象 的具体位置，所以对象访问方式也是由虚拟机实现而定的，主流的访问方式主要有两种：使用句柄、直接指针。\n句柄\n如果使用句柄的话，那么 Java 堆中将会划分出一块内存来作为句柄池，reference 中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与对象类型数据各自的具体地址信息\n直接指针\n如果使用直接指针访问，reference 中存储的直接就是对象的地址。\n这两种对象访问方式各有优势，使用句柄来访问的最大好处就是 reference 中存储的 是稳定句柄地址，在对象被移动（垃圾收集时移动对象是非常普遍的行为）时只会改变 句柄中的实例数据指针，而 reference 本身不需要被修改。\n使用直接指针来访问最大的好处就是速度更快，它节省了一次指针定位的时间开销，由于对象访问在 Java 中非常频繁，因此这类开销积少成多也是一项极为可观的执行成本，就 HotSpot 虚拟机而言，它主要使用第二种方式进行对象访问，但从整个软件开发的范围来看，在各种语言、框架中使用句柄来访问的情 况也十分常见。\n实战：OutOfMemoryError 异常 在《Java 虚拟机规范》的规定里，除了程序计数器外，虚拟机内存的其他几个运行 时区域都有发生 OutOfMemoryError（下文称 OOM）异常的可能，接下来将通过若干实例 来验证异常实际发生的代码场景，并且将初步介绍若干最基本的与自动内存管理子系统相关的 HotSpot 虚拟机参数。\n堆溢出 Java 堆用于储存对象实例，我们只要不断地创建对象，并且保证 GC Roots 到对象 之间有可达路径来避免垃圾回收机制清除这些对象，那么随着对象数量的增加，总容量 触及最大堆的容量限制后就会产生内存溢出异常。\n以下代码中限制 Java 堆的大小为 20MB，不可扩展（将堆的最小值-Xms 参数 与最大值-Xmx 参数设置为一样即可避免堆自动扩展），通过参数 -XX:+HeapDumpOnOutOf-MemoryError 可以让虚拟机在出现内存溢出异常的时候 Dump 出当前的内存堆转储快照以便进行事后分析 。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 /** * VM Args：-Xms20m -Xmx20m -XX:+HeapDumpOnOutOfMemoryError * * * @author zzm */ public class HeapOOM { static class OOMObject { } public static void main(String[] args) { List\u0026lt;OOMObject\u0026gt; list = new ArrayList\u0026lt;OOMObject\u0026gt;(); while (true) { list.add(new OOMObject()); } } } 运行结果\n1 2 3 java.lang.OutOfMemoryError: Java heap space Dumping heap to java_pid38436.hprof ... Heap dump file created [27710387 bytes in 0.093 secs] Java 堆内存的 OutOfMemoryError 异常是实际应用中最常见的内存溢出异常情况。 出现 Java 堆内存溢出时，异常堆栈信息“java.lang.OutOfMemoryError”会跟随进一步提示 “Java heap space”。\n要解决这个内存区域的异常，常规的处理方法是首先通过内存映像分析工具（如 Eclipse Memory Analyzer）对 Dump 出来的堆转储快照进行分析。第一步首先应确认内 存中导致 OOM 的对象是否是必要的，也就是要先分清楚到底是出现了内存泄漏 （Memory Leak）还是内存溢出（Memory Overflow）。\n如果是内存泄漏，可进一步通过工具查看泄漏对象到 GC Roots 的引用链，找到泄 漏对象是通过怎样的引用路径、与哪些 GC Roots 相关联，才导致垃圾收集器无法回收 它们，根据泄漏对象的类型信息以及它到 GC Roots 引用链的信息，一般可以比较准确 地定位到这些对象创建的位置，进而找出产生内存泄漏的代码的具体位置。\n如果不是内存泄漏，换句话说就是内存中的对象确实都是必须存活的，那就应当检 查 Java 虚拟机的堆参数（-Xmx 与-Xms）设置，与机器的内存对比，看看是否还有向上调整的空间。再从代码上检查是否存在某些对象生命周期过长、持有状态时间过长、存储结构设计不合理等情况，尽量减少程序运行期的内存消耗。\n虚拟机栈和本地方法栈溢出 《Java 虚拟机规范》明确允许 Java 虚拟机实现自行选择是否支持栈的动态扩展，而 HotSpot 虚拟机的选择是不支持扩展，因此对于 HotSpot 来说，-Xoss 参数（设置本地方法栈大小）虽然存在，但实际上是没有任何效果的，栈容量只能由 -Xss 参数来设定。\n关于虚拟机栈和本地方法栈。在《Java 虚拟机规范》中描述了两种异常\n如果线程请求的栈深度大于虚拟机所允许的最大深度，将抛出 StackOverflowError 异常。 如果虚拟机的栈内存允许动态扩展，当扩展栈容量无法申请到足够的内存时， 将抛出 OutOfMemoryError 异常。 HotSpot虚拟机是不支持扩展，所以除非在创建线程申请内存时就因无法获得足够内存而出现 OutOfMemoryError 异常，否则在线程运行时是不会因为扩展而导致内存溢出的，只会因为栈容量无法容纳新的栈帧而导致 StackOverflowError 异常。\n为了验证这点，我们可以做两个实验，先将实验范围限制在单线程中操作。\n尝试下面两种行为是否能让 HotSpot 虚拟机产生 OutOfMemoryError 异常：\n使用-Xss参数减少 栈内存容量\n结果：抛出 StackOverflowError 异常，异常出现时输出的堆栈深度相应缩小。\n首先，对第一种情况进行测试，具体如代码如下所示。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 /** * VM Args：-Xss128k * * @author zzm */ public class JavaVMStackSOF1 { private int stackLength = 1; public void stackLeak() { stackLength++; stackLeak(); } public static void main(String[] args) throws Throwable { JavaVMStackSOF oom = new JavaVMStackSOF(); try { oom.stackLeak(); } catch (Throwable e) { System.out.println(\u0026#34;stack length:\u0026#34; + oom.stackLength); throw e; } } } 运行结果\n1 2 3 4 5 stack length:10830 Exception in thread \u0026#34;main\u0026#34; java.lang.StackOverflowError at com.mj.jvmpractice.JavaVMStackSOF.stackLeak(JavaVMStackSOF.java:12) at com.mj.jvmpractice.JavaVMStackSOF.stackLeak(JavaVMStackSOF.java:13) at com.mj.jvmpractice.JavaVMStackSOF.stackLeak(JavaVMStackSOF.java:13) 对于不同版本的 Java 虚拟机和不同的操作系统，栈容量最小值会有所限制，这主要取决于操作系统内存分页大小。譬如上述方法中的参数-Xss128k 可以正常用于 32 位 Windows 系统下的 JDK 6，但是如果用于 64 位 Windows 系统下的 JDK 11，则会提示 栈容量最小不能低于 180K，而在 Linux 下这个值则可能是 228K，如果低于这个最小限 制，HotSpot 虚拟器启动时会给出如下提示：\n1 The stack size specified is too small, Specify at least 160k 定义了大量的本地变量，增大此方法帧中本地变量表的长度\n结果：抛出 StackOverflowError 异常，异常出现时输出的堆栈深度相应缩小。\n为了多占局部变量表空间，不得不定义一长串变量，具体如代码如下所示。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 /** * @author zzm */ public class JavaVMStackSOF2 { private static int stackLength = 0; public static void test() { long unused1, unused2, unused3, unused4, unused5, unused6, unused7, unused8, unused9, unused10, unused11, unused12, unused13, unused14, unused15, unused16, unused17, unused18, unused19, unused20, unused21, unused22, unused23, unused24, unused25, unused26, unused27, unused28, unused29, unused30, unused31, unused32, unused33, unused34, unused35, unused36, unused37, unused38, unused39, unused40, unused41, unused42, unused43, unused44, unused45, unused46, unused47, unused48, unused49, unused50, unused51, unused52, unused53, unused54, unused55, unused56, unused57, unused58, unused59, unused60, unused61, unused62, unused63, unused64, unused65, unused66, unused67, unused68, unused69, unused70, unused71, unused72, unused73, unused74, unused75, unused76, unused77, unused78, unused79, unused80, unused81, unused82, unused83, unused84, unused85, unused86, unused87, unused88, unused89, unused90, unused91, unused92, unused93, unused94, unused95, unused96, unused97, unused98, unused99, unused100; stackLength++; test(); unused1 = unused2 = unused3 = unused4 = unused5 = unused6 = unused7 = unused8 = unused9 = unused10 = unused11 = unused12 = unused13 = unused14 = unused15 = unused16 = unused17 = unused18 = unused19 = unused20 = unused21 = unused22 = unused23 = unused24 = unused25 = unused26 = unused27 = unused28 = unused29 = unused30 = unused31 = unused32 = unused33 = unused34 = unused35 = unused36 = unused37 = unused38 = unused39 = unused40 = unused41 = unused42 = unused43 = unused44 = unused45 = unused46 = unused47 = unused48 = unused49 = unused50 = unused51 = unused52 = unused53 = unused54 = unused55 = unused56 = unused57 = unused58 = unused59 = unused60 = unused61 = unused62 = unused63 = unused64 = unused65 = unused66 = unused67 = unused68 = unused69 = unused70 = unused71 = unused72 = unused73 = unused74 = unused75 = unused76 = unused77 = unused78 = unused79 = unused80 = unused81 = unused82 = unused83 = unused84 = unused85 = unused86 = unused87 = unused88 = unused89 = unused90 = unused91 = unused92 = unused93 = unused94 = unused95 = unused96 = unused97 = unused98 = unused99 = unused100 = 0; } public static void main(String[] args) { try { test(); } catch (Error e) { System.out.println(\u0026#34;stack length:\u0026#34; + stackLength); throw e; } } } 运行结果\n1 2 3 4 5 stack length:7334 Exception in thread \u0026#34;main\u0026#34; java.lang.StackOverflowError at com.mj.jvmpractice.JavaVMStackSOF2.test(JavaVMStackSOF2.java:14) at com.mj.jvmpractice.JavaVMStackSOF2.test(JavaVMStackSOF2.java:14) at com.mj.jvmpractice.JavaVMStackSOF2.test(JavaVMStackSOF2.java:14) 实验结果表明：\n无论是由于栈帧太大还是虚拟机栈容量太小，当新的栈帧内存无法分配的时候， HotSpot 虚拟机抛出的都是 StackOverflowError 异常。可是如果在允许动态扩展栈容量大小的虚拟机上，相同代码则会导致不一样的情况。譬如远古时代的 Classic 虚拟机，这款虚拟机可以支持动态扩展栈内存的容量，在 Windows 上的 JDK 1.0.2 运行代码 JavaVMStackSOF2 的话（如果这时候要调整栈容量就应该改用-oss 参数了），得到的结果是：\n1 2 3 4 5 stack length:3716 java.lang.OutOfMemoryError at org.fenixsoft.oom.JavaVMStackSOF.leak (JavaVMStackSOF.java:27) at org.fenixsoft.oom.JavaVMStackSOF.leak (JavaVMStackSOF.java:28) at org.fenixsoft.oom.JavaVMStackSOF.leak (JavaVMStackSOF.java:28) 额外情况：通过不断建立线程的方式，产生内存溢出\n结果：在栈空间不可扩展的 HotSpot 上产生内存溢出异常\n但是这样产生的内存溢出异常和栈空间是否足够并不存在任何直接的关系，主要取决于操作系统本身的内存使用状态。甚至可以说，在这种情况下，给每个线程的栈分配的内存越大，反而越 容易产生内存溢出异常。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 /** * VM Args：-Xss2M （这时候不妨设大些，请在 32 位系统下运行） * * @author zzm */ public class JavaVMStackOOM { private void dontStop() { while (true) { } } public void stackLeakByThread() { while (true) { Thread thread = new Thread(new Runnable() { @Override public void run() { dontStop(); } }); thread.start(); } } public static void main(String[] args) throws Throwable { JavaVMStackOOM oom = new JavaVMStackOOM(); oom.stackLeakByThread(); } } 运行结果\n1 Exception in thread \u0026#34;main\u0026#34; java.lang.OutOfMemoryError: unable to create native thread 原因其实不难理解：操作系统分配给每个进程的内存是有限制的，那虚拟机栈和本地方法栈可用内存为总内存即为减去最大堆容量，再减去最大方法区容量，减去直接内存和虚拟机进程本身耗费的内存，剩下的内存就由虚拟机栈和本地方法栈来分配了。因此为每个线程分配到的栈内存越大，可以建立的线程数量自然就越少，建立线程时就越容易把剩下的内存耗尽，以下代码演示了这种情况\n当出现 StackOverflowError 异常时，会有明确错误堆栈可供分析，相对而言比较容易 定位到问题所在。\n如果使用 HotSpot 虚拟机默认参数，栈深度在大多数情况下（因为每 个方法压入栈的帧大小并不是一样的，所以只能说大多数情况下）到达 1000~2000 的深度是完全没有问题，对于正常的方法调用（包括不能做尾递归优化的递归调用），这个深度应该完全够用了。\n但是，如果是建立过多线程导致的内存溢出，在不能减少线程数量或者更换 64 位虚拟机的情况下，就只能通过减少最大堆和减少栈容量来换取更多的线程。 这种通过“减少内存”的手段来解决内存溢出的方式，如果没有这方面处理经验，一般比较难以想到，这一点读者需要在开发 32 位系统的多线程应用时注意。也是由于这种问 题较为隐蔽，从 JDK 7 起，以上提示信息中“unable to create native thread”后面，虚拟机会特别注明原因可能是“possibly out of memory or process/resource limits reached” 。\n运行时常量池溢出 String::intern()是一个本地方法，它的作用是如果字符串常量池中已经包含一个等于 此 String 对象的字符串，则返回代表池中这个字符串的 String 对象的引用；否则，会将 此 String 对象包含的字符串添加到常量池中，并且返回此 String 对象的引用。\n以下代码在 JDK 6 或更早之前的 HotSpot 虚拟机中，常量池都是分配在永久代中，我们可以通过-XX： PermSize 和-XX：MaxPermSize 限制永久代的大小，即可间接限制其中常量池的容量。\n而使用 JDK 7 或更高版本的 JDK 来运行这段程序并不会得到相同的结果，因为自 JDK 7 起，原本存放在永久代的字符串常量池被移至 Java 堆之中，所以在 JDK 7 及以上版本，限制方法区的容量对该测试用例来说是毫无意义的。这时候使用 -Xmx 参数限制最大堆到 6MB 就会出现Exception in thread \u0026quot;main\u0026quot; java.lang.OutOfMemoryError: Java heap space异常\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 /** * VM Args：-XX:PermSize=6M -XX:MaxPermSize=6M * * @author zzm */ public class RuntimeConstantPoolOOM { public static void main(String[] args) { // 使用 Set 保持着常量池引用，避免 Full GC 回收常量池行为 Set\u0026lt;String\u0026gt; set = new HashSet\u0026lt;String\u0026gt;(); // 在 short 范围内足以让 6MB 的 PermSize 产生 OOM 了 short i = 0; while (true) { set.add(String.valueOf(i++).intern()); } } } 方法区溢出 方法区的主要职责是用于存放类型的相关信息，如类名、访问修饰符、常量池、字段描述、方法描述等。对于这部分区域的测试，基本的思路是运行时产生大量的类去填满方法区，直到溢出为止。虽然直接使用 Java SE API 也可以动态产生类（如反射时的 GeneratedConstructorAccessor 和动态代理等），但在本次实验中操作起来比较麻烦。在以下代码里借助了 CGLib 直接操 作字节码运行时生成了大量的动态类。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 /** * VM Args：-XX:PermSize=10M -XX:MaxPermSize=10M * * @author zzm */ public class JavaMethodAreaOOM { public static void main(String[] args) { while (true) { Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(OOMObject.class); enhancer.setUseCache(false); enhancer.setCallback(new MethodInterceptor() { public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable { return proxy.invokeSuper(obj, args); } }); enhancer.create(); } } static class OOMObject { } } 值得特别注意的是，当前的很多主流框架，如 Spring、Hibernate 对类进行增强时，都会使用到 CGLib 这类字节码技术，当增强的类越多，就需要越大的方法区以保证动态生成的新类型可以载入内存。另外，很多运行于 Java 虚拟机上的动态语言 （例如 Groovy 等）通常都会持续创建新类型来支撑语言的动态性，一个类如果要被垃圾收集器回收，要达成的条件是比较苛刻的。在经常运行时生成大量动态类的应用场景里，就应该特别关注这些类的回收状况。\nJDK7的运行结果:\n1 2 3 java.lang.OutOfMemoryError: PermGen space Dumping heap to java_pid46662.hprof ... Heap dump file created [5386258 bytes in 0.029 secs] 在 JDK 8 以后，永久代便完全退出了历史舞台，元空间作为其替代者登场。在 默认 设置下，前面列举的那些正常的动态创建新类型的测试用例已经很难再迫使虚拟机产生 方法区的溢出异常了。不过为了让使用者有预防实际应用里出现类似于上述代码那样的破坏性的操作，HotSpot 还是提供了一些参数作为元空间的防御措施，主要包括：\n-XX:MaxMetaspaceSize：设置元空间最大值，默认是-1，即不限制，或者说只受 限于本地内存大小。 -XX:MetaspaceSize：指定元空间的初始空间大小，以字节为单位，达到该值就会触发垃圾收集进行类型卸载，同时收集器会对该值进行调整：如果释放了大量的空间， 就适当降低该值；如果释放了很少的空间，那么在不超过-XX:MaxMetaspaceSize（如 果设置了的话）的情况下，适当提高该值。 -XX:MinMetaspaceFreeRatio：作用是在垃圾收集之后控制最小的元空间剩余容量 的百分比，可减少因为元空间不足导致的垃圾收集的频率。类似的还有-XX:MaxMetaspaceFreeRatio，用于控制最大的元空间剩余容量的百分比。 本机直接内存溢出 **直接内存（Direct Memory）**的容量大小可通过-XX:MaxDirectMemorySize 参数来 指定，如果不去指定，则默认与 Java 堆最大值（由-Xmx 指定）一致，以下代码越过了 DirectByteBuffer 类直接通过反射获取 Unsafe 实例进行内存分配（Unsafe 类的 getUnsafe()方法指定只有引导类加载器才会返回实例，体现了设计者希望只有虚拟机标准类库里面的类才能使用 Unsafe 的功能，在 JDK 10 时才将 Unsafe 的部分功能通过 VarHandle 开放给外部使用），因为虽然使用 DirectByteBuffer 分配内存也会抛出内存溢 出异常，但它抛出异常时并没有真正向操作系统申请分配内存，而是通过计算得知内存无法分配就会在代码里手动抛出溢出异常，真正申请分配内存的方法是 Unsafe::allocateMemory()。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 /** * VM Args： * -Xmx20M * -XX:MaxDirectMemorySize=10M * * @author zzm */ public class DirectMemoryOOM { private static final int _1MB = 1024 * 1024; public static void main(String[] args) throws Exception { Field unsafeField = Unsafe.class.getDeclaredFields()[0]; unsafeField.setAccessible(true); Unsafe unsafe = (Unsafe) unsafeField.get(null); while (true) { unsafe.allocateMemory(_1MB); } } } 运行结果：\n1 Exception in thread \u0026#34;main\u0026#34; java.lang.OutOfMemoryError at sun.misc.Unsafe.allocateMemory(Native Method) at org.fenixsoft.oom.DMOOM.main(DMOOM.java:20) 由直接内存导致的内存溢出，一个明显的特征是在 Heap Dump 文件中不会看见有 什么明显的异常情况，如果发现内存溢出之后产生的 Dump 文件很小，而程序中又直接或间接使用了 DirectMemory（典型的间接使用就是 NIO），那就可以考虑重点检查一下直接内存方面的原因了。\nJVM垃圾收集子系统详解 Java内存模型的各部分当中的程序计数器、虚拟机栈、本地方法栈 3 个区域和线程绑定，栈中的栈帧随着方法的进入和退出而有条不紊地执行着出栈和入栈操作。每一个栈帧中分配多少内存基本上是在类结构确定下来时就已知的，因此这几个区域的内存分配和回收都具备确定性，在这几个区域内就不需要过多考虑如何回收的问题，当方法结束或者线程结束时，内存自然就跟随着回收了。\n而 Java 堆和方法区这两个区域则有着很显著的不确定性，只有处于运行期间，我们才能知道程序究竟会创建哪些对象，创建多少个对象，这部分内存的分配和回收是动态的。垃圾收集器所关注的正是这部分内存该如何管理。\n死亡对象判断方法 堆中几乎放着所有的对象实例，对堆垃圾回收前的第一步就是要判断哪些对象已经死亡（即不能再被任何途径使用的对象）。\n引用计数法 给对象中添加一个引用计数器：\n每当有一个地方引用它，计数器就加 1； 当引用失效，计数器就减 1； 任何时候计数器为 0 的对象就是不可能再被使用的。 这个方法实现简单，效率高，也有很多使用这种方式进行内存管理的技术(如Python)、但是目前主流的虚拟机中并没有选择这个算法来管理内存，其最主要的原因是它很难解决对象之间相互循环引用的问题。\n所谓对象之间的相互引用问题，如下面代码所示：除了对象 objA 和 objB 相互引用着对方之外，这两个对象之间再无任何引用。但是他们因为互相引用对方，导致它们的引用计数器都不为 0，于是引用计数算法无法通知 GC 回收器回收他们。\n1 2 3 4 5 6 7 8 9 10 11 public class ReferenceCountingGc { Object instance = null; public static void main(String[] args) { ReferenceCountingGc objA = new ReferenceCountingGc(); ReferenceCountingGc objB = new ReferenceCountingGc(); objA.instance = objB; objB.instance = objA; objA = null; objB = null; } } 可达性分析算法 当前主流的商用程序语言（Java、C#，上溯至前面提到的古老的 Lisp）的内存管理子系统，都是通过可达性分析（Reachability Analysis）算法来判定对象是否存活的。这个算法的基本思想就是通过一系列的称为 “GC Roots” 的对象作为起点，从这些节点开始向下搜索，节点所走过的路径称为引用链，当一个对象到 GC Roots 没有任何引用链相连的话，则证明此对象是不可用的，需要被回收。\n下图中的 Object 6 ~ Object 10 之间虽有引用关系，但它们到 GC Roots 不可达，因此为需要被回收的对象。\n固定可作为 GC Roots 的对象包括以下几种：\n在虚拟机栈（栈帧中的本地变量表）中引用的对象，譬如各个线程被调用的方法堆栈中使用到的参数、局部变量、临时变量等。\n在方法区中类静态属性引用的对象，譬如 Java 类的引用类型静态变量。\n在方法区中常量引用的对象，譬如字符串常量池（String Table）里的引用。\n在本地方法栈中 JNI（即通常所说的 Native 方法）引用的对象。\nJava 虚拟机内部的引用，如基本数据类型对应的 Class 对象，一些常驻的异常对象 （比如 NullPointExcepiton、OutOfMemoryError）等，还有系统类加载器。\n所有被同步锁（synchronized 关键字）持有的对象。\n引用类型 无论是通过引用计数法判断对象引用数量，还是通过可达性分析法判断对象的引用链是否可达，判定对象的存活都与“引用”有关。\nJDK1.2 之前，Java 中引用的定义很传统：如果 reference 类型的数据存储的数值代表的是另一块内存的起始地址，就称这块内存代表一个引用。\nJDK1.2 以后，Java 对引用的概念进行了扩充，将引用分为强引用、软引用、弱引用、虚引用四种（引用强度逐渐减弱）\n强引用（StrongReference）\n以前我们使用的大部分引用实际上都是强引用，这是使用最普遍的引用。如果一个对象具有强引用，那就类似于必不可少的生活用品，垃圾回收器绝不会回收它。当内存空间不足，Java 虚拟机宁愿抛出 OutOfMemoryError 错误，使程序异常终止，也不会靠随意回收具有强引用的对象来解决内存不足问题。\n软引用（SoftReference）\n如果一个对象只具有软引用，那就类似于可有可无的生活用品。如果内存空间足够，垃圾回收器就不会回收它，如果内存空间不足了，就会回收这些对象的内存。只要垃圾回收器没有回收它，该对象就可以被程序使用。软引用可用来实现内存敏感的高速缓存。\n软引用可以和一个引用队列（ReferenceQueue）联合使用，如果软引用所引用的对象被垃圾回收，JAVA 虚拟机就会把这个软引用加入到与之关联的引用队列中。\n弱引用（WeakReference）\n弱引用也用来描述那些非必须得对象，但是它的强度比软引用更弱一些。被弱引用关联的对象只能生存到下一次垃圾收集发生为止。在进行垃圾回收是，不管当前内存空间足够与否，都会回收它的内存。在 JDK 1.2 版之后提供了 WeakReference 类来实现弱引用。\n虚引用（PhantomReference）\n\u0026ldquo;虚引用\u0026quot;顾名思义，就是形同虚设，与其他几种引用都不同，虚引用并不会决定对象的生命周期。如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收。\n为一个对象设置虚引用关联的唯一目的只是为了能在这个对象被收集器回收时 收到一个系统通知。\n在 JDK 1.2 版之后提供了 PhantomReference 类来实现虚引用。\n方法区回收 实际上《Java 虚拟机规范》中提到过可以不要求虚拟机在方法区中实现垃圾收集，事实上也确实有未实现或未能完整实现方法区类型卸载的收集器存在（如 JDK 11 时期的 ZGC 收集器就不支持类卸载）。这也侧面反应了方法区的回收相对于堆来说不频繁，并且回收条件苛刻。\n方法区的垃圾收集主要回收两部分内容：废弃的常量和不再使用的类型。\n如何判断一个常量是废弃常量？ 运行时常量池主要回收的是废弃的常量。那么，我们如何判断一个常量是废弃常量呢？\n假如在字符串常量池中存在字符串 \u0026ldquo;abc\u0026rdquo;，如果当前没有任何 String 对象引用该字符串常量的话，就说明常量 \u0026ldquo;abc\u0026rdquo; 就是废弃常量，如果这时发生内存回收的话而且有必要的话，\u0026ldquo;abc\u0026rdquo; 就会被系统清理出常量池了。\n如何判断一个类是无用的类 方法区主要回收的是无用的类，那么如何判断一个类是无用的类的呢？\n判定一个常量是否是“废弃常量”比较简单，而要判定一个类是否是“无用的类”的条件则相对苛刻许多。类需要同时满足下面 3 个条件才能算是 “无用的类” ：\n该类所有的实例都已经被回收，也就是 Java 堆中不存在该类的任何实例。 加载该类的 ClassLoader 已经被回收，这个条件除非是经过精心设计的可替换类加载器的场景，否则通常是很难达成的。 该类对应的 java.lang.Class 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 虚拟机可以对满足上述 3 个条件的无用类进行回收，这里说的仅仅是“可以”，而并不是和对象一样不使用了就会必然被回收。关于是否要对类型进行回收， HotSpot 虚拟机提供了-Xnoclassgc 参数进行控制\n垃圾收集算法 分代收集理论 当前商业虚拟机的垃圾收集器，大多数都遵循了“分代收集”（Generational Collection）的理论进行设计，分代收集名为理论，实质是一套符合大多数程序运行实 际情况的经验法则，它建立在两个分代假说之上：\n弱分代假说（Weak Generational Hypothesis）：绝大多数对象都是朝生夕灭的。\n强分代假说（Strong Generational Hypothesis）：熬过越多次垃圾收集过程的对象就越难以消亡。\n这也就延伸出了多款垃圾收集器的统一原则：收集器应该将 Java 堆划分出不同的区域，然后将回收对象依据其年龄（年龄即对象熬过垃圾收集过 程的次数）分配到不同的区域之中存储。\n显而易见，如果一个区域中大多数对象都是朝生夕灭，难以熬过垃圾收集过程的话，那么把它们集中放在一起，每次回收时只关注如 何保留少量存活而不是去标记那些大量将要被回收的对象，就能以较低代价回收到大量的空间；如果剩下的都是难以消亡的对象，那把它们集中放在一块，虚拟机便可以使用较低的频率来回收这个区域，这就同时兼顾了垃圾收集的时间开销和内存的空间有效利 用。\n所以 Java 堆被划分为了几个不同的区域，垃圾收集器才可以每次些部分的区域，因此才有了下面针对于各分代区域的相关术语。\n部分收集 (Partial GC)：\n新生代收集（Minor GC / Young GC）：只对新生代进行垃圾收集； 老年代收集（Major GC / Old GC）：只对老年代进行垃圾收集。需要注意的是 Major GC 在有的语境中也用于指代整堆收集； 混合收集（Mixed GC）：对整个新生代和部分老年代进行垃圾收集。 整堆收集 (Full GC)：收集整个 Java 堆和方法区。\n继续衍化发展出了“标记-复制算法”“标记-清除算法”“标记-整理算法”等针对性的垃圾收集算法。\n标记-清除算法 该算法分为“标记”和“清除”阶段：：首先标记出所有需要回收的对象，在标记完成后，统一回收掉所有被标记的对象，也可以反过来，标记存活的对象，统一回收所有未被标记的对象。它是最基础的收集算法，后续的算法都是对其不足进行改进得到。这种垃圾收集算法会带来两个明显的问题：\n对象分配时的效率问题 空间问题（标记清除后会产生大量不连续的碎片） 标记-复制算法 为了解决效率问题，“标记-复制”收集算法出现了。它可以将内存分为大小相同的两块，每次使用其中的一块。当这一块的内存使用完后，就将还存活的对象复制到另一块去，然后再把使用的空间一次清理掉。这样就使每次的内存回收都是对内存区间的一半进行回收。\n现在的商用 Java 虚拟机大多都优先采用了这种收集算法去回收新生代，新生代中的对象有 98%熬不过第一轮收集。因此并不需要按照 1∶1 的比例来划分新生代的内存空间。在此思想上提出了一种更优化的半区复制分代策略，现在称为“Appel 式回收”。\nAppel 式回收的具体做法是把 新生代分为一块较大的 Eden 空间和两块较小的 Survivor 空间，每次分配内存只使用 Eden 和其中一块 Survivor。发生垃圾搜集时，将 Eden 和 Survivor 中仍然存活的对象一 次性复制到另外一块 Survivor 空间上，然后直接清理掉 Eden 和已用过的那块 Survivor 空间。HotSpot 虚拟机默认 Eden 和 Survivor 的大小比例是 8∶1，也即每次新生代中可 用内存空间为整个新生代容量的 90%（Eden 的 80%加上一个 Survivor 的 10%），只有一 个 Survivor 空间，即 10%的新生代是会被“浪费”的。当然，任何人都没有办法百分百保证每次回收都只有不多于 10%的 对象存活，当 Survivor 空间不足以容纳一次 Minor GC 之后存活的对象时，这些对象将通过分配担保（Handle Promotion）机制直接进入老年代。\n标记-整理算法 根据老年代的特点提出的一种标记算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象回收，而是让所有存活的对象向一端移动，然后直接清理掉边界以外的内存。\n是否移动对象是一项优缺点并存的风险决策：\n如果移动存活对象，尤其是在老年代这种每次回收都有大量对象存活区域，移动存活对象并更新所有引用这些对象的地方将会是一种极为负重的操作，而且这种对象移动 操作必须全程暂停用户应用程序才能进行（“Stop The World”） 。 如果完全不移动和整理存活对象的话，弥散于堆中的存活对象导致的空间碎片化问题就只能依赖更为复杂的内存分配器和内存访问器来解决。 基于以上两点，是否移动对象都存在弊端，移动则内存回收时会更复杂，不移动则内存分配时会更复杂。从垃圾收集的停顿时间来看，不移动对象停顿时间会更短，甚至可以不需要停顿，但是从整个程序的运行效率来看，移动对象会更划算。即使不移动对象会使得收集器的效率提升一些，减少垃圾收集时的用户线程停顿时间，但因内存分配和访问相比垃圾收集频率要高得多，这部分的耗时增加，总吞吐量仍然是下降的。\n经典垃圾收集器 如果说收集算法是内存回收的方法论，那么垃圾收集器就是内存回收的具体实现。\n虽然我们对各个收集器进行比较，但并非要挑选出一个最好的收集器。因为直到现在为止还没有最好的垃圾收集器出现，更加没有万能的垃圾收集器，我们能做的就是根据具体应用场景选择适合自己的垃圾收集器。试想一下：如果有一种四海之内、任何场景下都适用的完美收集器存在，那么我们的 HotSpot 虚拟机就不会实现那么多不同的垃圾收集器了。\nSerial收集器 Serial（串行）收集器是最基本、历史最悠久的垃圾收集器了。大家看名字就知道这个收集器是一个单线程收集器了。它的 “单线程” 的意义不仅仅意味着它只会使用一条垃圾收集线程去完成垃圾收集工作，更重要的是它在进行垃圾收集工作的时候必须暂停其他所有的工作线程（ \u0026ldquo;Stop The World\u0026rdquo; ），直到它收集结束。\n新生代采用标记-复制算法，老年代采用标记-整理算法。\n虚拟机的设计者们当然知道 Stop The World 带来的不良用户体验，所以在后续的垃圾收集器设计中停顿时间在不断缩短（仍然还有停顿，寻找最优秀的垃圾收集器的过程仍然在继续）。\n但是 Serial 收集器有没有优于其他垃圾收集器的地方呢？当然有，它简单而高效（与其他收集器的单线程相比）。Serial 收集器由于没有线程交互的开销，自然可以获得很高的单线程收集效率（桌面端、部分微服务场景）。Serial 收集器对于运行在 Client 模式下的虚拟机来说是个不错的选择。\nSerial Old 收集器 Serial 收集器的老年代版本，它同样是一个单线程收集器。它主要有两大用途：一种用途是在 JDK1.5 以及以前的版本中与 Parallel Scavenge 收集器搭配使用，另一种用途是作为 CMS 收集器的后备方案。\nParNew 收集器 ParNew 收集器其实就是 Serial 收集器的多线程版本，除了使用多线程进行垃圾收集外，其余行为包括 Serial 收集器可用的所有控制参数（例如：-XX： SurvivorRatio、-XX： PretenureSizeThreshold、-XX：HandlePromotionFailure 等）、控制参数、收集算法、回收策略等等）和 Serial 收集器完全一样。\n它是许多运行在 Server 模式下的虚拟机的首要选择，除了 Serial 收集器外，只有它能与 CMS 收集器（真正意义上的并发收集器，后面会介绍到）配合工作。ParNew 收集器在单核心处理器的环境中绝对不会有比 Serial 收集器更好的效果，当然，随着可以被使用的处理器核心数量的增加，ParNew 对于垃圾收集时系统资源的高效利用还是很有好处的。\nParallel Scavenge 收集器 Parallel Scavenge 收集器也是一款新生代收集器，同样基于标记复制算法，\n其设计重点是吞吐量（高效率的利用 CPU）。\nCMS 等垃圾收集器的关注点更多的是用户线程的停顿时间（提高用户体验）。\n所谓吞吐量就是 CPU 中用于运行用户代码的时间与 CPU 总消耗时间的比值。\n如果虚拟机完成某个任务，用户代码加 上垃圾收集总共耗费了 100 分钟，其中垃圾 收集花掉 1 分钟，那吞吐量就是 99%。\nParallel Scavenge 收集器提供了很多参数供用户找到最合适的停顿时间或最大吞吐量，如果对于收集器运作不太了解，手工优化存在困难的时候，使用 Parallel Scavenge 收集器配合自适应调节策略，把内存管理优化交给虚拟机去完成也是一个不错的选择。\n这是 JDK1.8 默认收集，默认使用 Parallel Scavenge + Parallel Old\n使用 java -XX:+PrintCommandLineFlags -version 命令查看\n1 2 3 4 -XX:InitialHeapSize=262921408 -XX:MaxHeapSize=4206742528 -XX:+PrintCommandLineFlags -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseParallelGC java version \u0026#34;1.8.0_211\u0026#34; Java(TM) SE Runtime Environment (build 1.8.0_211-b12) Java HotSpot(TM) 64-Bit Server VM (build 25.211-b12, mixed mode) Parallel Old 收集器 Parallel Scavenge 收集器的老年代版本。使用多线程和“标记-整理”算法。在注重吞吐量以及 CPU 资源的场合，都可以优先考虑 Parallel Scavenge 收集器和 Parallel Old 收集器。\nCMS 收集器 CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。它非常符合在注重用户体验的应用上使用。\nCMS（Concurrent Mark Sweep）收集器是 HotSpot 虚拟机第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程（基本上）同时工作。\n从名字中的Mark Sweep这两个词可以看出，CMS 收集器是一种 “标记-清除”算法实现的，它的运作过程相比于前面几种垃圾收集器来说更加复杂一些。整个过程分为四个步骤：\n初始标记： 暂停所有的其他线程，并记录下直接与 GCRoots 相连的对象，速度很快 ； 并发标记： 同时开启 GC 和用户线程，用一个闭包结构去记录可达对象。但在这个阶段结束，这个闭包结构并不能保证包含当前所有的可达对象。因为用户线程可能会不断的更新引用域，所以 GC 线程无法保证可达性分析的实时性。所以这个算法里会跟踪记录这些发生引用更新的地方。 重新标记： 重新标记阶段就是为了修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段的时间稍长，远远比并发标记阶段时间短。 并发清除： 开启用户线程，同时 GC 线程开始对未标记的区域做清扫。 它是一款优秀的垃圾收集器，主要优点：并发收集、低停顿。但是它有下面三个明显的缺点：\n对 CPU 资源敏感； 无法处理浮动垃圾； 它使用的回收算法-“标记-清除”算法会导致收集结束时会有大量空间碎片产生。 Garbage First 收集器 G1 (Garbage-First) 是一款面向服务器的垃圾收集器,主要针对配备多颗处理器及大容量内存的机器. 以极高概率满足 GC 停顿时间要求的同时,还具备高吞吐量性能特征。\n被视为 JDK1.7 中 HotSpot 虚拟机的一个重要进化特征。它具备以下特点：\n并行与并发：G1 能充分利用 CPU、多核环境下的硬件优势，使用多个 CPU（CPU 或者 CPU 核心）来缩短 Stop-The-World 停顿时间。部分其他收集器原本需要停顿 Java 线程执行的 GC 动作，G1 收集器仍然可以通过并发的方式让 java 程序继续执行。 分代收集：虽然 G1 可以不需要其他收集器配合就能独立管理整个 GC 堆，但是还是保留了分代的概念。 空间整合：与 CMS 的“标记-清理”算法不同，G1 从整体来看是基于“标记-整理”算法实现的收集器；从局部上来看是基于“标记-复制”算法实现的。 可预测的停顿：这是 G1 相对于 CMS 的另一个大优势，降低停顿时间是 G1 和 CMS 共同的关注点，但 G1 除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为 M 毫秒的时间片段内。 G1 收集器在后台维护了一个优先列表，每次根据允许的收集时间，优先选择回收价值最大的 Region(这也就是它的名字 Garbage-First 的由来) 。这种使用 Region 划分内存空间以及有优先级的区域回收方式，保证了 G1 收集器在有限时间内可以尽可能高的收集效率（把内存化整为零）。\n新一代的垃圾收集器 ZGC 收集器 ZGC（The Z Garbage Collector）是JDK 11中推出的一款低延迟垃圾回收器。\n它的设计目标包括：\n停顿时间不超过10ms； 停顿时间不会随着堆的大小，或者活跃对象的大小而增加； 支持8MB~4TB级别的堆（未来支持16TB）。 在性能方面，尽管目前还处于实验状态，还没有完成所有特性，稳定性打磨和性能 调优也仍在进行，但即使是这种状态下的 ZGC，其性能表现已经相当亮眼，从官方给 出的测试结果来看，用“令人震惊的、革命性的 ZGC”来形容都不为过。\n特点：\n与 Shenandoah 和 G1 一样，ZGC 也采用基于 Region 的堆内存布局。 染色指针技术（Colored Pointer，其他 类似的技术中可能将它称为 Tag Pointer 或者 Version Pointer）。 ZGC 完全没有使用记忆集，没事分代。 如何选择垃圾收集器 一般来说，收集器的选择就从以上这几点出发来考虑。举个例子，假设某个直接面 向用户提供服务的 B/S 系统准备选择垃圾收集器，一般来说延迟时间 是这类应用的主要 关注点，那么：\n如果你有充足的预算但没有太多调优经验，那么一套带商业技术支持的专有硬件 或者软件解决方案是不错的选择，Azul 公司以前主推的 Vega 系统和现在主推的 Zing VM 是这方面的代表，这样你就可以使用传说中的 C4 收集器了。\n如果你虽然没有足够预算去使用商业解决方案，但能够掌控软硬件型号，使用较 新的版本，同时又特别注重延迟，那 ZGC 很值得尝试。\n如果你对还处于实验状态的收集器的稳定性有所顾虑，或者应用必须运行在 Windows 操作系统下，那 ZGC 就无缘了，试试 Shenandoah 吧。\n如果你接手的是遗留系统，软硬件基础设施和 JDK 版本都比较落后，那就根据内 存规模衡量一下，对于大概 4GB 到 6GB 以下的堆内存，CMS 一般能处理得比较好，而对于更大的堆内存，可重点考察一下 G1。\n参考的一手资料出处 本教程基本上照搬了周志明老师的深入理解Java虚拟机，相当于本书的读后感或笔记\n新一代垃圾回收器ZGC的探索与实践\nJavaGuide\n《Java虚拟机规范》\n要学习虚拟机，《Java虚拟机规范》无论如何都是必须读的。这本书的概念和细节描述与 Sun 的早期虚拟机（Sun Classic 虚拟机）高度吻合，随着技术的发展，高性能虚拟机真正的细节实现方式已经渐渐与虚拟机规范所描述的方 式差距越来越大，如果只能选择一本参考书来了解 Java 虚拟机，那必然是这本书。\n《Java 语言规范》\n虽然 Java 虚拟机并不是 Java 语言专有的，但是了解 Java 语言的各种细节规定对虚拟机的行为也是很有帮助的， 它与《Java虚拟机规范》一样都是 Oracle 官方直接出版的书籍，而且这本书还是由 Java 之父 James Gosling 亲自执笔撰写。\n2.网站资源\n高级语言虚拟机圈子：http://hllvm.group.iteye.com/。\n里面有一些关于虚拟机的讨论，并不只限于 Java 虚拟机，包括了所有针对高级语言虚拟机（High-Level Language Virtual Machine）的讨论，不过该网站针对 Java 虚拟机的讨论还是绝对的主流。圈主 RednaxelaFX（莫枢）的博客（http://rednaxelafx.iteye.com/）是另外一个非常有价值的虚拟机及编译原理等资料的分 享园地。\nHotSpot Internals：https://wiki.openjdk.java.net/display/HotSpot/Main。\n这是一个关于 OpenJDK 的 Wiki 网站，许多文章都由 JDK 的开发团队编写，更新很慢，但是有很大的参考价值。\nThe HotSpot Group：http://openjdk.java.net/groups/hotspot/。\nHotSpot 组群，里面有关于虚拟机开发、编译器、垃圾收集和运行时四个邮件组，包含了关于 HotSpot 虚拟机最新的讨论。\n","date":"2022-11-24T00:00:00Z","image":"https://cyber-blog.github.io/p/jvm-gc/cover_hu3214150652761430953.png","permalink":"https://cyber-blog.github.io/p/jvm-gc/","title":"JVM自动内存管理和垃圾回收"},{"content":"随着生产环境ubuntu系统的更新。FreeSWITC1.6版本的安装和部署所依赖的一些库已经不兼容或不存在，导致安装会出现一些问题。为解决这些问题FreeSWITC的版本也需要升级。并且使用Docker的方式部署和安装。\n构建镜像 clone freeswitch repository\n1 git clone https://github.com/signalwire/freeswitch.git 进入freeswitch官网申请账号并获取自己账号的TOKEN。参考官方文档\n修改Dockerfile。包括修改TOKEN和添加apt-get国内源等。参考如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 # vim:set ft=dockerfile: ARG DEBIAN_VERSION=buster FROM debian:${DEBIAN_VERSION} ARG TOKEN=YOU_TOKEN # Source Dockerfile: # https://github.com/docker-library/postgres/blob/master/9.4/Dockerfile # explicitly set user/group IDs RUN groupadd -r freeswitch --gid=999 \u0026amp;\u0026amp; useradd -r -g freeswitch --uid=999 freeswitch RUN apt-get update \u0026amp;\u0026amp; apt-get install -y ca-certificates \u0026amp;\u0026amp; sed -i \u0026#39;s#http://deb.debian.org#https://mirrors.163.com#g\u0026#39; /etc/apt/sources.list \u0026amp;\u0026amp; apt-get clean # grab gosu for easy step-down from root RUN apt-get update \u0026amp;\u0026amp; apt-get install -y --no-install-recommends dirmngr gnupg2 wget \\ \u0026amp;\u0026amp; gpg2 --keyserver hkp://keyserver.ubuntu.com --recv-keys B42F6819007F00F88E364FD4036A9C25BF357DD4 \\ \u0026amp;\u0026amp; gpg2 --keyserver hkp://keyserver.ubuntu.com --recv-keys 655DA1341B5207915210AFE936B4249FA7B0FB03 \\ \u0026amp;\u0026amp; gpg2 --output /usr/share/keyrings/signalwire-freeswitch-repo.gpg --export 655DA1341B5207915210AFE936B4249FA7B0FB03 \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* \\ \u0026amp;\u0026amp; wget -O /usr/local/bin/gosu \u0026#34;https://github.com/tianon/gosu/releases/download/1.2/gosu-$(dpkg --print-architecture)\u0026#34; \\ \u0026amp;\u0026amp; wget -O /usr/local/bin/gosu.asc \u0026#34;https://github.com/tianon/gosu/releases/download/1.2/gosu-$(dpkg --print-architecture).asc\u0026#34; \\ \u0026amp;\u0026amp; gpg --verify /usr/local/bin/gosu.asc \\ \u0026amp;\u0026amp; rm /usr/local/bin/gosu.asc \\ \u0026amp;\u0026amp; chmod +x /usr/local/bin/gosu \\ \u0026amp;\u0026amp; apt-get purge -y --auto-remove wget dirmngr gnupg2 # make the \u0026#34;en_US.UTF-8\u0026#34; locale so freeswitch will be utf-8 enabled by default RUN apt-get update \u0026amp;\u0026amp; apt-get install -y locales \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* \\ \u0026amp;\u0026amp; localedef -i en_US -c -f UTF-8 -A /usr/share/locale/locale.alias en_US.UTF-8 ENV LANG en_US.utf8 # https://freeswitch.org/confluence/display/FREESWITCH/Debian RUN apt-get update \u0026amp;\u0026amp; apt-get install lsb-release -y --no-install-recommends \\ \u0026amp;\u0026amp; echo \u0026#34;machine freeswitch.signalwire.com login signalwire password ${TOKEN}\u0026#34; \u0026gt; /etc/apt/auth.conf \\ \u0026amp;\u0026amp; echo \u0026#34;deb [signed-by=/usr/share/keyrings/signalwire-freeswitch-repo.gpg] https://freeswitch.signalwire.com/repo/deb/debian-release/ `lsb_release -sc` main\u0026#34; \u0026gt; /etc/apt/sources.list.d/freeswitch.list \\ \u0026amp;\u0026amp; apt-get update \u0026amp;\u0026amp; apt-get install -y freeswitch-all \\ \u0026amp;\u0026amp; apt-get purge -y --auto-remove ca-certificates lsb-release \\ \u0026amp;\u0026amp; apt-get clean \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* COPY docker-entrypoint.sh / # Add anything else here ## Ports # Open the container up to the world. ### 8021 fs_cli, 5060 5061 5080 5081 sip and sips, 64535-65535 rtp EXPOSE 8021/tcp EXPOSE 5060/tcp 5060/udp 5080/tcp 5080/udp EXPOSE 5061/tcp 5061/udp 5081/tcp 5081/udp EXPOSE 7443/tcp EXPOSE 5070/udp 5070/tcp EXPOSE 64535-65535/udp EXPOSE 16384-32768/udp # Volumes ## Freeswitch Configuration VOLUME [\u0026#34;/etc/freeswitch\u0026#34;] ## Tmp so we can get core dumps out VOLUME [\u0026#34;/tmp\u0026#34;] # Limits Configuration COPY build/freeswitch.limits.conf /etc/security/limits.d/ # Healthcheck to make sure the service is running SHELL [\u0026#34;/bin/bash\u0026#34;] HEALTHCHECK --interval=15s --timeout=5s \\ CMD fs_cli -x status | grep -q ^UP || exit 1 ENTRYPOINT [\u0026#34;/docker-entrypoint.sh\u0026#34;] CMD [\u0026#34;freeswitch\u0026#34;] 进入docker/master目录执行构建命令\n1 docker build -t freeswitch . 配置文件修改 修改启动映射的端口号\n修改日志输出位置\n修改ESL监听映射配置\n修改event_socket.conf\n1 2 3 4 5 6 7 8 9 10 11 12 \u0026lt;configuration name=\u0026#34;event_socket.conf\u0026#34; description=\u0026#34;Socket Client\u0026#34;\u0026gt; \u0026lt;settings\u0026gt; \u0026lt;param name=\u0026#34;nat-map\u0026#34; value=\u0026#34;false\u0026#34;/\u0026gt; \u0026lt;!-- 配置为0.0.0.0代表指定IPV4的所有IP地址链接 --\u0026gt; \u0026lt;param name=\u0026#34;listen-ip\u0026#34; value=\u0026#34;0.0.0.0\u0026#34;/\u0026gt; \u0026lt;param name=\u0026#34;listen-port\u0026#34; value=\u0026#34;8021\u0026#34;/\u0026gt; \u0026lt;param name=\u0026#34;password\u0026#34; value=\u0026#34;ClueCon\u0026#34;/\u0026gt; \u0026lt;!-- 表示使用acl名称为lan的集合进行IP校验 --\u0026gt; \u0026lt;param name=\u0026#34;apply-inbound-acl\u0026#34; value=\u0026#34;lan\u0026#34;/\u0026gt; \u0026lt;!--\u0026lt;param name=\u0026#34;stop-on-bind-error\u0026#34; value=\u0026#34;true\u0026#34;/\u0026gt;--\u0026gt; \u0026lt;/settings\u0026gt; \u0026lt;/configuration\u0026gt; 修改acl.conf.xml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 \u0026lt;configuration name=\u0026#34;acl.conf\u0026#34; description=\u0026#34;Network Lists\u0026#34;\u0026gt; \u0026lt;network-lists\u0026gt; \u0026lt;!-- These ACL\u0026#39;s are automatically created on startup. rfc1918.auto - RFC1918 Space nat.auto - RFC1918 Excluding your local lan. localnet.auto - ACL for your local lan. loopback.auto - ACL for your local lan. --\u0026gt; \u0026lt;list name=\u0026#34;lan\u0026#34; default=\u0026#34;allow\u0026#34;\u0026gt; \u0026lt;node type=\u0026#34;deny\u0026#34; cidr=\u0026#34;192.168.42.0/24\u0026#34;/\u0026gt; \u0026lt;node type=\u0026#34;allow\u0026#34; cidr=\u0026#34;192.168.42.42/32\u0026#34;/\u0026gt; \u0026lt;node type=\u0026#34;allow\u0026#34; cidr=\u0026#34;121.28.78.50/32\u0026#34;\u0026gt; \u0026lt;/list\u0026gt; \u0026lt;!-- This will traverse the directory adding all users with the cidr= tag to this ACL, when this ACL matches the users variables and params apply as if they digest authenticated. --\u0026gt; \u0026lt;list name=\u0026#34;domains\u0026#34; default=\u0026#34;deny\u0026#34;\u0026gt; \u0026lt;!-- domain= is special it scans the domain from the directory to build the ACL --\u0026gt; \u0026lt;node type=\u0026#34;allow\u0026#34; domain=\u0026#34;$${domain}\u0026#34;/\u0026gt; \u0026lt;!-- use cidr= if you wish to allow ip ranges to this domains acl. --\u0026gt; \u0026lt;!-- \u0026lt;node type=\u0026#34;allow\u0026#34; cidr=\u0026#34;192.168.0.0/24\u0026#34;/\u0026gt; --\u0026gt; \u0026lt;/list\u0026gt; \u0026lt;/network-lists\u0026gt; \u0026lt;/configuration\u0026gt; 启动容器 运行命令。注意端口和数据卷的映射\n1 2 3 4 5 6 7 # 一部分RTP端口的映射需要占用大量的启动时间。直接使用--net=host参数 docker run -it --name freeswitch \\ -v /data/freeswitch/configuration:/etc/freeswitch \\ -v /data/freeswitch/tmp:/tmp \\ -v /data/freeswitch/log:/log \\ --net host \\ majiang213/freeswitch ","date":"2022-10-09T00:00:00Z","image":"https://cyber-blog.github.io/p/fs-install4docker/cover_hu6307248181568134095.jpg","permalink":"https://cyber-blog.github.io/p/fs-install4docker/","title":"Docker 构建 FreeSWITCH 并部署"},{"content":" 新增节点如果有磁盘则需要挂载磁盘\n将k8s的master节点的公钥复制出来在新增的节点\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # 查看公钥 cat /root/.ssh/id_rsa.pub # 以下命令在新增节点执行 # 创建 rancher 用户 $ useradd rancher # 添加到 docker 组 $ usermod -aG docker rancher # 切换到前面创建的用户 $ su rancher # 进入自己的 home 目录 $ cd ~ # 创建 .ssh 目录 $ mkdir .ssh # 写入3个服务的公钥 $ echo \u0026#34;master节点公钥\u0026#34; \u0026gt;\u0026gt; .ssh/authorized_keys # 设置权限 $ chmod 700 .ssh $ chmod 644 .ssh/authorized_keys 执行完以上操作后可以用master节点连接进行测试\n安装集群相同版本的docker\n1 2 3 # 预先设置环境变量VERSION可以指定版本的前缀，如VERSION=18.06 将下载18.06.*的最新版本 VERSION=18.06 curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun 没有启动过 docker 的情况下，配置目录还不存在，先创建 mkdir -p /etc/docker ，然后在 /data 下面创建一个 docker 目录（命令：mkdir /data/docker），然后编辑配置文件 vi /etc/docker/daemon.json 添加如下配置：\n1 2 3 4 5 6 7 { \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;https://dockerhub.azk8s.cn\u0026#34;, \u0026#34;https://reg-mirror.qiniu.com\u0026#34; ], \u0026#34;graph\u0026#34;: \u0026#34;/data/docker\u0026#34; } 然后启动 docker\n1 2 3 4 # 开机启动 systemctl enable docker # 启动服务 systemctl start docker 在master节点当中导出rancher相关镜像\n1 2 3 4 5 # 导出镜像 docker save $(docker images | grep rancher | awk \u0026#39;{print $1\u0026#34;:\u0026#34;$2}\u0026#39;) \u0026gt; rancher20201204.tar # 导入镜像 docker load -i rancher20201204.tar 编辑rancher-cluster.yml文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 nodes: - address: 165.227.114.63 user: rancher role: [controlplane,worker,etcd] - address: 165.227.116.167 user: rancher role: [worker] - address: 165.227.127.226 user: rancher role: [worker] - address: 新节点ip user: rancher role: [worker] services: etcd: snapshot: true creation: 6h retention: 24h 在master节点运行\n1 rke up --update-only --config ./rancher-cluster.yml ","date":"2020-12-04T00:00:00Z","permalink":"https://cyber-blog.github.io/p/rancher-add-node/","title":"已有Rancher集群添加节点"},{"content":"GitLab CI/CD 是一个内置在GitLab中的工具，用于通过持续方法进行软件开发：\nContinuous Integration (CI) 持续集成 Continuous Delivery (CD) 持续交付 Continuous Deployment (CD) 持续部署 持续集成的工作原理是将小的代码块推送到Git仓库中托管的应用程序代码库中，并且每次推送时，都要运行一系列脚本来构建、测试和验证代码更改，然后再将其合并到主分支中。\n持续交付和部署相当于更进一步的CI，可以在每次推送到仓库默认分支的同时将应用程序部署到生产环境。\n这些方法使得可以在开发周期的早期发现bugs和errors，从而确保部署到生产环境的所有代码都符合为应用程序建立的代码标准。\nGitLab CI/CD 由一个名为 .gitlab-ci.yml 的文件进行配置，改文件位于仓库的根目录下。文件中指定的脚本由GitLab Runner执行。\n它涉及到在每次小的迭代中就不断地构建、测试和部署代码更改，从而减少了基于已经存在bug或失败的先前版本开发新代码的机会。\nContinuous Integration（持续集成） 假设一个应用程序，其代码存储在GitLab的Git仓库中。开发人员每天都要多次推送代码更改。对于每次向仓库的推送，你都可以创建一组脚本来自动构建和测试你的应用程序，从而减少了向应用程序引入错误的机会。这种做法称为持续集成，对于提交给应用程序（甚至是开发分支）的每项更改，它都会自动连续进行构建和测试，以确保所引入的更改通过你为应用程序建立的所有测试，准则和代码合规性标准。\nContinuous Delivery（持续交付） 持续交付是超越持续集成的更进一步的操作。应用程序不仅会在推送到代码库的每次代码更改时进行构建和测试，而且，尽管部署是手动触发的，但作为一个附加步骤，它也可以连续部署。此方法可确保自动检查代码，但需要人工干预才能从策略上手动触发以必输此次变更。\nContinuous Deployment（持续部署） 与持续交付类似，但不同之处在于，你无需将其手动部署，而是将其设置为自动部署。完全不需要人工干预即可部署你的应用程序。\n基本 CI/CD 工作流程 一旦你将提交推送到远程仓库的分支上，那么你为该项目设置的CI/CD管道将会被触发。GitLab CI/CD 的执行步骤如下图所示：\n通过GitLab UI所有的步骤都是可视化的。\n深入研究基本工作流程，则可以在DevOps生命周期的每个阶段看到GitLab中可用的功能，如下图所示：\nVerify 通过持续集成自动构建和测试你的应用程序 使用GitLab代码质量（GitLab Code Quality）分析你的源代码质量 通过浏览器性能测试（Browser Performance Testing）确定代码更改对性能的影响 执行一系列测试，比如Container Scanning , Dependency Scanning , JUnit tests 用Review Apps部署更改，以预览每个分支上的应用程序更改 Package 用Container Registry存储Docker镜像 用NPM Registry存储NPM包 用Maven Repository存储Maven artifacts 用Conan Repository存储Conan包 Release 持续部署，自动将你的应用程序部署到生产环境 持续交付，手动点击以将你的应用程序部署到生产环境 用GitLab Pages部署静态网站 仅将功能部署到一个Pod上，并让一定比例的用户群通过Canary Deployments访问临时部署的功能（PS：即灰度发布） 在Feature Flags之后部署功能 用GitLab Releases将发布说明添加到任意Git tag 使用Deploy Boards查看在Kubernetes上运行的每个CI环境的当前运行状况和状态 使用Auto Deploy将应用程序部署到Kubernetes集群中的生产环境 使用GitLab CI/CD，还可以：\n通过Auto DevOps轻松设置应用的整个生命周期 将应用程序部署到不同的环境 安装你自己的GitLab Runner Schedule pipelines 使用安全测试报告（Security Test reports）检查应用程序漏洞 GitLab Runner GitLab-CI GitLab-CI就是一套配合GitLab使用的持续集成系统（当然，还有其它的持续集成系统，同样可以配合GitLab使用，比如Jenkins）。而且GitLab8.0以后的版本是默认集成了GitLab-CI并且默认启用的。 GitLab-Runner GitLab-Runner是配合GitLab-CI进行使用的。GitLab里面的每一个工程都会定义一个属于这个工程的软件集成脚本，用来自动化地完成一些软件集成工作。当这个工程的仓库代码发生变动时，比如有人push了代码，GitLab就会将这个变动通知GitLab-CI。这时GitLab-CI会找出与这个工程相关联的Runner，并通知这些Runner把代码更新到本地并执行预定义好的执行脚本。 所以，GitLab-Runner就是一个用来执行软件集成脚本的东西。你可以想象一下：Runner就像一个个的工人，而GitLab-CI就是这些工人的一个管理中心，所有工人都要在GitLab-CI里面登记注册，并且表明自己是为哪个工程服务的。当相应的工程发生变化时，GitLab-CI就会通知相应的工人执行软件集成脚本。如下图所示： ​\tGitLab-Runner可以分类三种类型：Shared （共享）、Group（组私有）、Specific （库私有）。 Shared：这种Runner是所有工程都能够用的。只有系统管理员能够创建Shared类型的Runner。\n​\tGroup：指定组私有的runner，需要有组对应的权限。\nSpecific：这种Runner只能为指定的工程服务。拥有该工程访问权限的人都能够为该工程创建Shared类型的runner。\nGitLab Runner安装 一般来说，构建任务都会占用很多的系统资源 (如编译代码)，而 GitLab CI 又是 GitLab 的一部分，如果由 GitLab CI 来运行构建任务的话，在执行构建任务的时候，GitLab 的性能会大幅下降。\n所以GitLab Runner可以安装到不同的机器上，所以在构建任务运行期间并不会影响到 GitLab 的性能。\n关于GitLab Runner的安装基本上分为三种方式:\n直接安装在物理机当中 安装在Docker容器当中 通过Helm安装在Kubernetes集群当中 Helm安装GitLab Runner 添加 GitLab Helm 存储库:\n1 helm repo add gitlab https://charts.gitlab.io 如果使用 Helm 2，你必须初始化 Helm:\n1 helm init 配置GitLab Runner安装所需的配置文件values.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 ## Specify a imagePullPolicy ## imagePullPolicy: IfNotPresent ## The GitLab Server URL (with protocol) that want to register the runner against ## ref: https://docs.gitlab.com/runner/commands/README.html#gitlab-runner-register ## gitlabUrl: https://gitlab.example.com/ ## The registration token for adding new Runners to the GitLab server. This must ## be retrieved from your GitLab instance. ## ref: https://docs.gitlab.com/ee/ci/runners/ ## runnerRegistrationToken: \u0026#34;\u0026#34; ## Set the certsSecretName in order to pass custom certificates for GitLab Runner to use ## Provide resource name for a Kubernetes Secret Object in the same namespace, ## this is used to populate the /etc/gitlab-runner/certs directory ## ref: https://docs.gitlab.com/runner/configuration/tls-self-signed.html#supported-options-for-self-signed-certificates ## 如果GitLab使用了自签名的ssl证书则需要指定secret，详情见下一小节 #certsSecretName: ## Configure the maximum number of concurrent jobs ## ref: https://docs.gitlab.com/runner/configuration/advanced-configuration.html#the-global-section ## concurrent: 10 ## Defines in seconds how often to check GitLab for a new builds ## ref: https://docs.gitlab.com/runner/configuration/advanced-configuration.html#the-global-section ## checkInterval: 30 ## For RBAC support: ## create 为false则不默认创建账户，推荐自动创建 rbac: create: false serviceAccountName: your-service-account ## Run the gitlab-bastion container with the ability to deploy/manage containers of jobs ## cluster-wide or only within namespace clusterWideAccess: false ## If RBAC is disabled in this Helm chart, use the following Kubernetes Service Account name. ## # serviceAccountName: default ## Configuration for the Pods that the runner launches for each new job ## runners: ## Default container image to use for builds when none is specified ## image: ubuntu:20.04 ## Configuration for the Pods that that the runner launches for each new job config: | [[runners]] pre_clone_script = \u0026#34;echo \u0026#39;10.10.10.224 git.netstar.com\u0026#39; \u0026gt;\u0026gt; /etc/hosts\u0026#34; [runners.kubernetes] helper_image = \u0026#34;harbor.wangxingcloud.com/system/gitlab-runner-helper:x86_64-e95f89a0\u0026#34; image = \u0026#34;ubuntu:20.04\u0026#34; privileged = true [[runners.kubernetes.volumes.empty_dir]] name = \u0026#34;docker-certs\u0026#34; mount_path = \u0026#34;/certs/client\u0026#34; medium = \u0026#34;Memory\u0026#34; [[runners.kubernetes.volumes.host_path]] name = \u0026#34;docker\u0026#34; mount_path = \u0026#34;/var/run/docker.sock\u0026#34; read_only = true host_path = \u0026#34;/var/run/docker.sock\u0026#34; ## Run all containers with the privileged flag enabled ## This will allow the docker:stable-dind image to run if you need to run Docker ## commands. Please read the docs before turning this on: ## ref: https://docs.gitlab.com/runner/executors/kubernetes.html#using-docker-dind ## privileged: true ## Namespace to run Kubernetes jobs in (defaults to \u0026#39;default\u0026#39;) ## # namespace: ## Build Container specific configuration ## builds: # cpuLimit: 200m # memoryLimit: 256Mi cpuRequests: 100m memoryRequests: 128Mi ## Service Container specific configuration ## services: # cpuLimit: 200m # memoryLimit: 256Mi cpuRequests: 100m memoryRequests: 128Mi ## Helper Container specific configuration ## helpers: # cpuLimit: 200m # memoryLimit: 256Mi cpuRequests: 100m memoryRequests: 128Mi ## Affinity for pod assignment ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity ## 主机调度-亲和性 affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/os operator: In values: - linux values.yaml全部配置参数详情查看:\nhttps://gitlab.com/gitlab-org/charts/gitlab-runner/blob/master/values.yaml\n配置文件保存后输入如下命令\n1 2 3 4 5 # For Helm 2 helm install --namespace \u0026lt;NAMESPACE\u0026gt; --name gitlab-runner -f \u0026lt;CONFIG_VALUES_FILE\u0026gt; gitlab/gitlab-runner # For Helm 3 helm install --namespace \u0026lt;NAMESPACE\u0026gt; gitlab-runner -f \u0026lt;CONFIG_VALUES_FILE\u0026gt; gitlab/gitlab-runner \u0026lt;NAMESPACE\u0026gt; 是你想要安装 GitLab Runner 的 Kubernetes 名称空间\n运行kubectl create namespace \u0026lt;NAMESPACE\u0026gt; 创建新的命名空间 \u0026lt;CONFIG_VALUES_FILE\u0026gt; 是包含自定义配置的值文件的路径。参见 使用 Helm Chart 配置 GitLab Runner 来创建它\n值得注意的是配置文件的第三项，如果GitLab使用的是自生成的证书则需要配置certsSecretName的值，配置值为同一命名空间的Kubernetes secret的值\n运行runner的时候报错ERROR: Job failed (system failure): secrets is forbidden: User \u0026quot;system:serviceaccount:gitlab:default\u0026quot; cannot create resource \u0026quot;secrets\u0026quot; in API group \u0026quot;\u0026quot; in the namespace \u0026quot;gitlab\u0026quot;原因是没有特别指定serviceaccount 那么将使用默认账户 system:serviceaccount::default 最终的原因就是没有创建 对应 namespaces 的 集群角色绑定clusterrolebinding 执行命令，创建clusterrolebinding解决\n1 kubectl create clusterrolebinding gitlab-cluster-admin --clusterrole=cluster-admin --group=system:serviceaccounts --namespace=\u0026lt;NAMESPACE\u0026gt; 向gitLab Runner提供访问gitLab的自生成证书 需要向 GitLab Runner Helm Chart 配置文件提供一个 Kubernetes Secret，它将会挂载在容器的 /etc/GitLab-Runner/certs目录。\n运行如下命令创建Kubernetes secret\n1 2 3 4 kubectl --namespace \u0026lt;NAMESPACE\u0026gt; create secret generic \u0026lt;SECRET_NAME\u0026gt; --from-file=\u0026lt;CERTIFICATE_FILENAME\u0026gt; 使用.crt结尾的文件必须为gitlab.your-domain.com.crt的固定格式 SECRET_NAME Kubernetes 的资源名称(例如:gitlab-domain-cert) \u0026lt;CERTIFICATE_FILENAME\u0026gt;.crt结尾的证书文件 如果源文件 \u0026lt;CERTIFICATE_FILENAME\u0026gt; 不遵循 \u0026lt;gitlab-hostname.crt\u0026gt; 格式，那么有必要指定在目标文件中使用的文件名: 最后在values.yaml配置文件中配置secret的名称\n1 2 3 4 5 6 ## Set the certsSecretName in order to pass custom certificates for GitLab Runner to use ## Provide resource name for a Kubernetes Secret Object in the same namespace, ## this is used to populate the /etc/gitlab-runner/certs directory ## ref: https://docs.gitlab.com/runner/configuration/tls-self-signed.html#supported-options-for-self-signed-certificates ## certsSecretName: \u0026lt;SECRET NAME\u0026gt; 参考资料\nhttps://docs.gitlab.com/runner/install/kubernetes.html#providing-a-custom-certificate-for-accessing-gitlab\n使用Helm Chart更新GitLab Runner 1 helm upgrade --namespace \u0026lt;NAMESPACE\u0026gt; -f \u0026lt;CONFIG_VALUES_FILE\u0026gt; \u0026lt;RELEASE-NAME\u0026gt; gitlab/gitlab-runner \u0026lt;RELEASE-NAME\u0026gt;是你在安装GitLab Runner时给出的名字，参考 使用Helm安装 GitLab Runner 时使用的是gitlab-runner。\n如果你想更新到 GitLab Runner Helm Chart 的特定版本而不是最新的版本，在你的 Helm upgrade 命令中添加参数\n1 -version \u0026lt; Runner Helm Chart version \u0026gt; 使用Helm Chart删除GitLab Runner 要卸载 GitLab Runner Chart，请运行以下命令:\n1 helm delete --namespace \u0026lt;NAMESPACE\u0026gt; \u0026lt;RELEASE-NAME\u0026gt; 参考资料\nhttps://docs.gitlab.com/runner/install/kubernetes.html#required-configuration\n离线安装Gitlab-runner 离线环境使用helm安装gitlab-runner。需要先将所需的docker镜像导入到离线环境的harbor中。\n在有网络的环境，运行下面命令生成k8s资源的yml文件\n1 helm template --namespace \u0026lt;NAMESPACE\u0026gt; gitlab-runner -f \u0026lt;CONFIG_VALUES_FILE\u0026gt; gitlab/gitlab-runner \u0026gt; values.yaml 讲生成的yml文件复制到要离线安装的k8s环境后，运行kubectl apply -f values.yaml命令\n目前生成的template.yaml文件中的所有资源没有命名空间的相关配置，需要手动添加\n离线方式运行gitlab-runner需要重新指定执行器的gitlab-runner-helper镜像的位置 image: gitlab/gitlab-runner:alpine-v13.4.1\nserviceAccountName: your-service-account 需要修改\n1 2 3 4 5 6 7 bash [[runners]] (...) executor = \u0026#34;docker\u0026#34; [runners.kubernetes] (...) helper_image = \u0026#34;my.registry.local/gitlab/gitlab-runner-helper:tag\u0026#34; 参考https://www.cnblogs.com/wu-wu/p/13269950.html\ngitlab-runner安装当中需要注意的问题 本章节记录介绍安装当中的坑以及解决办法\nkubernetes集群有污点导致执行器无法启动的问题及解决办法 因为本次使用的kubernetes的机器中有污点taint并且无法删除。在无法删除的情况下，需要给执行器启动的pos添加默认配置的污点容忍。这个问题只能通过修改gitlab-runner的config.toml文件来解决,helm安装gitlab-runner的配置文件values.yaml无法进行相关配置，并且因为使用helm安装的gitlab-runner的配置文件config.toml是通过values.yaml自动生成的，导致无法挂载数据卷进行配置。\n解决方案 gitlab-runner 容器启动时会运行脚本/bin/bash /scripts/entrypoint。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 #!/bin/bash set -e mkdir -p /home/gitlab-runner/.gitlab-runner/ cp /scripts/config.toml /home/gitlab-runner/.gitlab-runner/ # Register the runner # 中间部分省略...... # Run pre-entrypoint-script if ! bash /scripts/pre-entrypoint-script; then exit 1 fi # 增加hosts域名解析 sed -i \u0026#39;9r /scripts/hosts\u0026#39; /home/gitlab-runner/.gitlab-runner/config.toml # 固定主机调度 sed -i \u0026#39;38r /scripts/affinity.txt\u0026#39; /home/gitlab-runner/.gitlab-runner/config.toml cat \u0026gt;\u0026gt;/home/gitlab-runner/.gitlab-runner/config.toml \u0026lt;\u0026lt;EOF [[runners.kubernetes.volumes.host_path]] name = \u0026#34;gitlab-runner-cache\u0026#34; mount_path = \u0026#34;/opt/cache\u0026#34; read_only = false host_path = \u0026#34;/data/gitlab-runner/cache\u0026#34; [[runners.kubernetes.volumes.host_path]] name = \u0026#34;docker\u0026#34; mount_path = \u0026#34;/var/run/docker.sock\u0026#34; read_only = true host_path = \u0026#34;/var/run/docker.sock\u0026#34; [runners.kubernetes.node_tolerations] \u0026#34;cattle.io/os=linux\u0026#34; = \u0026#34;NoSchedule\u0026#34; EOF # Start the runner exec /entrypoint run --user=gitlab-runner \\ --working-directory=/home/gitlab-runner 该脚本在gitlab-runner绑定的配置映射中进行配置。在该脚本的末尾追加以下内容\n1 2 [runners.kubernetes.node_tolerations] \u0026#34;cattle.io/os=linux\u0026#34; = \u0026#34;NoSchedule\u0026#34; 参考资料\nhttps://docs.gitlab.com/runner/executors/kubernetes.html#define-keywords-in-the-configuration-toml\nhttps://gitlab.com/gitlab-org/gitlab-runner/-/issues/2578\n关于Helm安装的gitlab-runner所启动的执行器经常因为Could not resolve host:git.domain.com报错的问题 helm安装的gitlab-runner所启动的执行器为k8s中的pod。会有pod容器内无法解析gitlab所dns的域名的问题，并且是间歇性的，80%左右是会解析不到的。并且这些配置需要修改config.toml。helm安装的gitlab-runner目前无法修改配置文件。\n目前解决办法同上，在配置映射的entrypoint脚本的末尾添加命令sed -i '9r /scripts/hosts' /home/gitlab-runner/.gitlab-runner/config.toml\nhosts的内容为\n1 pre_clone_script = \u0026#34;echo \u0026#39;10.10.0.18 git.wx.com\u0026#39; \u0026gt;\u0026gt; /etc/hosts\u0026#34;\t参考资料\nhttps://gitlab.com/gitlab-org/gitlab-runner/-/issues/4129\nhttps://gitlab.com/gitlab-org/charts/gitlab-runner/-/merge_requests/265\n在容器中构建时使用docker 在容器中进行构建时如果需要用到docker时，目前采用的是操纵宿主机docker的方式，需要给runner启动的执行器挂载数据卷，config.toml配置文件，在配置映射的entrypoint脚本的末尾追加\n1 2 3 4 5 [[runners.kubernetes.volumes.host_path]] name = \u0026#34;docker\u0026#34; mount_path = \u0026#34;/var/run/docker.sock\u0026#34; read_only = true host_path = \u0026#34;/var/run/docker.sock\u0026#34; 参考资料\nhttps://docs.gitlab.com/runner/executors/kubernetes.html#using-docker-in-your-builds\n缓存相关配置 GitLab Runner对缓存方案的支持有限，目前采用挂载Volume的方式做缓存。安装GitLab Runner时默认使用/opt/cache目录作为缓存空间。挂载数据卷并且，配置主机调度至固定节点（以上操作需要修改配置映射中的entrypoint文件，entrypoint文件详情见上面）\n配置固定主机节点\n1 2 3 4 5 6 7 8 [runners.kubernetes.affinity.node_affinity] [[runners.kubernetes.affinity.node_affinity.preferred_during_scheduling_ignored_during_execution]] weight = 100 [runners.kubernetes.affinity.node_affinity.preferred_during_scheduling_ignored_during_execution.preference] [[runners.kubernetes.affinity.node_affinity.preferred_during_scheduling_ignored_during_execution.preference.match_expressions]] key = \u0026#34;kubernetes.io/hostname\u0026#34; operator = \u0026#34;In\u0026#34; values = [\u0026#34;rancher-node2\u0026#34;] 挂载数据卷\n1 2 3 4 5 [[runners.kubernetes.volumes.host_path]] name = \u0026#34;gitlab-runner-cache\u0026#34; mount_path = \u0026#34;/opt/cache\u0026#34; read_only = false host_path = \u0026#34;/data/gitlab-runner/cache\u0026#34; 参考资料\nhttps://help.aliyun.com/document_detail/106968.html?aly_as=b8lTvr8V\nhttps://blog.csdn.net/xichenguan/article/details/101439395\nhttps://www.cnblogs.com/5bug/p/12733755.html\ngitlab-runner 部署到k8s时报错:deployment.extensions demo is forbidden: user \u0026ldquo;system:serviceaccount:gitlab:default\u0026rdquo; cannot get resource deployments in api group extensions in the namespace default 原因为：k8s使用的RBAC权限访问控制，当前对namespace default 没有操作权限。默认对每一个命名空间有一个默认的serviceaccount ：default 如果在CI里面没有特别指定serviceaccount 那么将使用默认账户 system:serviceaccount:gitlab:default 最终的原因就是没有创建 对应 namespaces 的 集群角色绑定clusterrolebinding 解决办法： 执行一下命令，创建clusterrolebinding即可\n1 kubectl create clusterrolebinding gitlab-cluster-admin --clusterrole=cluster-admin --group=system:serviceaccounts --namespace=default GitLab CI/CD 的使用文档 为了使用GitLab CI/CD，你需要一个托管在GitLab上的应用程序代码库，并且在根目录中的.gitlab-ci.yml文件中指定构建、测试和部署的脚本。\n在这个文件中，你可以定义要运行的脚本，定义包含的依赖项，选择要按顺序运行的命令和要并行运行的命令，定义要在何处部署应用程序，以及指定是否 要自动运行脚本或手动触发脚本。\n为了可视化处理过程，假设添加到配置文件中的所有脚本与在计算机的终端上运行的命令相同。\n一旦你已经添加了.gitlab-ci.yml到仓库中，GitLab将检测到该文件，并使用名为GitLab Runner的工具运行你的脚本。该工具的操作与终端类似。\n这些脚本被分组到作业（jobs），它们共同组成一个流水线（pipeline）。流水线就是一次构建的完整的执行流程。\n.gitlab-ci.yml 该文件规定了一个流水线(pipeline)，流水线中可以有多个作业（job），每个作业必须具有唯一的名称（不能使用关键字），每个作业时独立执行的。每个作业至少包含一个script。\njob before_script 定义shell脚本，该脚本在每个作业之前运行。该关键字也可以定义在文件的开头，作为全局默认执行。该关键字定义的脚本和script定义的脚本在一个shell当中串联执行，如果失败导致整个作业的失败，其他作业将不再执行。但是作业失败不会影响after_script运行。\nscript 指定运行的shell脚本，每个作业至少包含一个script。可以是一条命令也可以是多条命令\n1 2 3 4 job: script: - unname -a - bundle exec rspec script命令将需要用单引号和双引号引起来。例如，包含冒号命令:需要加引号，以便被YAML解析器当作字符串来解析。而不是一个key: value。需要注意的特殊字符有::,{ },[ ],,,\u0026amp;,*,#,?,|,-,\u0026lt; \u0026gt;,=,!,%,@\nafter_script 定义作业之后运行的命令（包括失败的作业），指定脚本在新的shell中执行，与before_script和script分开。\ntags 用于指定作业所运行的runner。（结合runner配置的标签来使用）\nallow_failure 允许作业失败，默认值为false。\nwhen 控制作业运行。\non_success 前面阶段中的所有作业都成功时才执行，默认值。 on_failure前面的阶段出现失败时运行。 always总是执行。 manual手动执行。 delayed延迟执行作业。start_in指定延迟时间（秒）。 timeout 1 2 3 4 5 6 build: script: build.sh timeout: 3 hours 30 minutes test: script: rspec timeout: 3h 30m 作业级别的超时可以覆盖项目级别的超时，但不能超过runner特定的超时。\nonly\u0026amp;except 该关键字逐渐被rules替代\nonly定义构建哪些分支或标签的时会执行的作业。 except定义构建哪些分支或标签的时不会执行的作业。 rules 构建规则，rules不能和only\u0026amp;except组合使用\n可用规则\nif\n1 2 3 4 5 6 7 8 9 10 11 12 13 variables: domain: example.com condescan: stage: codescan tags: - build script: - echo \u0026#34;codescan\u0026#34; - sleep 5; rules: - if: \u0026#34;$domain == \u0026#39;example.com\u0026#39;\u0026#34; when: manual - when: on_success ## 默认执行 如果domain的值匹配，则需要手动运行。\n不匹配on_sucess。\n条件判断从上到下，匹配成功则不会判断接下来的if。\n多条件匹配可以使用 \u0026amp;\u0026amp; ||。\nchanges （指定文件发生变化）\n接收一个文件路径的数组（指定代码库根路径的相对路径）\n1 2 3 rules: - changes: - Jenkinsfile exists (指定文件存在)\nartifacts 编译时生成的文件统称为gitlab的制品，作业完成后制品将被发送到gitlab，可在gitlab ui中下载。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 release: script: - mvn package -U artifacts: # 用于再和并请求 UI中公开作业制品，每个合并请求最多公开10个作业制品 expose_as: \u0026#39;artifact 1\u0026#39; # 通过name指定所创建的制品的名称。默认名称为artifacts # name: \u0026#34;$CI_JOB_NAME\u0026#34; # 作业名称 name: \u0026#34;$CI_COMMIT_REF_NAME\u0026#34; # 分支名称 # 保存时间，默认30天 expire_in: \u0026#39;10\u0026#39; # 10秒 # expire_in: \u0026#39;3 week\u0026#39; paths: - target/*.jar ## 可以是文件也可以是目录。也可以是通配符 dependencies 获取制品，定义要获取制品的列表，只能从当前阶段之前的阶段获取。\n1 2 3 unittest dependencies: - build # 定义作业名称 extends 继承引入配置文件的作业\n1 2 3 4 5 6 7 include: - project: \u0026#39;cidevops/cddevops-gitlabci-service\u0026#39; ref: master file: \u0026#39;job/build.yml\u0026#39; build: stage: build extends: .build .build 的内容\n1 2 3 4 5 6 7 build: stage: build tags: - build script: - $BUILD_SHELL - ls stages 用于定义作业可以使用的阶段，并且是全局定义。同一阶段的作业并行运行，不同阶段的作业顺序执行。\n.pre\u0026amp;.post .pre和.post无需定义默认存在的阶段，.pre始终是整个流水线的第一个运行阶段。.post始终是最后一个运行阶段。用户定义的阶段都在两者之间运行。\nstage 指定job所运行的阶段。依赖于全局定义的stages。并且同一个stage的作业可以并行执行。（同时运行需要注意runner配置的最大同时运行数）\nvariables 定义变量，可以在全局定义。也可以在作业中定义。\n.gitlab-ci.yml 预设变量参考\nhttps://blog.csdn.net/github_35631540/article/details/107864258\n关于变量方面的坑和问题 https://gitlab.com/gitlab-org/gitlab/-/issues/233289\nworkflow 顶级关键字用于是否创建整个流水线\n1 2 3 4 5 6 7 variables: domain: example.com workflow: rules: - if: \u0026#34;$domain == \u0026#39;example.com\u0026#39;\u0026#34; when: always ## 可以设置为always或never，默认值为always。 - when: never cache 存储编译项目所需的运行时依赖项，指定项目工作空间中需要在作业之间缓存的文或目录。\n全局cache定义在作业之外，针对所有作业生效，作业中定义的cache优先级更高。\n1 2 3 4 5 6 7 8 9 10 11 12 13 cache: paths: - my/files build: script: echo \u0026#34;hello\u0026#34; cache: # 可以配置特定分支的缓存，为每个不同的作业配置cache:key时，会为每个作业分配独立的cache。key可以使用任何预定义的变量，默认为default。 key: build paths: - target/ # 缓存策略，默认在执行作业开始时下载文件，并在结束时重新上传文件。 policy: pull # 跳过下载步骤 # policy: push # 跳过上传步骤 include local\n引入外部yaml或yml文件。引入同一代码库的文件，使用相对于根目录的路径进行引入，需要与配置文件在同一分支上。 （如果引入的配置文件和原文件有重复的部分则重复的部分不会生效）\n1 2 include: local: \u0026#39;ci/localci.yml\u0026#39; file\n引入其他项目某一分支的文件\n1 2 3 4 include: - project: demo/demo-java-service ref: master # 分支 file: \u0026#34;.gitlab-ci.yml\u0026#34; template\n引入官方提供的模板\n1 2 include: - template:Auto-DevOps.gitlab-ci.yml remote\n引入远程配置，通过http/https引入。引入的文件必须可以通过get请求公开访问，不支持身份验证\n1 2 include： - remote: \u0026#34;https://gitlab.com/awesome-project/raw/master.gitlab-ci-template.yml\u0026#34; 自定义CI配置文件路径\n默认情况下会在项目的根目录查找.gitlab-ci.yml文件，可以指定目录，也包括项目外的位置\n.gitlab-ci.yml #default .my/path/.gitlab-ci.yml http://example.com/generate/ci/config.yml .gitlab-ci.yml@mygroup/another-project # 文件名@项目名称 image 执行器为docker或k8s时，默认在注册runner的时候需要填写一个基础镜像。如果指定了全局image则所有作业使用此image创建容器并在其中运行。如果全局未指定，则会查看作业中是否有指定，如果有则此job按照指定镜像创建容器并运行，没有则使用默认镜像。\nservices 工作期间启动另一个docker容器，并link到image定义的docker容器。这样两个镜像会互通，服务镜像可以运行任何应用程序，但是最常见的是运行数据库，如mysql。\n如果在运行单元测试时需要数据库则可以指定。\ntrigger 触发器。定义当前job执行完成后触发其他项目的流水线\n注意，触发器不能使用变量\nenvironment 如果在作业中定义环境，则gitlab可以去追踪。可以在gitlab ui的运维-》环境中去查看，如果environment指定且不存在该名称下的环境，则将自动创建一个新环境。\n1 2 3 4 5 6 7 deploy： stage： deploy script: - echo \u0026#34;deploy\u0026#34; environment: name: production # 环境名称 url: http://example.com 环境地址 实战:GitLab CI/CD 自动化部署推送到Harbor 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 stages: - package - docker_build # maven打包 package: image: maven:3.6.3-openjdk-11 stage: package script: - ${PACKAGE} - cp target/demo-0.0.1-SNAPSHOT.jar /opt/cache/ only: - master # 将jar包build为新的镜像并更新到k8s集群 docker_build: image: 10.10.10.233/system/docker-kubectl:latest stage: docker_build script: - FULL_IMAGE_NAME=10.10.10.233/test/demo:0.0.1-${CI_PIPELINE_ID} - mkdir -p ${BUILD_DIR} - echo ${FULL_IMAGE_NAME} - cp /opt/cache/demo-0.0.1-SNAPSHOT.jar ${BUILD_DIR}/app.jar - cp Dockerfile ${BUILD_DIR}/Dockerfile - echo build $FULL_IMAGE_NAME - docker build -t $FULL_IMAGE_NAME ./${BUILD_DIR} - docker login 10.10.10.233 -u${HARBOR_USERNAME} -p${HARBOR_PASSWORD} - docker push $FULL_IMAGE_NAME - sh ./apply_deployment.sh ${FULL_IMAGE_NAME} only: - master # 定义变量 variables: PACKAGE: \u0026#39;mvn clean package -Dmaven.repo.local=/opt/cache/.m2/repository\u0026#39; # 构建命令 BUILD_DIR: \u0026#39;docker_build\u0026#39; Dockerfile\n1 2 3 4 5 FROM 10.10.10.233/system/openjdk:11.0.5-jdk-stretch-tools-arthas COPY app.jar /opt/dubbo-app/app.jar WORKDIR /opt/dubbo-app EXPOSE 20880 CMD java -server ${JAVA_DEBUG_OPTS} ${JAVA_OPTS} ${JMX_OPTS} ${JAVA_AGENT_OPTS} -XX:+DisableExplicitGC -verbose:gc --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.math=ALL-UNNAMED --add-opens=java.base/java.math=ALL-UNNAMED -Xlog:gc*:gc.log::filecount=10,filesize=1048576 -jar app.jar 部署到k8s脚本\n1 2 3 4 5 6 7 8 9 10 11 12 #!/bin/bash mkdir -p /etc/deploy echo \u0026#39;YXBpVmV....\u0026#39; |base64 -d \u0026gt; /etc/deploy/config # 字符串内容为kubeconfig文件的base64编码 kubectl -n default get Deployment demo if [[ $? -eq 0 ]]; then echo \u0026#34;工作负载存在，更新负载镜像\u0026#34; kubectl set image --record=true Deployment/demo demo=${1} else echo \u0026#34;工作负载不存在，创建新工作负载\u0026#34; kubectl apply -f Service.yml kubectl apply -f Deployment.yml fi 在gitlab-runner的k8s执行器容器内如果要访问k8s集群时，如果直接挂载数据卷的方式指定使用之前安装k8s时生成的config配置文件会存在间歇性访问不到k8s集群的情况，原因未知。\n以上访问方式是将config文件通过base64编码后再解码后访问的方式。\n输入以下命令讲config文件编码echo $(cat ~/.kube/config | base64) | tr -d \u0026quot; \u0026quot;\n参考https://help.aliyun.com/document_detail/106968.html?aly_as=b8lTvr8V\ngitlab CI/CD集成 SonarQube质量平台 SonarQube（曾用名Sonar（声纳）[1]）是一个开源的代码质量管理系统。\n支持超过25种编程语言[2]：Java、C/C++、C#、PHP、Flex、Groovy、JavaScript、Python、PL/SQL、COBOL等。（不过有些是商业软件插件） 可以在Android开发中使用 提供重复代码、编码标准、单元测试、代码覆盖率、代码复杂度、潜在Bug、注释和软件设计报告[3][4] 提供了指标历史记录、计划图（“时间机器”）和微分查看 提供了完全自动化的分析：与Maven、Ant、Gradle和持续集成工具（Atlassian Bamboo、Jenkins、Hudson等）[5][6][7] 与Eclipse开发环境集成 与JIRA、Mantis、LDAP、Fortify等外部工具集 支持扩展插件[8][9] 利用SQALE计算技术债务[10] 支持Tomcat。不过计划从SonarQube 4.1起终止对Tomcat的支持[11]。 SonarQube的安装 SonarQube需要依赖Postgresql数据库，都使用docker进行安装\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # 创建数据卷相关的目录 mkdir -p /data/postgresql/data mkdir -p /var/lib/postgresql ln -s /data/postgresql/data/ /var/lib/postgresql/data # 运行镜像 docker run --name postgresql -p 5432:5432 -e POSTGRES_USER=sonar -e POSTGRES_PASSWORD=sonar -e POSTGRE_DB=sonar -v /data/postgresql/data:/var/lib/postgresql/data -d postgres # 创建数据卷相关的目录 mkdir -p /data/sonarqube/data mkdir -p /data/sonarqube/extensions mkdir -p /data/sonarqube/logs mkdir -p /opt/sonarqube/extensions/plugins chmod -R 777 /data/sonarqube/ chmod -R 777 /opt/sonarqube/ ln -s /data/sonarqube /opt/sonarqube # 运行镜像 docker run --name sonarqube \\ --link postgresql \\ -e SONARQUBE_JDBC_URL=jdbc:postgresql://postgresql:5432/sonar \\ -e SONAR_JDBC_USERNAME=sonar \\ -e SONAR_JDBC_PASSWORD=sonar \\ -p 9000:9000 \\ -d \\ -v /data/sonarqube/data:/opt/sonarqube/data \\ -v /data/sonarqube/extensions:/opt/sonarqube/extensions \\ -v /data/sonarqube/logs:/opt/sonarqube/logs \\ sonarqube docker镜像启动报错：Elasticsearch: Max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]\n在镜像启动的物理机运行如下命令\n1 sysctl -w vm.max_map_count=262144 参考自StackOverflow\nSonarQube整合gitlab CI/CD 分析Java代码 该分析基于sonar-scanner的maven插件。\n全局配置\n首先修改maven的全局配置文件settings.xml，添加下图示例内容\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 \u0026lt;settings\u0026gt; \u0026lt;pluginGroups\u0026gt; \u0026lt;pluginGroup\u0026gt;org.sonarsource.scanner.maven\u0026lt;/pluginGroup\u0026gt; \u0026lt;/pluginGroups\u0026gt; \u0026lt;profiles\u0026gt; \u0026lt;profile\u0026gt; \u0026lt;id\u0026gt;sonar\u0026lt;/id\u0026gt; \u0026lt;activation\u0026gt; \u0026lt;activeByDefault\u0026gt;true\u0026lt;/activeByDefault\u0026gt; \u0026lt;/activation\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;!-- Optional URL to server. Default value is http://localhost:9000 --\u0026gt; \u0026lt;sonar.host.url\u0026gt; http://myserver:9000 \u0026lt;/sonar.host.url\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;/profile\u0026gt; \u0026lt;/profiles\u0026gt; \u0026lt;/settings\u0026gt; 分析\n需要在.gitlab-ci.yml添加如下stage\n1 2 3 4 5 6 7 8 9 10 11 code_analysis: image: maven:3.6.3-openjdk-11 stage: code_analysis script: | mvn clean verify sonar:sonar \\ -Dsonar.projectKey=${CI_PROJECT_NAME} \\ -Dsonar.host.url=http://10.10.10.236:31548 \\ -Dsonar.login=username \\ -Dsonar.password=password \\ -Dmaven.repo.local=/opt/cache/.m2/repository \\ --settings /opt/cache/settings.xml 参考自官方文档\n在gitlab CI/CD使用当中遇到的问题 trigger触发器不能使用变量去定义 1 2 3 4 5 6 7 8 9 10 11 12 # 使用变量去填充是不行的，只能直接写死 server-deploy: stage: server-deploy trigger: ${TEST}/${TEST}-server only: master # 直接写死才能正确触发 server-deploy: stage: server-deploy trigger: ndp/foo-server only: master 在同一个流水线当中的job之间变量无法传递 解决方法参考如下\nhttps://gitlab.com/gitlab-org/gitlab/-/issues/233289\n","date":"2020-11-24T00:00:00Z","image":"https://cyber-blog.github.io/p/gitlab-cicd/cover_hu13978031391275047888.png","permalink":"https://cyber-blog.github.io/p/gitlab-cicd/","title":"GitLab CI/CD 简介文档"},{"content":"​\tJenkins具有丰富的插件生态，足以满足我们日常工作的需求，但如果我们想通过具体的Jenkins任务直接对外提供服务，而不想将内部的具体实现对外暴露(否则，需添加对应的用户权限，通过页面执行job)；可以对外直接提供接口，第三方直接调用接口（比如提供给开发，提测前回归冒烟用例集），执行相应的Jenkins任务并获取任务结果。\n​\tJenkins API没有统一的入口，而是采用“…/api/” 的REST API样式，其中”…” 表示Jenkins资源的URL。\n​\t常见的Jenkins资源包括：站点（实例）、Job和Build。\n​\tJenkins的Remote API以REST-like的形式进行提供，通过对特定的API执行POST请求即可。\n​\t可以提前提供job的config的ftl模板，然后使用freemarker自动生成对应的job模板。后台调用jenkins RESTApi接口动态创建不同的JOB。\n官方文档地址\nhttps://wiki.jenkins.io/display/JENKINS/Remote+access+API\nREST API 站点所有支持的API都可以通过地址http://jenkins.dev.com/api 获取大致描述\nJOB API 获取JOB的相关信息 ​\t我们通过url直接获取job的相关信息，生成xml格式或者JSON，如获取job的信息，要获取json的地址\n​\thttp://jenkins.dev.com/job/service-sample/api/json\n​\t可以获取name，build等的所有信息。将url中的json改为xml则可以获取xml格式的相关信息\n获取build的信息 ​\t刚才我们获取的是job所有的相关信息，可能我们还需要获取某一次build的信息\n则可以通过地址`http://jenkins.dev.com/job/service-sample/{build_number}/api/json`获取 执行build ​\t不带参数执行，POST请求\n​\thttp://jenkins.dev.com/job/service-sample/build\n​\t带参数执行，POST请求\n​\thttp://jenkins.dev.com/job/service-sample/buildWithParameters\n获取config.xml ​\tJOB的配置信息，通过地址http://jenkins.dev.com/job/service-sample/config.xml获取到的是一个xml的格式，这个里面包括了详细的配置信息。\n信息过滤 ​\tJenkins API将Jenkins资源模型抽象为树形结构，可以通过tree来指定返回Jenkins资源的层次。之前获取的所有相关信息，可以通过添加参数tree=XXX[]，来过滤出你所感兴趣的节点，类似于正则表达式 详细的可以参考，http://jenkins.dev.com/api的\u0026quot;Controlling the amount of data you fetch\u0026quot;这一节\n假设通过http://jenkins.dev.com/job/service-sample/api/json 会获取所有的信息 ，那么添加 tree=builds[*] 可以获取所有builds下的节点\n直接http get请求 http://192.168.6.224:8080/job/pythontest/api/json?tree=builds[*]\n站点 API 站点信息查询 http://jenkins.dev.com/api/json可以查询到站点中所有的job信息\n查询同样支持，通过tree进行过滤，通过请求http://jenkins.dev.com/api/json?\u0026amp;tree=jobs[name[*]] 可以过滤出所有jobs的name\n创建JOB ​\t要创建一个Job，首先要明白每个Job的配置信息都保存在config.xml中。假设我们要新创建一个job,叫 testjob 那么第一步，要在Jenkins的安装目录下，找到子目录\\jobs，创建一个名为testjob的目录。如果不创建这个目录，直接调用API，会失败。\n​\t然后准备config.xml文件，作为job的配置信息，可以从刚才的service-sample配置直接拷贝得到，然后简单修改下name。举例通过Postman进行，访问的API地址，请求方法为POST，需要身份验证。\nhttp://192.168.6.224:8080/createItem?name=testjob Content-Type值为application/xml 请求的Boyd中类型选择为binary。选择config.xml文件。\n安全 在调用Jenkins API 时需要以HTTP Basic Auth验证方式提供用户名和密码。\n另外，在Jenkins 2之后默认开启CSRF protection （跨域访问伪造保护），对有些Jenkins API的调用还需要提供Jenkins-Crumb；否则会出现”403 No valid crumb was included in the request“ 的错误。\n获取Jenkins生成的crumb值:\n通过浏览器直接打开：http://jenkins.dev.com/crumbIssuer/api/xml\n返回结果类似：\n1 2 3 4 \u0026lt;defaultCrumbIssuer _class=\u0026#34;hudson.security.csrf.DefaultCrumbIssuer\u0026#34;\u0026gt; \u0026lt;crumb\u0026gt;068ab0b4e0622b374d8822b22cee8b18\u0026lt;/crumb\u0026gt; \u0026lt;crumbRequestField\u0026gt;Jenkins-Crumb\u0026lt;/crumbRequestField\u0026gt; \u0026lt;/defaultCrumbIssuer\u0026gt;1234 以通过Postman”执行一次新的build“为例:\n选择HTTP POST方法 输入URL: http://jenkins.dev.com/job/service-sample/build 选择Authorization Type为Basic Auth，并输入Username和Password （Jenkins用户名和密码） 在Headers中填入一个新的header: Key为Jenkins-Crumb Value为上面一步获取到的Jenkins生成的crumb值 JAVA API 1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.offbytwo.jenkins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jenkins-client\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.3.8\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; ​\tJAVA API使用Jenkins Client\n1 JenkinsServer jenkins = new JenkinsServer(new URI(\u0026#34;http://jenkins.dev.com/\u0026#34;), \u0026#34;admin\u0026#34;, \u0026#34;password\u0026#34;) ​\tJenkinsServer 类提供了进入 API 的主入口点。您可以根据 Jenkins 服务器的位置和(可选)用户名和密码/令牌创建对它的引用。\n1 Map\u0026lt;String, Job\u0026gt; jobs = jenkins.getJobs(); ​\t返回所有job，job的名称（小写）为key的Map。Job 类仅提供摘要信息(名称和 url)。\n1 JobWithDetails job = jobs.get(\u0026#34;My Job\u0026#34;).details(); ​\t获取指定job。JobWithDetails 类提供了Job的相信信息。\n使用JAVA API构建 ​\t示例代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 JobWithDetails job = jenkinsServer.getJob(\u0026#34;service-sample\u0026#34;); // 执行参数 Map\u0026lt;String,String\u0026gt; params = new HashMap\u0026lt;\u0026gt;(); params.put(\u0026#34;dev\u0026#34;,\u0026#34;true\u0026#34;); params.put(\u0026#34;BRANCH\u0026#34;,\u0026#34;master\u0026#34;); QueueReference reference = job.build(params,true); // 获取构建的queueItem QueueItem queueItem = jenkinsServer.getQueueItem(new QueueReference(queuePart)); // 获取执行对象，如果为空则还没有构建结束 Executable executable = queueItem.getExecutable(); // 构建结束后获取构建结果和构建日志,构建没有结束则会报错 Build build = jenkinsServer.getBuild(queueItem); BuildWithDetails details = build.details(); BuildResult result = details.getResult(); String logs = details.getConsoleOutputText(); JAVA API创建JOB ​\t示例代码\n1 2 3 4 /** * 配置文件为xml格式以字符串类型传入 */ jenkinsServer.createJob(\u0026#34;jobName\u0026#34;, configXmlStr, true); 参考资料\nhttps://blog.csdn.net/nklinsirui/article/details/80832005#%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3\nhttps://www.cnblogs.com/zjsupermanblog/archive/2017/07/26/7238422.html\n","date":"2020-10-12T00:00:00Z","permalink":"https://cyber-blog.github.io/p/jenkins-restful-api/","title":"Jenkins REST API介绍"},{"content":"生成 CA 证书 1 2 3 4 5 6 7 8 # 生成 KEY openssl genrsa -out ca.key 4096 # 生成证书 openssl req -x509 -new -nodes -sha512 -days 3650 \\ -subj \u0026#34;/C=CN/ST=省/L=市/O=组织/OU=组织单位/CN=常用名(或域名)\u0026#34; \\ -key ca.key \\ -out ca.crt 导入 CA 证书 内部人员需要把该 ca.crt 证书添加到受信任的根证书中，这样通过改 CA 证书生成的其他证书也会被信任。\nWindows Windows 运行输入 certmgr.msc 然后添加。操作如下图所示：\nCentOS导入 CA 证书 1 2 3 4 5 6 7 8 9 10 11 # 如果没有 ca-certificates 就安装 yum install -y ca-certificates # 把 ca.crt 证书放到下面目录 /etc/pki/ca-trust/source/anchors # 执行下面命令更新证书 update-ca-trust # 如果根证书是以 *.pem 结尾，需要转换成crt，然后再执行上述步骤。命令如下： openssl x509 -in ca.pem -inform PEM -out ca.crt Debian导入CA证书 1 2 3 4 5 6 7 8 9 10 11 12 13 # 一般情况下应该是已经有安装的了 apt-get install ca-certificates # 建立local目录是为了方便区分这是自己填加的证书，可以自定义，但一定要在/usr/share/ca-certificates目录下 mkdir -p /usr/share/ca-certificates/local # 如果根证书是以 *.pem 结尾，需要转换成crt，然后再执行上述步骤。命令如下： openssl x509 -in ca.pem -inform PEM -out ca.crt # 将CA证书复制至/usr/share/ca-certificates/local目录 # 执行下面命令后会进入一个图形化界面，第一个界面选yes，然后选中刚刚添加的证书，回车即可 dpkg-reconfigure ca-certificates 使用 CA 证书生成其他证书 例如生成泛域名证书：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # 生成 KEY openssl genrsa -out *.majiang.com.key 4096 # 生成证书请求 openssl req -sha512 -new \\ -subj \u0026#34;/C=CN/ST=河北省/L=石家庄市/O=MJ/OU=MJ/CN=*.majiang.com\u0026#34; \\ -key *.majiang.com.key \\ -out *.majiang.com.csr # 设置证书信息，主要是泛域名时，可以指定额外的 x.com cat \u0026gt; majiang.com.ext \u0026lt;\u0026lt;-EOF authorityKeyIdentifier=keyid,issuer basicConstraints=CA:FALSE keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment extendedKeyUsage = serverAuth subjectAltName = @alt_names [alt_names] DNS.1=*.majiang.com DNS.2=majiang.com EOF # 使用 CA 证书生成具体的证书 openssl x509 -req -sha512 -days 3650 \\ -extfile majiang.com.ext \\ -CA ca.crt -CAkey ca.key -CAcreateserial \\ -in *.majiang.com.csr \\ -out *.majiang.com.crt 操作过程中生成的下面两个关键的文件：\n私钥：*.majiang.com.key 公钥：*.majiang.com.crt 网站配置使用上面两个文件即可。\n参考链接\nhttps://blog.csdn.net/isea533/article/details/106397248\nhttps://github.com/goharbor/harbor/blob/v1.9.3/docs/configure_https.md https://gist.github.com/Soarez/9688998\n","date":"2020-04-01T00:00:00Z","image":"https://cyber-blog.github.io/p/ca-manage/cover_hu2789920558969423306.png","permalink":"https://cyber-blog.github.io/p/ca-manage/","title":"内部 CA 证书管理"},{"content":" yum安装慢的解决方法，修改yum源为阿里云\n1.Docker安装配置 1.1 防火墙配置 禁用防火墙\n1 systemctl disable firewalld 关闭防火墙\n1 systemctl stop firewalld 禁用 SELinux，目的是让容器可以读取主机文件系统\n1 setenforce 0 配置禁用 SELinux,修改 SELINUX 为 disabled\n1 2 3 4 vi /etc/sysconfig/selinux SELINUX=disabled #SELINUX=enforcing 禁用 swap 分区\n1 2 3 4 #禁用当前的 swap sudo swapoff -a #同时永久禁掉swap分区，打开如下文件注释掉swap那一行 sudo vi /etc/fstab 1.2 挂载磁盘 挂载磁盘，参考： Linux 不重启(动态)挂载磁盘以及简单的数据迁移\n一般购买服务器的时候会挂载一个100G以上的硬盘，需要先对硬盘进行格式化和挂载配置，然后修改 docker 配置为该挂载的路径。下面是磁盘的相关操作步骤。\n通过 fdisk -l 查看系统挂载的磁盘，阿里云中通常第二个磁盘为 /dev/vdb ，如果磁盘没有分区和格式化，先进行第 2 步操作。\n对磁盘分区，执行 fdisk /dev/vdb 进入配置后，输入 n ，后续提示全部使用默认设置，最后输入 w 写入配置进行保存。\n格式化磁盘，使用 mkfs.ext4 /dev/vdb1 进行格式化。\n输入命令编辑配置文件 vi /etc/fstab ，在最后一行添加如下配置：\n1 /dev/vdb1 /data ext4 defaults 1 2 ​ 手工和上面的配置对齐好看就行。\n5 .挂载目录，在根目录创建 /data 目录，然后使用命令 mount -a 或 mount /dev/vdb1 /data进行挂载，这种方式重启后会失效，还需继续下一步配置。\n接下来就可以使用 /data 目录了。 1.3 安装docker 在阿里巴巴开源镜像站搜docker-ce。帮助中给了一个地址：\nhttps://developer.aliyun.com/article/110806\n使用官方安装脚本自动安装 （仅适用于公网环境）\n1 curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun 安装完成后先别着急设置 docker 自动启动和启动服务。没有启动过 docker 的情况下，配置目录还不存在，先创建 mkdir -p /etc/docker ，然后在 /data下面创建一个 docker 目录（命令： mkdir /data/docker ），然后编辑配置文件 vi /etc/docker/daemon.json 添加如下配置：\n1 2 3 4 { \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://dockerhub.azk8s.cn\u0026#34;, \u0026#34;https://reg-mirror.qiniu.com\u0026#34;], \u0026#34;graph\u0026#34;: \u0026#34;/data/docker\u0026#34; } 配置好docker 后，可以设置开机启动和启动服务了。\n1 2 3 4 # 开机启动 systemctl enable docker # 启动服务 systemctl start docker 1.4 docker离线安装 在没有外网连接的特殊环境下安装docker的步骤\n参考自：https://www.centlinux.com/2019/02/install-docker-ce-on-offline-centos-7-machine.html\n首先找到一台有外网的linux服务器运行如下命令\nDockerce 需要一些在 EPEL (enterpriselinux 的额外包) yum 存储库中可用的软件包。因此，我们必须首先安装 epelyum 存储库。\n1 yum install -y epel-release.noarch 使用yum-config-manager安装 Docker CE yum 存储库\n1 yum-config-manager --add-repo=https://download.docker.com/linux/centos/docker-ce.repo 启用 dockerce (Nightly) yum 存储库。\n1 yum-config-manager --enable docker-ce-nightly 我们在 Linux 服务器中添加了两个 yum 存储库，因此，我们应该为 yum 包管理器构建缓存。\n1 yum makecache fast 下载 Docker CE 和软件依赖\n1 2 3 4 5 6 7 # 创建一个下载 dockerce 和相关软件包的目录。 mkdir ~/docker cd ~/docker # 执行命令下载 dockerce 和依赖包。 yumdownloader --resolve docker-ce # 压缩 tar cvzf ~/docker.tar.gz * 将下载的这些包安装在没有外网链接的服务器上\n1 2 3 4 5 6 7 # 创建文件夹 mkdir docker # 解压缩 tar xvf docker.tar.gz -C ~/docker cd docker # 安装并忽略依赖关系 rpm -ivh --replacefiles --replacepkgs *.rpm 1.5 修改hostname 根据本次安装的集群规模为每台服务器修改为规律的hostname\n1 hostnamectl set-hostname master1 2. kubectl下载安装 国内直接使用阿里巴巴开源镜像站安装：\nhttps://opsx.alibaba.com/mirror\n从列表找到 kubernetes，点击帮助，显示如下信息。\n重要提示：\n可以从安裝列表去掉 kubeadm、kubelet，只安装 kubectl\nDebian / Ubuntu\n1 2 3 4 5 6 7 apt-get update \u0026amp;\u0026amp; apt-get install -y apt-transport-https curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat \u0026lt;\u0026lt;EOF \u0026gt;/etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF apt-get update apt-get install -y kubectl CentOS / RHEL / Fedora\n1 2 3 4 5 6 7 8 9 cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF 如果需要离线安装的话,则在在线环境安装之后将可执行的二进制kubectl文件复制出来即可\n1 2 # 查找文件位置，找到后复制出来 find / -name \u0026#34;kubectl\u0026#34; 3. rke下载安装 下载RKE二进制文件在您的工作站上，打开Web浏览器并导航到我们的RKE版本页面。下载适用于您的操作系统和体系结构的最新RKE安装程序：\nMacOS：rke_darwin-amd64 Linux（Intel / AMD）：rke_linux-amd64 Linux（ARM 32位）：rke_linux-arm Linux（ARM 64位）：rke_linux-arm64 Windows（32位）：rke_windows-386.exe Windows（64位）：rke_windows-amd64.exe 将RKE二进制文件复制到您的文件夹中 $PATH 并重命名 rke（或 rke.exe 用于 Windows）。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # MacOS $ mv rke_darwin-amd64 rke # 增加可执行权限 $ chmod +x rke # Linux $ mv rke_linux-amd64 rke # 增加可执行权限 $ chmod +x rke # Windows PowerShell \u0026gt; mv rke_windows-amd64.exe rke.exe # 验证安装： $ rke --version # 输出类似下面版本信息 rke version v1.0.5 4. helm3 下载安装 官方文档：Installing Helm\n针对 Linux 系统，使用下面的命令进行安装：\n1 2 3 $ curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 $ chmod 700 get_helm.sh $ ./get_helm.sh 5. ssh-key各个节点的配置 假设分别为 NODE1, NODE2, NODE3 3个节点。使用的 root 账号生成ssh-key。\n1 2 3 4 5 $ ssh-keygen -t rsa # 之后会有一些默认选项和可以手工配置的选项，可以自行配制，或者一直回车用默认值 # 执行完成后，会生成两个文件 Your identification has been saved in /root/.ssh/id_rsa. Your public key has been saved in /root/.ssh/id_rsa.pub. 查看 id_rsa.pub 文件，复制文件内容。在每个节点上，切换到rancher用户，然后执行如下命令\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # 创建 rancher 用户 $ useradd rancher # 添加到 docker 组 $ usermod -aG docker rancher # 切换到前面创建的用户 $ su rancher # 进入自己的 home 目录 $ cd ~ # 创建 .ssh 目录 $ mkdir .ssh # 写入NODE1的root用户的公钥 $ echo \u0026#34;复制的内容\u0026#34; \u0026gt;\u0026gt; .ssh/authorized_keys # 设置权限 $ chmod 700 .ssh $ chmod 644 .ssh/authorized_key # 配置后可以在 NODE1 测试 # 连接成功后通过 exit 退出 $ ssh rancher@NODE2 特别注意！\n两个潜在的坑，ssh配置authorized_keys后仍然需要输入密码的问题\n注意 $HOME/.ssh 目录 或 $HOME目录的权限 最好是700，我就在这里栽跟头了。\n注意 uthorized_keys 的权限 chmod 644 authorized_keys 这个也是要注意的。\n6. rke 配置集群 6.1 rke配置离线集群 在离线环境当中安装集群时，要求您使用一个 Linux 工作站，它可以访问互联网和您的私有镜像库。请确保至少有 20GB 的磁盘空间可用。如果要使用 ARM64 主机，则镜像仓库必须支持 Docker Manifest。截至 2020 年 4 月，Amazon Elastic Container Registry 不支持 Docker Manifest 功能。\n查找您用的 Rancher 版本所需要的资源 浏览我们的版本发布页面，查找您想安装的 Rancher v2.x.x 版本。不要下载标记为 rc 或 Pre-release 的版本，因为它们在生产环境下是不稳定的。\n从发行版 Assets 部分下载以下文件，这些文件是离线环境下安装 Rancher 所必需的：\nRelease 文件 描述 rancher-images.txt 此文件包含安装 Rancher、创建集群和运行 Rancher 工具所需的镜像列表。 rancher-save-images.sh 这个脚本会从 DockerHub 中拉取在文件rancher-images.txt中描述的所有镜像，并将它们保存为文件rancher-images.tar.gz。 rancher-load-images.sh 这个脚本会载入文件rancher-images.tar.gz中的镜像，并将它们推送到您自己的私有镜像库。 收集 cert-manager 镜像 如果您使用自己的证书，或者要在外部负载均衡器上终止 TLS，请跳过此步骤。\n在安装高可用过程中，如果选择使用 Rancher 默认的自签名 TLS 证书，则还必须将 cert-manager 镜像添加到 rancher-images.txt 文件中。如果使用自己的证书，则跳过此步骤。\n获取最新的cert-manager Helm chart，解析模板,获取镜像详细信息：\n注意： 由于cert-manager最近的改动，您需要升级cert-manager版本。如果您要升级 Rancher 并且使用cert-manager的版本低于 v0.12.0，请看我们的升级文档。\n1 2 3 4 helm repo add jetstack https://charts.jetstack.io helm repo update helm fetch jetstack/cert-manager --version v0.12.0 helm template ./cert-manager-\u0026lt;version\u0026gt;.tgz | grep -oP \u0026#39;(?\u0026lt;=image: \u0026#34;).*(?=\u0026#34;)\u0026#39; \u0026gt;\u0026gt; ./rancher-images.txt 对镜像列表进行排序和唯一化，去除重复的镜像源：\n1 sort -u rancher-images.txt -o rancher-images.txt 将镜像保存到您的工作站中 为rancher-save-images.sh 文件添加可执行权限：\n1 chmod +x rancher-save-images.sh 执行脚本rancher-save-images.sh并以--image-list ./rancher-images.txt 作为参数，创建所有需要镜像的压缩包：\n1 ./rancher-save-images.sh --image-list ./rancher-images.txt 结果： Docker 会开始拉取用于离线安装所需的镜像。这个过程会花费几分钟时间。完成时，您的当前目录会输出名为rancher-images.tar.gz的压缩包。请确认输出文件是否存在。\n推送镜像到私有镜像库 下一步，您将使用脚本将文件 rancher-images.tar.gz 中的镜像上传到您自己的私有镜像库。\n文件 rancher-images.txt 和 rancher-images.tar.gz 应该位于工作站中运行 rancher-load-images.sh 脚本的同一目录下。\n登录私有镜像库：\n1 docker login \u0026lt;REGISTRY.YOURDOMAIN.COM:PORT\u0026gt; 为 rancher-load-images.sh 添加可执行权限：\n1 chmod +x rancher-load-images.sh 使用脚本 rancher-load-images.sh提取rancher-images.tar.gz文件中的镜像，根据文件rancher-images.txt中的镜像列表对提取的镜像文件重新打 tag 并推送到您的私有镜像库：\n1 ./rancher-load-images.sh --image-list ./rancher-images.txt --registry \u0026lt;REGISTRY.YOURDOMAIN.COM:PORT\u0026gt;\t参考自官方文档\n参考下面的示例创建rancher-cluster.yml文件，替换 nodes 3个节点的配置信息。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 nodes: - address: 192.168.0.28 user: rancher role: [controlplane,worker,etcd] - address: 192.168.0.30 user: rancher role: [worker,etcd] - address: 192.168.0.26 user: rancher role: [worker,etcd] services: etcd: snapshot: true creation: 6h retention: 24h kube-controller: # CIDR pool used to assign IP addresses to pods in the cluster cluster_cidr: 10.90.0.0/16 # 如果有私有的镜像仓库则可选该配置 private_registries: - url: \u0026lt;REGISTRY.YOURDOMAIN.COM:PORT\u0026gt; # 私有镜像库地址 user: rancher password: \u0026#34;*********\u0026#34; is_default: true 常见的RKE节点选项\nOption Required Description address yes 公共DNS或IP地址 user yes 可以运行docker命令的用户 role yes 分配给节点的Kubernetes角色列表 internal_address no 内部集群流量的私有DNS或IP地址 ssh_key_path no 用于对节点进行身份验证的SSH私钥的路径（默认~/.ssh/id_rsa) 高级配置\nRKE有许多配置选项可用于自定义安装以适合您的特定环境。\n有关选项和功能的完整列表，请参阅RKE文档。\n要为更大的Rancher安装调整您的etcd集群，请参阅etcd设置指南。\n特别注意：如果使用了阿里云等公有云环境时，确保主机之间端口能够互相访问，例如在阿里云\n配置：\n根据自己的配置修改保存好 rancher-cluster.yml 后，执行下面的命令：\n1 rke up --config ./rancher-cluster.yml 该命令根据网速等多种情况，需要等待很久。这期间输出的部分日志如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [root@localhost ~]# rke up --config ./rancher-cluster.yml INFO[0000] Initiating Kubernetes cluster INFO[0000] [certificates] Generating admin certificates and kubeconfig INFO[0000] Successfully Deployed state file at [./rancher-cluster.rkestate] INFO[0000] Building Kubernetes cluster INFO[0000] [dialer] Setup tunnel for host [192.168.0.1] INFO[0000] [dialer] Setup tunnel for host [192.168.0.2] INFO[0000] [dialer] Setup tunnel for host [192.168.0.3] INFO[0000] [network] Deploying port listener containers INFO[0000] [network] Pulling image [rancher/rke-tools:v0.1.34] on host [10.10.1.238] INFO[0000] [network] Pulling image [rancher/rke-tools:v0.1.34] on host [10.10.1.241] INFO[0000] [network] Pulling image [rancher/rke-tools:v0.1.34] on host [10.10.1.242] INFO[0030] [network] Successfully pulled image [rancher/rke-tools:v0.1.34] on host [10.10.1.241] 集群创建成功后，在当前目录下面有以下文件：\n1 2 3 -rw-r----- 1 root root 5382 Aug 1 16:17 kube_config_rancher-cluster.yml -rw-r----- 1 root root 114422 Aug 1 16:18 rancher-cluster.rkestate -rw-r--r-- 1 root root 423 Aug 1 15:50 rancher-cluster.yml RKE 自动创建了一个 kube_config_rancher-cluster.yml ，这个文件有 kubectl 和 helm 的凭据。\n保存您的文件 重要 以下文件需要被维护，用来故障排查和升级集群。\n将以下文件的副本保存在安全的位置：\nrancher-cluster.yml：RKE 配置文件 kube_config_rancher-cluster.yml：Kubeconfig 文件 rancher-cluster.rkestate：Kubernetes 集群状态文件 注意： 后两个文件名的“rancher-cluster”部分取决于您如何命名 RKE 集群的配置文件。\n将 kube_config_rancher-cluster.yml 复制到 $HOME/.kube/config ，如果想在其他节点通过kubectl 访问集群，将该文件复制到其他节点的相同路径下。 现在可以通过 kubectl 来查看当前的节点状态：\n1 2 3 4 5 $ kubectl get nodes NAME STATUS ROLES AGE VERSION 192.168.0.1 Ready controlplane,etcd,worker 16m v1.14.3 192.168.0.2 Ready worker 16m v1.14.3 192.168.0.3 Ready worker 16m v1.14.3 7.helm安装rancher 参考文档：helm-rancher\n离线安装参考文档\n7.1 添加稳定版Helm Chart仓库 1 helm repo add rancher-stable https://releases.rancher.com/server-charts/stable 如果为离线环境安装的话则参考以下步骤执行：\n从可以访问 Internet 的系统中，获取最新的 Rancher Helm Chart，然后将内容复制到可以访问 Rancher Server 集群的系统中。\n如果您还没有在有互联网访问的系统上安装helm。注意：请参考Helm 版本要求来选择一个 Helm 版本来安装 Rancher。\n使用helm repo add来添加仓库，不同的地址适应不同的 Rancher 版本，请替换命令中的\u0026lt;CHART_REPO\u0026gt;，替换为latest，stable或alpha。更多信息请参考如何选择 Rancher 版本。\nlatest： 最新版，建议在尝试新功能时使用。 stable：稳定版，建议生产环境中使用。 alpha：预览版， 未来版本的实验性预览。 1 helm repo add rancher-\u0026lt;CHART_REPO\u0026gt; https://releases.rancher.com/server-charts/\u0026lt;CHART_REPO\u0026gt; 获取最新的 Rancher Chart，您会看到对应的 tgz 文件下载到本地。\n1 helm fetch rancher-\u0026lt;CHART_REPO\u0026gt;/rancher 7.2 为Rancher创建名称空间 我们需要定义一个名称空间，应在该名称空间中安装由Chart创建的资源。这应该始终是cattle-system\n1 kubectl create namespace cattle-system 7.3 选择您的SSL配置 Rancher Server 在默认情况下被设计为安全的，并且需要 SSL/TLS 配置。\n当在离线环境的 Kubernetes 中安装 Rancher 时，推荐两种证书生成方式。\n**注意：**如果要在外部终止 SSL/TLS，请参阅在外部负载均衡器上终止 TLS。\n设置 Chart 选项 描述 是否需要 cert-manager？ Rancher 生成的自签名证书 ingress.tls.source=rancher 使用 Rancher 生成的 CA 签发的自签名证书 此项为默认选项。在渲染 Rancher Helm 模板时不需要传递此项。 是 已有的证书 ingress.tls.source=secret 通过创建 Kubernetes Secret（s）使用您已有的证书文件。 渲染 Rancher Helm 模板时必须传递此选项。 否 重要\nRancher 中国技术支持团队建议您使用“您已有的证书” ingress.tls.source=secret 这种方式，从而减少对 cert-manager 的运维成本。\n选择 CA 颁发的可靠证书。\n根据您自己的证书创建Kubernetes secrent，以供Rancher使用。\n1 2 3 kubectl -n cattle-system create secret tls tls-rancher-ingress \\ --cert=tls.crt \\ --key=tls.key 大部分情况下会使用自生成的证书，自生成证书的步骤参考https://blog.csdn.net/isea533/article/details/106397248。\n注意证书相关文件的名称只能是固定的tls 和cacerts\n.crt 和 .pem 的转换只需要修改后缀名称即可。参考https://stackoverflow.com/questions/991758/how-to-get-pem-file-from-key-and-crt-files\n使用自生成的证书创建Kubernetes secrent\n参考 https://rancher.com/docs/rancher/v2.x/en/installation/resources/tls-secrets/\n1 2 3 4 5 6 7 8 # 使用自己生成的证书。将泛域名证书的名称修改为tls kubectl -n cattle-system create secret tls tls-rancher-ingress \\ --cert=tls.crt \\ --key=tls.key # 将之前生成的 ca.crt 修改为 cacerts.pem kubectl -n cattle-system create secret generic tls-ca \\ --from-file=cacerts.pem=./cacerts.pem 7.4 安装rancher 在线环境直接运行以下命令即可\n1 2 3 4 5 6 7 8 9 10 11 helm install rancher rancher-stable/rancher \\ --namespace cattle-system \\ --set hostname=rancher.my.org \\ --set ingress.tls.source=secret # 使用自生成证书则使用以下命令 helm install rancher rancher-stable/rancher \\ --namespace cattle-system \\ --set hostname=lims-rancher.sh-tec.com \\ --set ingress.tls.source=secret \\ --set privateCA=true 7.4.1 离线环境安装步骤 渲染您的 Rancher Helm 模板\n设置 Rancher Helm 模板时，Chart 中有几个选项是专门为离线安装设计的。\nChart 选项 值 描述 certmanager.version \u0026quot;\u0026quot; 根据运行的 cert-manager 版本配置适当的 Rancher TLS 颁发者。 systemDefaultRegistry \u0026lt;REGISTRY.YOURDOMAIN.COM:PORT\u0026gt; 配置 Rancher，在创建集群时，Rancher Server 始终从私有镜像仓库中拉取镜像 useBundledSystemChart true 配置 Rancher Server 使用内置的 system-chart，system-chart中包含监控，日志，告警和全局 DNS 等功能所需的 Chart。这些 Helm charts 位于 GitHub 中，但是由于您处于离线环境中，因此使用 Rancher 中内置的 Chart 比设置一个 Git 镜像简单得多。当然您也可以选择自己手动镜像 GitHub 中的 Rancher System Chart。自 v2.3.0 起可用 使用已有的证书\n根据您已有的证书创建 Kubernetes 密文，以供 Rancher 使用。证书的common name将需要与以下命令中的hostname选项匹配，否则 ingress controller 将无法为 Rancher 设置入口。\n设置 Rancher 模板，声明您选择的选项。使用下面表中的参考选项，需要给 Rancher 配置使用私有镜像库。\n占位符 描述 \u0026lt;VERSION\u0026gt; Rancher 版本 \u0026lt;RANCHER.YOURDOMAIN.COM\u0026gt; 负载均衡器对应的 DNS \u0026lt;REGISTRY.YOURDOMAIN.COM:PORT\u0026gt; 私有镜像库对应的 DNS 1 2 3 4 5 6 7 helm template rancher ./rancher-\u0026lt;VERSION\u0026gt;.tgz --output-dir . \\ --namespace cattle-system \\ --set hostname=\u0026lt;RANCHER.YOURDOMAIN.COM\u0026gt; \\ --set rancherImage=\u0026lt;REGISTRY.YOURDOMAIN.COM:PORT\u0026gt;/rancher/rancher \\ --set ingress.tls.source=secret \\ --set systemDefaultRegistry=\u0026lt;REGISTRY.YOURDOMAIN.COM:PORT\u0026gt; \\ # 自v2.2.0可用，设置默认的系统镜像仓库 --set useBundledSystemChart=true # 自v2.3.0可用，使用内嵌的 Rancher system charts 如果您使用的是由私有 CA 签名的证书，则在--set ingress.tls.source=secret之后添加--set privateCA=true：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 helm template rancher ./rancher-\u0026lt;VERSION\u0026gt;.tgz --output-dir . \\ --namespace cattle-system \\ --set hostname=\u0026lt;RANCHER.YOURDOMAIN.COM\u0026gt; \\ --set rancherImage=\u0026lt;REGISTRY.YOURDOMAIN.COM:PORT\u0026gt;/rancher/rancher \\ --set ingress.tls.source=secret \\ --set privateCA=true \\ --set systemDefaultRegistry=\u0026lt;REGISTRY.YOURDOMAIN.COM:PORT\u0026gt; \\ # 自v2.2.0可用，设置默认的系统镜像仓库 --set useBundledSystemChart=true # 自v2.3.0可用，使用内嵌的 Rancher system charts # example helm template rancher ./rancher-2.4.4.tgz --output-dir . \\ --namespace cattle-system \\ --set hostname=rancher.wx.com \\ --set ingress.tls.source=secret \\ --set privateCA=true \\ --set useBundledSystemChart=true 可选：要安装指定的 Rancher 版本，请设置rancherImageTag的值，例如：--set rancherImageTag = v2.3.6\n然后请参考添加 TLS 密文发布证书文件，以便 Rancher 和 ingress controller 可以使用它们。\n安装rancher\n1 kubectl -n cattle-system apply -R -f ./rancher 验证 Rancher Server 是否已成功部署\n1 2 3 4 # 检查 Rancher Server 是否运行成功： kubectl -n cattle-system rollout status deploy/rancher Waiting for deployment \u0026#34;rancher\u0026#34; rollout to finish: 0 of 3 updated replicas are available... deployment \u0026#34;rancher\u0026#34; successfully rolled out 1 2 # 重置rancher密码 kubectl --kubeconfig kube_config_rancher-cluster.yml -n cattle-system exec $(kubectl --kubeconfig kube_config_rancher-cluster.yml -n cattle-system get pods -l app=rancher | grep \u0026#39;1/1\u0026#39; | head -1 | awk \u0026#39;{ print $1 }\u0026#39;) -- reset-password harbor安装 harbor系统需求docker 17.06.0-ce+、docker-compose 1.18.0+。docker-compose为二进制可执行文件\ndocker-compose安装参考官方文档\n1 2 3 sudo curl -L \u0026#34;https://github.com/docker/compose/releases/download/1.27.4/docker-compose-$(uname -s)-$(uname -m)\u0026#34; -o /usr/local/bin/docker-compose # 添加可执行权限 sudo chmod +x /usr/local/bin/docker-compose harbor安装参照官方githubrelease页面下载最新的安装包\nharbor配置文件参考\n参考官方文档\n关于Harbor https 私有证书配置注意事项 https://blog.csdn.net/isea533/article/details/102872633\n自生成证书的步骤参考https://blog.csdn.net/isea533/article/details/106397248。\n.crt he .pem 的转换只需要修改后缀名称即可。参考https://stackoverflow.com/questions/991758/how-to-get-pem-file-from-key-and-crt-files\n注意，安装方式解决harbor不会自启动的问题参考https://www.cnblogs.com/kirito-c/p/11145881.html\n","date":"2020-01-21T00:00:00Z","image":"https://cyber-blog.github.io/p/ranchar-install/cover_hu4589927305658315706.png","permalink":"https://cyber-blog.github.io/p/ranchar-install/","title":"Ranchar CentOS 安装指南"}]